

Introduction

The Cisco Application Centric Infrastructure (ACI) is an architecture that allows the application to define the networking requirements in a programmatic way. This architecture simplifies, optimizes, and accelerates the entire application deployment lifecycle. Cisco Application Policy Infrastructure Controller (APIC) is the software, or operating system, that acts as the controller.

This document describes the features, issues, and limitations for the Cisco APIC software. For the features, issues, and limitations for the Cisco NX-OS software for the Cisco Nexus 9000 series switches, see the Cisco Nexus 9000 ACI-Mode Switches Release Notes, Release 16.0(3).

For more information about this product, see "Related Content."

Date

Description

August 9, 2023

Release 6.0(3d) became available.

New Software Features

Product Impact

Feature

Description

Base Functionality

Rogue endpoint exception list support for bridge domains and L3Outs

You can create a global rogue/COOP exception list, which excludes a MAC address from rogue endpoint control on all the bridge domains on which the MAC address is discovered, and you can create a rogue/COOP exception list for L3Outs. You can also exclude all MAC addresses for a bridge domain or L3Out. This simplifies creating the exception list when you want to make an exception for every MAC address; you do not need to enter each address individually.

For more information, see the Cisco APIC Basic Configuration Guide, Release 6.0(x).

Reliability

TCP MSS adjustment

You can adjust the transmission control protocol (TCP) maximum segment size (MSS) to avoid packet fragmentation or drops.

For more information, see the Cisco APIC System Management Configuration Guide, Release 6.0(x).

Upgrade/Downgrade

Faster database conversion

This release reduces the wait time for the database conversion process when compared to the previous releases.

For more information, see the Cisco APIC Installation and ACI Upgrade and Downgrade Guide.

Memory-based switch image installation

A switch installs either the 32-bit or 64-bit image based on the switch's amount of memory instead of based on a static mapping.

For more information, see the Cisco APIC Installation and ACI Upgrade and Downgrade Guide.

Interoperability

New TLVs for Azure Stack HCI

You can select new Time, Length, and Values (TLVs) for Azure Stack HCI.

For more information, see the Cisco APIC Support for CDP and LLDP document.

Support for Policy mode for Cisco ACI and VMware NSX-T integration

You can now use the NSX-T Policy API mode, which uses the new Segment object.

For more information, see the Cisco ACI and VMware NSX-T Data Center Integration document and the VMware NSX-T Data Center documentation.

Support for VMware vSphere 8.0

VMM domains now support VMware vSphere 8.0.

For more information, see the Cisco ACI vCenter Plug-in chapter in the Cisco ACI Virtualization Guide, 5.2(x) and Cisco ACI Virtual Edge Release Notes, Release 3.2(4).

Cisco ACI VMM Integration with Nutanix AHV

You can now integrate the Nutanix Acropolis Hypervisor (AHV) with Cisco Application Centric Infrastructure (ACI). The Cisco APIC integrates with Nutanix AHV and enhances the network management capabilities. The integration provides virtual and physical network automation and VM endpoints visibility in Cisco ACI.

For more information, see the Cisco ACI and Nutanix AHV Integration guide.

New Hardware Features

For the new hardware features, see the Cisco Nexus 9000 ACI-Mode Switches Release Notes, Release 16.0(3).

Changes in Behavior

● Scale enhancements:

◦ VRF instances

o Default (Dual Stack) scale profile

§ Switches with 32GB of RAM: 2000

§ Other switches: 800

o High Dual Stack, High LPM, High Policy scale profiles

§ LSE2 switches with 32GB of RAM: 2000

§ Other switches: 800

o Maximum LPM scale profile:

§ LSE2 switches with 32GB of RAM: 250

◦ External EPGs

o Switches with 32GB of RAM: 2000

o Other switches: 800

◦ Security TCAM size

o LSE2 switches with 32GB of RAM: 32,000

o Other switches: 8000

◦ Rogue Exception

o Fv Rogue Exception Mac (bridge domain-level MAC exception): 500

o Fabric Rogue Exception Mac (Fabric wide MAC exception list configuration): 10,000

Open Issues

Click the bug ID to access the Bug Search tool and see additional information about the bug. The "Exists In" column of the table specifies the 6.0(3) releases in which the bug exists. A bug might also exist in releases other than the 6.0(3) releases.

Bug ID

Description

Exists in

CSCvt99966

A SPAN session with the source type set to "Routed-Outside" goes down. The SPAN configuration is pushed to the anchor or non-anchor nodes, but the interfaces are not pushed due to the following fault: "Failed to configure SPAN with source SpanFL3out due to Source fvIfConn not available".

6.0(3d) and later

CSCvy40511

Traffic from an endpoint under a remote leaf switch to an external node and its attached external networks is dropped. This occurs if the external node is attached to an L3Out with a vPC and there is a redistribution configuration on the L3Out to advertise the reachability of the external nodes as direct-attached hosts.

6.0(3d) and later

CSCwa90084

- Traffic gets disrupted across a vPC pair on a given encapsulation.

OR

- EPG flood in encapsulation gets blackholed on a given encapsulation.

OR

- STP packets received on an encapsulation on a given port are not forwarded on all the leaf switches where the same EPG/same encapsulation is deployed.

6.0(3d) and later

CSCwd81562

A Cisco APIC that was previously part of the Cisco APIC cluster will not rejoin the cluster after the reload, decommission, and commission process.

6.0(3d) and later

CSCwe47517

Initially, a switch node upgrade fails with the error "The requested URL returned error: 404 Not Found." After a few minutes, the switch downloads the image from the Cisco APIC.

6.0(3d) and later

CSCwf20254

There is a delay in the download and programming of policies on a node.

6.0(3d) and later

CSCwf48875

When using two different host profiles (for example UCS C-Series and UCS B-Series) to deploy NSX, the uplink policy will be different for the host profiles. In this case, using one uplink profile with two policies might cause traffic disruption for a non-default teaming policy.

6.0(3d) and later

CSCwf50517

When using the "add controllers" API with 3 or more nodes in the payload, the API can timeout in some cases.

6.0(3d) and later

CSCwf71934

Multiple duplicate subnets are created on Nutanix for the same EPG.

6.0(3d) and later

CSCwf94748

Ingress and egress packet statistics can be viewed for VM and host in a Nutanix domain. For certain time stamps, the "MAX" packet count for the ingress and egress packet is shown incorrectly.

6.0(3d) and later

CSCwf99067

Deleting and re-adding RedirectDest with a different IP address, but the same MAC address, generates the following error: "Same virtual MAC is provided for different RedirectDest".

6.0(3d) and later

CSCwh03912

Disabling resilient hashing removes the backup PBR policy selection drop-down list. Upon clicking the Submit button, the following error displays: "Error: 400 - RsBackupPol can be created only if resilient hash is enabled"

6.0(3d) and later

CSCwh06326

Even after logging out from the Cisco APIC, the JSON Web Token (JWT) remains valid for about 10 minutes.

6.0(3d) and later

Resolved Issues

Bug ID

Description

Fixed in

CSCwd82212

There is a login denied error while importing or exporting a configuration.

6.0(3d)

CSCwe01680

User is not allowed to configure static route for an inband EPG which is not deployed on the current APIC.

6.0(3d)

CSCwe03646

After moving a port from one selector/profile to another profile in the same transaction (only possible using the REST API), the port might stop working. This is non-deterministic.

6.0(3d)

CSCwe13941

Following are some of the symptoms seen because of this issue :

1. Failure to verify APIC's CIMC credentials.

2. Failure to verify the power status.

3. Failure to verify the serial number of the APIC as seen in CIMC.

These symptoms can be seen during the following workflows:

1. APIC Cluster Initial Bootstrap.

2. Adding a new APIC to the cluster - Expansion.

3. Replacing an APIC in the cluster - RMA operation.

4. Recommission of APIC following a decommission.

6.0(3d)

CSCwe39842

PXE boot for vmedia installation of the Cisco APIC 6.0(3) release does not work on APIC-SERVER-M2/M3/L2/L3.

6.0(3d)

CSCwe41446

When APICs are upgraded to the 6.0(3) release and switches are still on older releases, the upgraded standby Cisco APIC cannot join the cluster.

6.0(3d)

CSCwe46071

A leaf node gets stuck in bootstrap. Although bootstrap eventually gets forced completed, the node might not download the entire expected configuration, resulting in a node that is not fully functional.

6.0(3d)

CSCwe47966

SMU installation fails in the 6.0(3) release due to collecting the techsupport files prior to installing the SMU.

6.0(3d)

CSCwf54771

User configuration is missing on APICs and switches following an ungraceful reload or power outage.

6.0(3d)

CSCwf72015

vAPICs hosted on ESXi hosts directly connected to the fabric must see the leaf switch using LLDP. Hosts cannot be connected by an intermediate switch, including UCS Fabric Interconnects. This applies to vAPIC clusters and vAPICs used in ACI mini deployments.

6.0(3d)

Known Issues

Click the bug ID to access the Bug Search tool and see additional information about the bug. The "Exists In" column of the table specifies the 6.0(3) releases in which the bug exists. A bug might also exist in releases other than the 6.0(3) releases.

Bug ID

Description

Exists in

CSCvj26666

The "show run leaf|spine <nodeId>" command might produce an error for scaled up configurations.

6.0(3d) and later

CSCvj90385

With a uniform distribution of EPs and traffic flows, a fabric module in slot 25 sometimes reports far less than 50% of the traffic compared to the traffic on fabric modules in non-FM25 slots.

6.0(3d) and later

CSCvm71833

Switch upgrades fail with the following error: Version not compatible.

6.0(3d) and later

CSCvq39764

When you click Restart for the Microsoft System Center Virtual Machine Manager (SCVMM) agent on a scaled-out setup, the service may stop. You can restart the agent by clicking Start.

6.0(3d) and later

CSCvq58953

One of the following symptoms occurs:

App installation/enable/disable takes a long time and does not complete.

Nomad leadership is lost. The output of the acidiag scheduler logs members command contains the following error:

Error querying node status: Unexpected response code: 500 (rpc error: No cluster leader)

6.0(3d) and later

CSCvr89603

The CRC and stomped CRC error values do not match when seen from the APIC CLI compared to the APIC GUI. This is expected behavior. The GUI values are from the history data, whereas the CLI values are from the current data.

6.0(3d) and later

CSCvs19322

Upgrading Cisco APIC from a 3.x release to a 4.x release causes Smart Licensing to lose its registration. Registering Smart Licensing again will clear the fault.

6.0(3d) and later

CSCvs77929

In the 4.x and later releases, if a firmware policy is created with different name than the maintenance policy, the firmware policy will be deleted and a new firmware policy gets created with the same name, which causes the upgrade process to fail.

6.0(3d) and later

CSCvx75380

svcredirDestmon objects get programmed in all of the leaf switches where the service L3Out is deployed, even though the service node may not be connected to some of the leaf switch.

There is no impact to traffic.

6.0(3d) and later

CSCvx78018

A remote leaf switch has momentary traffic loss for flushed endpoints as the traffic goes through the tglean path and does not directly go through the spine switch proxy path.

6.0(3d) and later

CSCvy07935

xR IP flush for all endpoints under the bridge domain subnets of the EPG being migrated to ESG. This will lead to a temporary traffic loss on remote leaf switch for all EPGs in the bridge domain. Traffic is expected to recover.

6.0(3d) and later

CSCvy10946

With the floating L3Out multipath recursive feature, if a static route with multipath is configured, not all paths are installed at the non-border leaf switch/non-anchor nodes.

6.0(3d) and later

CSCvy34357

Starting with the 6.0(3) release, the following apps built with the following non-compliant Docker versions cannot be installed nor run:

6.0(3d) and later

CSCvy45358

The file size mentioned in the status managed object for techsupport "dbgexpTechSupStatus" is wrong if the file size is larger than 4GB.

6.0(3d) and later

CSCvz06118

In the "Visibility and Troubleshooting Wizard," ERSPAN support for IPv6 traffic is not available.

6.0(3d) and later

CSCvz84444

While navigating to the last records in the various History sub tabs, it is possible to not see any results. The first, previous, next, and last buttons will then stop working too.

6.0(3d) and later

CSCvz85579

VMMmgr process experiences a very high load for an extended period of time that impacts other operations that involve it.

The process may consume excessive amount of memory and get aborted. This can be confirmed with the command "dmesg -T | grep oom_reaper" if messages such as the following are reported:

oom_reaper: reaped process 5578 (svc_ifc_vmmmgr.)

6.0(3d) and later

CSCwa78573

When the "BGP" branch is expanded in the Fabric > Inventory > POD 1 > Leaf > Protocols > BGP navigation path, the GUI freezes and you cannot navigate to any other page.

This occurs because the APIC gets large set of data in response, which cannot be handled by the browser for parts of the GUI that do not have the pagination.

6.0(3d) and later

CSCwe18213

The logical switch created for the EPG remains in the NSX-T manager after the EPG is disassociated from the domain, or the logical switch does not get created when the EPG is associated with the domain.

6.0(3d) and later

CSCwe50393

Using the back-to-back spine switch wizard will not display node IDs for the switch selection, and so the task in the wizard cannot be completed.

6.0(3d) and later

CSCwf80352

Cisco APIC does not accept special characters "#" and ";" in then fabric name field when upgrading to the 6.0(3) release. For example, if the fabric name is "Test#03, it will be truncated to "Test", which causes prevents switches from joining the fabric after they are reloaded during the upgrade. In this example, the Cisco APIC expects the name "Test#03", but the switch is assigned the name "Test".

6.0(3d) and later

N/A

If you are upgrading to Cisco APIC release 4.2(6o), 4.2(7l), 5.2(1g), or later, ensure that any VLAN encapsulation blocks that you are explicitly using for leaf switch front panel VLAN programming are set as "external (on the wire)." If these VLAN encapsulation blocks are instead set to "internal," the upgrade causes the front panel port VLAN to be removed, which can result in a datapath outage.

6.0(3d) and later

N/A

Beginning in Cisco APIC release 4.1(1), the IP SLA monitor policy validates the IP SLA port value. Because of the validation, when TCP is configured as the IP SLA type, Cisco APIC no longer accepts an IP SLA port value of 0, which was allowed in previous releases. An IP SLA monitor policy from a previous release that has an IP SLA port value of 0 becomes invalid if the Cisco APIC is upgraded to release 4.1(1) or later. This results in a failure for the configuration import or snapshot rollback.

The workaround is to configure a non-zero IP SLA port value before upgrading the Cisco APIC, and use the snapshot and configuration export that was taken after the IP SLA port change.

6.0(3d) and later

N/A

If you use the REST API to upgrade an app, you must create a new firmware.OSource to be able to download a new app image.

6.0(3d) and later

N/A

In a multipod configuration, before you make any changes to a spine switch, ensure that there is at least one operationally "up" external link that is participating in the multipod topology. Failure to do so could bring down the multipod connectivity. For more information about multipod, see the Cisco Application Centric Infrastructure Fundamentals document and the Cisco APIC Getting Started Guide.

6.0(3d) and later

N/A

With a non-english SCVMM 2012 R2 or SCVMM 2016 setup and where the virtual machine names are specified in non-english characters, if the host is removed and re-added to the host group, the GUID for all the virtual machines under that host changes. Therefore, if a user has created a micro segmentation endpoint group using "VM name" attribute specifying the GUID of respective virtual machine, then that micro segmentation endpoint group will not work if the host (hosting the virtual machines) is removed and re-added to the host group, as the GUID for all the virtual machines would have changed. This does not happen if the virtual name has name specified in all english characters.

6.0(3d) and later

N/A

A query of a configurable policy that does not have a subscription goes to the policy distributor. However, a query of a configurable policy that has a subscription goes to the policy manager. As a result, if the policy propagation from the policy distributor to the policy manager takes a prolonged amount of time, then in such cases the query with the subscription might not return the policy simply because it has not reached policy manager yet.

6.0(3d) and later

N/A

When there are silent hosts across sites, ARP glean messages might not be forwarded to remote sites if a leaf switch without -EX or a later designation in the product ID happens to be in the transit path and the VRF is deployed on that leaf switch, the switch does not forward the ARP glean packet back into the fabric to reach the remote site. This issue is specific to transit leaf switches without -EX or a later designation in the product ID and does not affect leaf switches that have -EX or a later designation in the product ID. This issue breaks the capability of discovering silent hosts.

6.0(3d) and later

N/A

Typically, faults are generally raised based on the presence of the BGP route target profile under the VRF table. However, if a BGP route target profile is configured without actual route targets (that is, the profile has empty policies), a fault will not be raised in this situation.

6.0(3d) and later

N/A

MPLS interface statistics shown in a switch's CLI get cleared after an admin or operational down event.

6.0(3d) and later

N/A

MPLS interface statistics in a switch's CLI are reported every 10 seconds. If, for example, an interface goes down 3 seconds after the collection of the statistics, the CLI reports only 3 seconds of the statistics and clears all of the other statistics.

6.0(3d) and later

Virtualization Compatibility Information

This section lists virtualization compatibility information for the Cisco APIC software.

● For a table that shows the supported virtualization products, see the ACI Virtualization Compatibility Matrix.

● For information about Cisco APIC compatibility with Cisco UCS Director, see the appropriate Cisco UCS Director Compatibility Matrix document.

● If you use Microsoft vSwitch and want to downgrade to Cisco APIC Release 2.3(1) from a later release, you first must delete any microsegment EPGs configured with the Match All filter.

● This release supports the following additional virtualization products:

Product

Supported Release

Information Location

Microsoft Hyper-V

2016 Update Rollup 1, 2, 2.1, and 3

N/A

VMM Integration and VMware Distributed Virtual Switch (DVS)

6.5, 6.7, and 7.0

Cisco ACI Virtualization Guide, Release 6.0(x)

Hardware Compatibility Information

This release supports the following Cisco APIC servers:

Product ID

Description

APIC-L2

Cisco APIC with large CPU, hard drive, and memory configurations (more than 1000 edge ports)

APIC-L3

Cisco APIC with large CPU, hard drive, and memory configurations (more than 1200 edge ports)

APIC-L4

Cisco APIC with large CPU, hard drive, and memory configurations (more than 1200 edge ports)

APIC-M2

Cisco APIC with medium-size CPU, hard drive, and memory configurations (up to 1000 edge ports)

APIC-M3

Cisco APIC with medium-size CPU, hard drive, and memory configurations (up to 1200 edge ports)

APIC-M4

Cisco APIC with medium-size CPU, hard drive, and memory configurations (up to 1200 edge ports)


The following list includes general hardware compatibility information:

● For the supported hardware, see the Cisco Nexus 9000 ACI-Mode Switches Release Notes, Release 16.0(3).

● Contracts using matchDscp filters are only supported on switches with "EX" on the end of the switch name. For example, N9K-93108TC-EX.

● When the fabric node switch (spine or leaf) is out-of-fabric, the environmental sensor values, such as Current Temperature, Power Draw, and Power Consumption, might be reported as "N/A." A status might be reported as "Normal" even when the Current Temperature is "N/A."

● First generation switches (switches without -EX, -FX, -GX, or a later suffix in the product ID) do not support Contract filters with match type "IPv4" or "IPv6." Only match type "IP" is supported. Because of this, a contract will match both IPv4 and IPv6 traffic when the match type of "IP" is used.

The following table provides compatibility information for specific hardware:

Product ID

Description

Cisco UCS M4-based Cisco APIC

The Cisco UCS M4-based Cisco APIC and previous versions support only the 10G interface. Connecting the Cisco APIC to the Cisco ACI fabric requires a same speed interface on the Cisco ACI leaf switch. You cannot connect the Cisco APIC directly to the Cisco N9332PQ ACI leaf switch, unless you use a 40G to 10G converter (part number CVR-QSFP-SFP10G), in which case the port on the Cisco N9332PQ switch auto-negotiates to 10G without requiring any manual configuration.

Cisco UCS M5-based Cisco APIC

The Cisco UCS M5-based Cisco APIC supports dual speed 10G and 25G interfaces. Connecting the Cisco APIC to the Cisco ACI fabric requires a same speed interface on the Cisco ACI leaf switch. You cannot connect the Cisco APIC directly to the Cisco N9332PQ ACI leaf switch, unless you use a 40G to 10G converter (part number CVR-QSFP-SFP10G), in which case the port on the Cisco N9332PQ switch auto-negotiates to 10G without requiring any manual configuration.

N2348UPQ

To connect the N2348UPQ to Cisco ACI leaf switches, the following options are available:

Directly connect the 40G FEX ports on the N2348UPQ to the 40G switch ports on the Cisco ACI leaf switches

Break out the 40G FEX ports on the N2348UPQ to 4x10G ports and connect to the 10G ports on all other Cisco ACI leaf switches.

Note: A fabric uplink port cannot be used as a FEX fabric port.

N9K-C9348GC-FXP

This switch does not read SPROM information if the PSU is in a shut state. You might see an empty string in the Cisco APIC output.

N9K-C9364C-FX

Ports 49-64 do not support 1G SFPs with QSA.

N9K-C9508-FM-E

The Cisco N9K-C9508-FM-E2 and N9K-C9508-FM-E fabric modules in the mixed mode configuration are not supported on the same spine switch.

N9K-C9508-FM-E2

The Cisco N9K-C9508-FM-E2 and N9K-C9508-FM-E fabric modules in the mixed mode configuration are not supported on the same spine switch.

The locator LED enable/disable feature is supported in the GUI and not supported in the Cisco ACI NX-OS switch CLI.

N9K-C9508-FM-E2

This fabric module must be physically removed before downgrading to releases earlier than Cisco APIC 3.0(1).

N9K-X9736C-FX

The locator LED enable/disable feature is supported in the GUI and not supported in the Cisco ACI NX-OS Switch CLI.

N9K-X9736C-FX

Ports 29 to 36 do not support 1G SFPs with QSA.

Miscellaneous Compatibility Information

This release supports the following products:

Product

Supported Release

Cisco NX-OS

16.0(3)

Cisco UCS Manager

2.2(1c) or later is required for the Cisco UCS Fabric Interconnect and other components, including the BIOS, CIMC, and the adapter.

CIMC HUU ISO

Network Insights Base, Network Insights Advisor, and Network Insights for Resources

For the release information, documentation, and download links, see the Cisco Network Insights for Data Center page.

For the supported releases, see the Cisco Data Center Networking Applications Compatibility Matrix.


● This release supports the partner packages specified in the L4-L7 Compatibility List Solution Overview document.

● A known issue exists with the Safari browser and unsigned certificates, which applies when connecting to the Cisco APIC GUI. For more information, see the Cisco APIC Getting Started Guide, Release 6.0(x).

● For compatibility with Day-2 Operations apps, see the Cisco Data Center Networking Applications Compatibility Matrix.

● Cisco Nexus Dashboard Insights creates a user in Cisco APIC called cisco_SN_NI. This user is used when Nexus Dashboard Insights needs to make any changes or query any information from the Cisco APIC. In the Cisco APIC, navigate to the Audit Logs tab of the System > History page. The cisco_SN_NI user is displayed in the User column.

Related Content

See the Cisco Application Policy Infrastructure Controller (APIC) page for the documentation.

The documentation includes installation, upgrade, configuration, programming, and troubleshooting guides, technical references, release notes, and knowledge base (KB) articles, as well as other documentation. KB articles provide information about a specific use case or a specific topic.

By using the "Choose a topic" and "Choose a document type" fields of the APIC documentation website, you can narrow down the displayed documentation list to make it easier to find the desired document.

You can watch videos that demonstrate how to perform specific tasks in the Cisco APIC on the Cisco Data Center Networking YouTube channel.

Temporary licenses with an expiry date are available for evaluation and lab use purposes. They are strictly not allowed to be used in production. Use a permanent or subscription license that has been purchased through Cisco for production purposes. For more information, go to Cisco Data Center Networking Software Subscriptions.

The following table provides links to the release notes, verified scalability documentation, and new documentation:

Document

Description

Cisco Nexus 9000 ACI-Mode Switches Release Notes, Release 16.0(3)

The release notes for Cisco NX-OS for Cisco Nexus 9000 Series ACI-Mode Switches.

Verified Scalability Guide for Cisco APIC, Release 6.0(3) and Cisco Nexus 9000 Series ACI-Mode Switches, Release 16.0(3)

This guide contains the maximum verified scalability limits for Cisco Application Centric Infrastructure (ACI) parameters for Cisco APIC and Cisco Nexus 9000 Series ACI-Mode Switches.

APIC REST API Configuration Procedures

This document resides on developer.cisco.com and provides information about and procedures for using the Cisco APIC REST APIs. The new REST API procedures for this release reside only here and not in the configuration guides. However, older REST API procedures are still in the relevant configuration guides.

The Cisco Application Centric Infrastructure (ACI) is an architecture that allows the application to define the networking requirements in a programmatic way. This architecture simplifies, optimizes, and accelerates the entire application deployment lifecycle. Cisco Application Policy Infrastructure Controller (APIC) is the software, or operating system, that acts as the controller.

This document describes the features, issues, and limitations for the Cisco APIC software. For the features, issues, and limitations for the Cisco NX-OS software for the Cisco Nexus 9000 series switches, see the Cisco Nexus 9000 ACI-Mode Switches Release Notes, Release 16.0(2).

For more information about this product, see "Related Content."

Date

Description

August 2, 2023

Release 6.0(2j) became available. Added the resolved bugs for this release.

July 5, 2023

In the Related Content section, added information about the APIC REST API Configuration Procedures document on developer.cisco.com.

June 21, 2023

In the Miscellaneous Compatibility Information section, added:

March 1, 2023

Release 6.0(2h) became available.

New Software Features

Product Impact

Feature

Description

Base Functionality

Support for Cisco APIC-M4/L4 servers

This release adds support for the APIC-L4 and APIC-M4 servers.

For more information, see the Cisco APIC M4/L4 Server Installation and Service Guide.

Support for Cisco APIC virtual form factor in ESXi

You can deploy a Cisco APIC cluster wherein all the Cisco APICs in the cluster are virtual APICs. You can deploy a virtual APIC on an ESXi using the OVF template.

For more information, see the Deploying Cisco Virtual APIC Using VMware vCenter document.

Support for Cisco APIC cloud form factor using AWS

You can deploy a Cisco APIC cluster wherein all the Cisco APICs in the cluster are virtual APICs. You can deploy a virtual APIC on AWS using the CloudFormation template.

For more information, see the Deploying Cisco Virtual APIC Using AWS document.

BGP additional paths

The BGP speaker can propagate and receive multiple paths for the same prefix without the new paths replacing any previous paths. This feature allows BGP speaker peers to negotiate whether they support advertising and receiving multiple paths per prefix and advertising such paths. Cisco APIC supports only the receive functionality.

For more information, see the Cisco APIC Layer 3 Networking Configuration Guide, Release 6.0(x).

Proportional ECMP

You can use the next-hop propagate and redistribute attached host features to avoid sub-optimal routing in the Cisco ACI fabric. When these features are enabled, packet flows from a non-border leaf switch are forwarded directly to the leaf switch connected to the next-hop address. All next-hops are now used for ECMP forwarding from the hardware. In addition, Cisco ACI now redistributes ECMP paths into BGP for both directly connected next-hops and recursive next-hops.

For more information, see the Cisco APIC Layer 3 Networking Configuration Guide, Release 6.0(x).

Support for config stripe winner policies

When you configure the Layer 3 IPv4 multicast, you can now configure the config stripe winner policy for a multicast group range within a pod.

For more information, see the Cisco APIC Layer 3 Networking Configuration Guide, Release 6.0(x).

Security

First hop security (FHS) support for VMM

FHS is supported on the VMware DVS VMM domain. Ensure to enable intra EPG isolation for implementing FHS within an EPG.

For more information, see the Cisco APIC Security Configuration Guide, Release 6.0(x).

TACACS external logging for switches

You can enable TACACS external logging for switches. When enabled, the Cisco APIC collects the same types of AAA data from the switches in the chosen TACACS monitoring destination group.

For more information, see the Cisco ACI TACACS External Logging.

Performance and Scalability

Scale enhancements

Upgrade/Downgrade

Auto firmware update for Cisco APIC on discovery

When you add a new Cisco APIC to the fabric either through Product Returns & Replacements (RMA), cluster expansion, or commission, it is automatically upgraded to the same version of the existing cluster.

For more information, see the the Cisco APIC Installation and ACI Upgrade and Downgrade Guide.

Installing switch software maintenance upgrade patches without reloading

Some switch software maintenance upgrade (SMU) patches do not require you to reload the switch after you install those patches.

For more information, see the Cisco APIC Installation and ACI Upgrade and Downgrade Guide.

Interoperability

Cisco Nexus Cloud support

This release adds support for Cisco Nexus Cloud, which enables telemetry collection from the Cisco Nexus switches.

For more information, see the Cisco Nexus Cloud documentation.

Ease of Use

Troubleshooting Cisco APIC QoS Policies

You can view the QoS statistics by using the Cisco APIC GUI.

For more information, see the Cisco APIC and QoS document.

New Hardware Features

For the new hardware features, see the Cisco Nexus 9000 ACI-Mode Switches Release Notes, Release 16.0(2).

Changes in Behavior

● Scale enhancements:

◦ 10,000 VRF instances per fabric

◦ Mis-Cabling Protocol (MCP): 2,000 VLANs per interface and 12,000 logical ports (port x VLAN) per leaf switch

◦ 200 IP SLA probes per leaf switch

◦ 24 leaf switches (12 pairs) in the same L3Out

◦ 2,000 sub-interfaces (BGP, OSPF, and static)

◦ 2,000 bidirectional forwarding detection (BFD) sessions

◦ Longest Prefix Matches (LPM): 440,000 IPv4 and 100,000 IPv6 routes

● To upgrade to this release, you must perform the following procedure:

1. Download the 6.0(2) Cisco APIC image and upgrade the APIC cluster to the 6.0(2) release. Before this step is completed, DO NOT download the Cisco ACI-mode switch images to the APIC. The 6.0(2) release has both 32-bit and 64-bit switch images, but releases prior to 6.0(2) do not support 64-bit images. As a result, downloading the 64-bit images at this time might cause errors or unexpected results.

2. Download both the 32-bit and 64-bit images to the Cisco APIC. Downloading only one of the images may result in errors during the upgrade process.

3. Create the maintenance groups and trigger the upgrade procedure as usual. Cisco APIC automatically deploys the correct image to the respective switch during the upgrade process.

For more information, see the Cisco APIC Installation and ACI Upgrade and Downgrade Guide.

● On the "Interface Configuration" GUI page (Fabric > Access Policies > Interface Configuration), the node table now contains the following columns:

◦ Interface Description: The user-entered description of the interface. You can edit the description by clicking … and choosing Edit Interface Configuration.

◦ Port Direction: The direction of the port. Possible values are "uplink," "downlink," and "default." The default value is "default," which indicates that the port uses its default direction. The other values display if you converted the port from uplink to downlink or downlink to uplink.

● There is now a "Switch Configuration" GUI page (Fabric > Access Policies > Switch Configuration) that shows information about the leaf and spine switches controlled by the Cisco APIC. This page also enables you to modify a switch's configuration to create an access policy group and fabric policy group, or to remove the policy groups from 1 or more nodes. This page is similar to the "Interface Configuration" GUI page that existed previously, but is for switches.

● You can no longer use telnet to connect to the management IP address of a Cisco APIC or Cisco ACI-mode switch.

● The "Images" GUI page (Admin > Firmware > Images) now includes a "Platform Type" column, which specifies whether a switch image is 64-bit or 32-bit. This column does not apply to Cisco APIC images.

● The initial cluster set up and bootstrapping procedure has been simplified with the introduction of the APIC Cluster Bringup GUI. The APIC Cluster Bringup GUI supports virtual and physical APIC platforms.

● When you configure a custom certificate for Cisco ACI HTTPS access, you can now choose the elliptic-curve cryptography (ECC) key type. Prior to this release, RSA was the only key type.

Open Issues

Click the bug ID to access the Bug Search tool and see additional information about the bug. The "Exists In" column of the table specifies the 6.0(2) releases in which the bug exists. A bug might also exist in releases other than the 6.0(2) releases.

Bug ID

Description

Exists in

CSCvt99966

A SPAN session with the source type set to "Routed-Outside" goes down. The SPAN configuration is pushed to the anchor or non-anchor nodes, but the interfaces are not pushed due to the following fault: "Failed to configure SPAN with source SpanFL3out due to Source fvIfConn not available".

6.0(2h) and later

CSCvy40511

Traffic from an endpoint under a remote leaf switch to an external node and its attached external networks is dropped. This occurs if the external node is attached to an L3Out with a vPC and there is a redistribution configuration on the L3Out to advertise the reachability of the external nodes as direct-attached hosts.

6.0(2h) and later

CSCwd81562

A Cisco APIC that was previously part of the Cisco APIC cluster will not rejoin the cluster after the reload, decommission, and commission process.

6.0(2h) and later

CSCwd82212

There is a login denied error while importing or exporting a configuration.

6.0(2h) and later

CSCwe01680

User is not allowed to configure static route for an inband EPG which is not deployed on the current APIC.

6.0(2h) and later

CSCwe13941

Following are some of the symptoms seen because of this issue :

1. Failure to verify APIC's CIMC credentials.

2. Failure to verify the power status.

3. Failure to verify the serial number of the APIC as seen in CIMC.

These symptoms can be seen during the following workflows:

1. APIC Cluster Initial Bootstrap.

2. Adding a new APIC to the cluster - Expansion.

3. Replacing an APIC in the cluster - RMA operation.

4. Recommission of APIC following a decommission.

6.0(2h) and later

CSCwe39842

PXE boot for vmedia installation of the Cisco APIC 6.0(2) release does not work on APIC-SERVER-M2/M3/L2/L3.

6.0(2h) and later

CSCwe41446

When APICs are upgraded to the 6.0(2) release and switches are still on older releases, the upgraded standby Cisco APIC cannot join the cluster.

6.0(2h) and later

CSCwe46071

A leaf node gets stuck in bootstrap. Although bootstrap eventually gets forced completed, the node might not download the entire expected configuration, resulting in a node that is not fully functional.

6.0(2h) and later

CSCwe47966

SMU installation fails in the 6.0(2) release due to collecting the techsupport files prior to installing the SMU.

6.0(2h) and later

CSCwe50393

Using the back-to-back spine switch wizard will not display node IDs for the switch selection, and so the task in the wizard cannot be completed.

6.0(2h) and later

CSCwf54771

User configuration is missing on APICs and switches following an ungraceful reload or power outage.

6.0(2h) and later

CSCwf72015

vAPICs hosted on ESXi hosts directly connected to the fabric must see the leaf switch using LLDP. Hosts cannot be connected by an intermediate switch, including UCS Fabric Interconnects. This applies to vAPIC clusters and vAPICs used in ACI mini deployments.

6.0(2h) and later

CSCwf80352

Cisco APIC does not accept special characters "#" and ";" in then fabric name field when upgrading to the 6.0(2) release. For example, if the fabric name is "Test#03, it will be truncated to "Test", which causes prevents switches from joining the fabric after they are reloaded during the upgrade. In this example, the Cisco APIC expects the name "Test#03", but the switch is assigned the name "Test".

6.0(2h)

CSCwh01298

The SSHD daemon does not listen on the IPV6 address.

6.0(2h)

Resolved Issues

Bug ID

Description

Fixed in

CSCwf80352

Cisco APIC does not accept special characters "#" and ";" in then fabric name field when upgrading to the 6.0(2) release. For example, if the fabric name is "Test#03, it will be truncated to "Test", which causes prevents switches from joining the fabric after they are reloaded during the upgrade. In this example, the Cisco APIC expects the name "Test#03", but the switch is assigned the name "Test".

6.0(2j)

CSCwh01298

The SSHD daemon does not listen on the IPV6 address.

6.0(2j)

CSCvz72941

While performing ID recovery, id-import gets timed out. Due to this, ID recovery fails.

6.0(2h)

CSCwc66053

Preconfiguration validations for L3Outs that occur whenever a new configuration is pushed to the Cisco APIC might not get triggered.

6.0(2h)

CSCwe19885

The Nexus Insights application cannot stream the telemetry data to NDI, even though the Cisco ACI site is registered and active.

6.0(2h)

Known Issues

Click the bug ID to access the Bug Search tool and see additional information about the bug. The "Exists In" column of the table specifies the 6.0(2) releases in which the bug exists. A bug might also exist in releases other than the 6.0(2) releases.

Bug ID

Description

Exists in

CSCvj26666

The "show run leaf|spine <nodeId>" command might produce an error for scaled up configurations.

6.0(2h) and later

CSCvj90385

With a uniform distribution of EPs and traffic flows, a fabric module in slot 25 sometimes reports far less than 50% of the traffic compared to the traffic on fabric modules in non-FM25 slots.

6.0(2h) and later

CSCvm71833

Switch upgrades fail with the following error: Version not compatible.

6.0(2h) and later

CSCvq39764

When you click Restart for the Microsoft System Center Virtual Machine Manager (SCVMM) agent on a scaled-out setup, the service may stop. You can restart the agent by clicking Start.

6.0(2h) and later

CSCvq58953

One of the following symptoms occurs:

App installation/enable/disable takes a long time and does not complete.

Nomad leadership is lost. The output of the acidiag scheduler logs members command contains the following error:

Error querying node status: Unexpected response code: 500 (rpc error: No cluster leader)

6.0(2h) and later

CSCvr89603

The CRC and stomped CRC error values do not match when seen from the APIC CLI compared to the APIC GUI. This is expected behavior. The GUI values are from the history data, whereas the CLI values are from the current data.

6.0(2h) and later

CSCvs19322

Upgrading Cisco APIC from a 3.x release to a 4.x release causes Smart Licensing to lose its registration. Registering Smart Licensing again will clear the fault.

6.0(2h) and later

CSCvs77929

In the 4.x and later releases, if a firmware policy is created with different name than the maintenance policy, the firmware policy will be deleted and a new firmware policy gets created with the same name, which causes the upgrade process to fail.

6.0(2h) and later

CSCvx75380

svcredirDestmon objects get programmed in all of the leaf switches where the service L3Out is deployed, even though the service node may not be connected to some of the leaf switch.

There is no impact to traffic.

6.0(2h) and later

CSCvx78018

A remote leaf switch has momentary traffic loss for flushed endpoints as the traffic goes through the tglean path and does not directly go through the spine switch proxy path.

6.0(2h) and later

CSCvy07935

xR IP flush for all endpoints under the bridge domain subnets of the EPG being migrated to ESG. This will lead to a temporary traffic loss on remote leaf switch for all EPGs in the bridge domain. Traffic is expected to recover.

6.0(2h) and later

CSCvy10946

With the floating L3Out multipath recursive feature, if a static route with multipath is configured, not all paths are installed at the non-border leaf switch/non-anchor nodes.

6.0(2h) and later

CSCvy34357

Starting with the 6.0(2) release, the following apps built with the following non-compliant Docker versions cannot be installed nor run:

6.0(2h) and later

CSCvy45358

The file size mentioned in the status managed object for techsupport "dbgexpTechSupStatus" is wrong if the file size is larger than 4GB.

6.0(2h) and later

CSCvz06118

In the "Visibility and Troubleshooting Wizard," ERSPAN support for IPv6 traffic is not available.

6.0(2h) and later

CSCvz84444

While navigating to the last records in the various History sub tabs, it is possible to not see any results. The first, previous, next, and last buttons will then stop working too.

6.0(2h) and later

CSCvz85579

VMMmgr process experiences a very high load for an extended period of time that impacts other operations that involve it.

The process may consume excessive amount of memory and get aborted. This can be confirmed with the command "dmesg -T | grep oom_reaper" if messages such as the following are reported:

oom_reaper: reaped process 5578 (svc_ifc_vmmmgr.)

6.0(2h) and later

CSCwa78573

When the "BGP" branch is expanded in the Fabric > Inventory > POD 1 > Leaf > Protocols > BGP navigation path, the GUI freezes and you cannot navigate to any other page.

This occurs because the APIC gets large set of data in response, which cannot be handled by the browser for parts of the GUI that do not have the pagination.

6.0(2h) and later

CSCwe18213

The logical switch created for the EPG remains in the NSX-T manager after the EPG is disassociated from the domain, or the logical switch does not get created when the EPG is associated with the domain.

6.0(2h) and later

N/A

If you are upgrading to Cisco APIC release 4.2(6o), 4.2(7l), 5.2(1g), or later, ensure that any VLAN encapsulation blocks that you are explicitly using for leaf switch front panel VLAN programming are set as "external (on the wire)." If these VLAN encapsulation blocks are instead set to "internal," the upgrade causes the front panel port VLAN to be removed, which can result in a datapath outage.

6.0(2h) and later

N/A

Beginning in Cisco APIC release 4.1(1), the IP SLA monitor policy validates the IP SLA port value. Because of the validation, when TCP is configured as the IP SLA type, Cisco APIC no longer accepts an IP SLA port value of 0, which was allowed in previous releases. An IP SLA monitor policy from a previous release that has an IP SLA port value of 0 becomes invalid if the Cisco APIC is upgraded to release 4.1(1) or later. This results in a failure for the configuration import or snapshot rollback.

The workaround is to configure a non-zero IP SLA port value before upgrading the Cisco APIC, and use the snapshot and configuration export that was taken after the IP SLA port change.

6.0(2h) and later

N/A

If you use the REST API to upgrade an app, you must create a new firmware.OSource to be able to download a new app image.

6.0(2h) and later

N/A

In a multipod configuration, before you make any changes to a spine switch, ensure that there is at least one operationally "up" external link that is participating in the multipod topology. Failure to do so could bring down the multipod connectivity. For more information about multipod, see the Cisco Application Centric Infrastructure Fundamentals document and the Cisco APIC Getting Started Guide.

6.0(2h) and later

N/A

With a non-english SCVMM 2012 R2 or SCVMM 2016 setup and where the virtual machine names are specified in non-english characters, if the host is removed and re-added to the host group, the GUID for all the virtual machines under that host changes. Therefore, if a user has created a micro segmentation endpoint group using "VM name" attribute specifying the GUID of respective virtual machine, then that micro segmentation endpoint group will not work if the host (hosting the virtual machines) is removed and re-added to the host group, as the GUID for all the virtual machines would have changed. This does not happen if the virtual name has name specified in all english characters.

6.0(2h) and later

N/A

A query of a configurable policy that does not have a subscription goes to the policy distributor. However, a query of a configurable policy that has a subscription goes to the policy manager. As a result, if the policy propagation from the policy distributor to the policy manager takes a prolonged amount of time, then in such cases the query with the subscription might not return the policy simply because it has not reached policy manager yet.

6.0(2h) and later

N/A

When there are silent hosts across sites, ARP glean messages might not be forwarded to remote sites if a leaf switch without -EX or a later designation in the product ID happens to be in the transit path and the VRF is deployed on that leaf switch, the switch does not forward the ARP glean packet back into the fabric to reach the remote site. This issue is specific to transit leaf switches without -EX or a later designation in the product ID and does not affect leaf switches that have -EX or a later designation in the product ID. This issue breaks the capability of discovering silent hosts.

6.0(2h) and later

N/A

Typically, faults are generally raised based on the presence of the BGP route target profile under the VRF table. However, if a BGP route target profile is configured without actual route targets (that is, the profile has empty policies), a fault will not be raised in this situation.

6.0(2h) and later

N/A

MPLS interface statistics shown in a switch's CLI get cleared after an admin or operational down event.

6.0(2h) and later

N/A

MPLS interface statistics in a switch's CLI are reported every 10 seconds. If, for example, an interface goes down 3 seconds after the collection of the statistics, the CLI reports only 3 seconds of the statistics and clears all of the other statistics.

6.0(2h) and later

Virtualization Compatibility Information

This section lists virtualization compatibility information for the Cisco APIC software.

● For a table that shows the supported virtualization products, see the ACI Virtualization Compatibility Matrix.

● For information about Cisco APIC compatibility with Cisco UCS Director, see the appropriate Cisco UCS Director Compatibility Matrix document.

● If you use Microsoft vSwitch and want to downgrade to Cisco APIC Release 2.3(1) from a later release, you first must delete any microsegment EPGs configured with the Match All filter.

● This release supports the following additional virtualization products:

Product

Supported Release

Information Location

Microsoft Hyper-V

2016 Update Rollup 1, 2, 2.1, and 3

N/A

VMM Integration and VMware Distributed Virtual Switch (DVS)

6.5, 6.7, and 7.0

Cisco ACI Virtualization Guide, Release 6.0(x)

Hardware Compatibility Information

This release supports the following Cisco APIC servers:

Product ID

Description

APIC-L2

Cisco APIC with large CPU, hard drive, and memory configurations (more than 1000 edge ports)

APIC-L3

Cisco APIC with large CPU, hard drive, and memory configurations (more than 1200 edge ports)

APIC-L4

Cisco APIC with large CPU, hard drive, and memory configurations (more than 1200 edge ports)

APIC-M2

Cisco APIC with medium-size CPU, hard drive, and memory configurations (up to 1000 edge ports)

APIC-M3

Cisco APIC with medium-size CPU, hard drive, and memory configurations (up to 1200 edge ports)

APIC-M4

Cisco APIC with medium-size CPU, hard drive, and memory configurations (up to 1200 edge ports)


The following list includes general hardware compatibility information:

● For the supported hardware, see the Cisco Nexus 9000 ACI-Mode Switches Release Notes, Release 16.0(2).

● Contracts using matchDscp filters are only supported on switches with "EX" on the end of the switch name. For example, N9K-93108TC-EX.

● When the fabric node switch (spine or leaf) is out-of-fabric, the environmental sensor values, such as Current Temperature, Power Draw, and Power Consumption, might be reported as "N/A." A status might be reported as "Normal" even when the Current Temperature is "N/A."

● First generation switches (switches without -EX, -FX, -GX, or a later suffix in the product ID) do not support Contract filters with match type "IPv4" or "IPv6." Only match type "IP" is supported. Because of this, a contract will match both IPv4 and IPv6 traffic when the match type of "IP" is used.

The following table provides compatibility information for specific hardware:

Product ID

Description

Cisco UCS M4-based Cisco APIC

The Cisco UCS M4-based Cisco APIC and previous versions support only the 10G interface. Connecting the Cisco APIC to the Cisco ACI fabric requires a same speed interface on the Cisco ACI leaf switch. You cannot connect the Cisco APIC directly to the Cisco N9332PQ ACI leaf switch, unless you use a 40G to 10G converter (part number CVR-QSFP-SFP10G), in which case the port on the Cisco N9332PQ switch auto-negotiates to 10G without requiring any manual configuration.

Cisco UCS M5-based Cisco APIC

The Cisco UCS M5-based Cisco APIC supports dual speed 10G and 25G interfaces. Connecting the Cisco APIC to the Cisco ACI fabric requires a same speed interface on the Cisco ACI leaf switch. You cannot connect the Cisco APIC directly to the Cisco N9332PQ ACI leaf switch, unless you use a 40G to 10G converter (part number CVR-QSFP-SFP10G), in which case the port on the Cisco N9332PQ switch auto-negotiates to 10G without requiring any manual configuration.

N2348UPQ

To connect the N2348UPQ to Cisco ACI leaf switches, the following options are available:

Directly connect the 40G FEX ports on the N2348UPQ to the 40G switch ports on the Cisco ACI leaf switches

Break out the 40G FEX ports on the N2348UPQ to 4x10G ports and connect to the 10G ports on all other Cisco ACI leaf switches.

Note: A fabric uplink port cannot be used as a FEX fabric port.

N9K-C9348GC-FXP

This switch does not read SPROM information if the PSU is in a shut state. You might see an empty string in the Cisco APIC output.

N9K-C9364C-FX

Ports 49-64 do not support 1G SFPs with QSA.

N9K-C9508-FM-E

The Cisco N9K-C9508-FM-E2 and N9K-C9508-FM-E fabric modules in the mixed mode configuration are not supported on the same spine switch.

N9K-C9508-FM-E2

The Cisco N9K-C9508-FM-E2 and N9K-C9508-FM-E fabric modules in the mixed mode configuration are not supported on the same spine switch.

The locator LED enable/disable feature is supported in the GUI and not supported in the Cisco ACI NX-OS switch CLI.

N9K-C9508-FM-E2

This fabric module must be physically removed before downgrading to releases earlier than Cisco APIC 3.0(1).

N9K-X9736C-FX

The locator LED enable/disable feature is supported in the GUI and not supported in the Cisco ACI NX-OS Switch CLI.

N9K-X9736C-FX

Ports 29 to 36 do not support 1G SFPs with QSA.

Miscellaneous Compatibility Information

This release supports the following products:

Product

Supported Release

Cisco NX-OS

16.0(2)

Cisco UCS Manager

2.2(1c) or later is required for the Cisco UCS Fabric Interconnect and other components, including the BIOS, CIMC, and the adapter.

CIMC HUU ISO

Network Insights Base, Network Insights Advisor, and Network Insights for Resources

For the release information, documentation, and download links, see the Cisco Network Insights for Data Center page.

For the supported releases, see the Cisco Data Center Networking Applications Compatibility Matrix.


● This release supports the partner packages specified in the L4-L7 Compatibility List Solution Overview document.

● A known issue exists with the Safari browser and unsigned certificates, which applies when connecting to the Cisco APIC GUI. For more information, see the Cisco APIC Getting Started Guide, Release 6.0(x).

● For compatibility with Day-2 Operations apps, see the Cisco Data Center Networking Applications Compatibility Matrix.

● Cisco Nexus Dashboard Insights creates a user in Cisco APIC called cisco_SN_NI. This user is used when Nexus Dashboard Insights needs to make any changes or query any information from the Cisco APIC. In the Cisco APIC, navigate to the Audit Logs tab of the System > History page. The cisco_SN_NI user is displayed in the User column.

Related Content

See the Cisco Application Policy Infrastructure Controller (APIC) page for the documentation.

The documentation includes installation, upgrade, configuration, programming, and troubleshooting guides, technical references, release notes, and knowledge base (KB) articles, as well as other documentation. KB articles provide information about a specific use case or a specific topic.

By using the "Choose a topic" and "Choose a document type" fields of the APIC documentation website, you can narrow down the displayed documentation list to make it easier to find the desired document.

You can watch videos that demonstrate how to perform specific tasks in the Cisco APIC on the Cisco Data Center Networking YouTube channel.

Temporary licenses with an expiry date are available for evaluation and lab use purposes. They are strictly not allowed to be used in production. Use a permanent or subscription license that has been purchased through Cisco for production purposes. For more information, go to Cisco Data Center Networking Software Subscriptions.

The following table provides links to the release notes, verified scalability documentation, and new documentation:

Document

Description

Cisco Nexus 9000 ACI-Mode Switches Release Notes, Release 16.0(2)

The release notes for Cisco NX-OS for Cisco Nexus 9000 Series ACI-Mode Switches.

Verified Scalability Guide for Cisco APIC, Release 6.0(2) and Cisco Nexus 9000 Series ACI-Mode Switches, Release 16.0(2)

This guide contains the maximum verified scalability limits for Cisco Application Centric Infrastructure (ACI) parameters for Cisco APIC and Cisco Nexus 9000 Series ACI-Mode Switches.

APIC REST API Configuration Procedures

This document resides on developer.cisco.com and provides information about and procedures for using the Cisco APIC REST APIs. The new REST API procedures for this release reside only here and not in the configuration guides. However, older REST API procedures are still in the relevant configuration guides.


Introduction

The Cisco Application Centric Infrastructure (ACI) is an architecture that allows the application to define the networking requirements in a programmatic way. This architecture simplifies, optimizes, and accelerates the entire application deployment lifecycle. Cisco Application Policy Infrastructure Controller (APIC) is the software, or operating system, that acts as the controller.

This document describes the features, issues, and limitations for the Cisco APIC software. For the features, issues, and limitations for the Cisco NX-OS software for the Cisco Nexus 9000 series switches, see the Cisco Nexus 9000 ACI-Mode Switches Release Notes, Release 16.0(1).

For more information about this product, see "Related Content."

Date

Description

January 11, 2023

In the Hardware Compatibility Information section, removed APIC-M1 and APIC-L1. The last date of support was October 31, 2021.

November 29, 2022

In the Known Issues section, added:

November 18, 2022

In the Open Issues section, added bug CSCwc66053.

October 14, 2022

Release 6.0(1j) became available. Added the resolved bugs for this release.

August 1, 2022

In the Miscellaneous Compatibility Information section, added:

July 13, 2022

Release 6.0(1g) became available.

New Software Features

Product Impact

Feature

Description

Base Functionality

BGP autonomous system (AS) enhancements

Cisco APIC now supports the Remove Private AS option to remove private autonomous system numbers from the AS_path in an eBGP route, and supports the AS-Path match clause while creating a BGP per-peer route-map.

For more information, see the Cisco APIC Layer 3 Networking Configuration Guide, Release 6.0(x).

Breakout port support with the Cisco N9K-C93600CD-GX and N9K-C9316D-GX switches

Dynamic and auto breakout ports are now supported with the Cisco N9K-C93600CD-GX and N9K-C9316D-GX switches.

For more information, see the Cisco APIC Layer 2 Networking Configuration Guide, Release 6.0(x).

Support for BFD on secondary IPv4/IPv6 subnets

Bidirectional Forwarding Detection (BFD) is now supported for static routes that are reachable using secondary IPv4/IPv6 subnets that are configured on routed interfaces. This feature was originally introduced in the 5.2(4) release and is now available in the 6.0 releases.

For more information, see the Cisco APIC Layer 3 Networking Configuration Guide, Release 6.0(x).

Support for PTP G.8275.1 on remote leaf switch peer links and on vPCs

You can now use the PTP Telecom profile (G.8275.1) on virtual port channels (vPCs) and on remote leaf switch peer links.

For more information, see the Cisco APIC System Management Configuration Guide, Release 6.0(x).

Ease of Use

SPAN extended filter entries

You can now configure extended filter entries for filter groups in a SPAN session, which enable you to monitor traffic originating from access nodes in leaf nodes.

For more information, see the Cisco APIC Basic Configuration Guide, Release 6.0(x).

Support for remote pools with a subnet mask of up to /28

Remote leaf switches now support remote pools with a subnet mask of up to /28. In prior releases, remote leaf switches supported remote pools with a subnet mask of up to /24.

For more information, see the Cisco APIC Basic Configuration Guide, Release 6.0(x).

Weight-based symmetric policy-based redirect (PBR)

In weight-based symmetric PBR, you can set weights for a PBR destination (service node) based on the capacity of the service node, and traffic is load balanced based on the set weights.

For more information, see the Cisco APIC Layer 4 to Layer 7 Services Deployment Guide, Release 6.0(x).

Interoperability

Support for SyncE on vPCs and on remote leaf switch peer links

You can now use SyncE on vPCs and on remote leaf switch peer links.

For more information, see the Cisco APIC System Management Configuration Guide, Release 6.0(x).

Security

Cisco Nexus 9000 switch secure erase

Cisco Nexus 9000 switches utilize persistent storage to maintain system software images, switch configuration, software logs, and operational history. Each of these areas can contain user-specific information such as details on network architecture and design, and potential target vectors for would-be attackers. The secure erase feature enables you comprehensively to erase this information, which you can do when you return a switch with return merchandise authorization (RMA), upgrade or replace a switch, or decommission a system that has reached its end-of-life.

For more information, see the Cisco APIC Getting Started Guide, Release 6.0(x).

Support for a user group map rule for SAML and OAuth 2

Authentication by an external server for SAML and OAuth 2 is based on user group map rule information, in addition to the standard CiscoAVpair-based authentication.

For more information, see the Cisco APIC Security Configuration Guide, Release 6.0(x).

Transport Layer Security version 1.3 support

Transport Layer Security (TLS) version 1.3 is now supported. This feature was originally introduced in the 5.2(5) release and is now available in the 6.0 releases.

New Hardware Features

For the new hardware features, see the Cisco Nexus 9000 ACI-Mode Switches Release Notes, Release 16.0(1).

Changes in Behavior

● Beginning with this release, the online help has been removed from the GUI. You can instead view the documentation by clicking the ? in the upper right of any GUI screen and choosing Help. The Help Center dialog that appears contains links to various Cisco APIC documentation.

● In the Cisco APIC GUI, on the "Interface Configuration" page (Fabric > Access Policies > Interface Configuration), the node table now contains the following columns:

◦ Pod: The ID of the pod to which the node belongs.

◦ Interface: The ID of interface.

◦ Node: The ID of the node.

◦ Port Type: The type of the port on the node (access or fabric).

◦ Admin State: The administrative state of the node.

◦ Port Mode: The mode of the port on the node (individual, port channel, or virtual port channel, fabric leaf port, fabric spine port, spine port, or FEX connected).

◦ Policy Group: The policy group to which the node belongs.

◦ Interface Description: An optional description of the interface.

● In the Cisco APIC GUI, On the "Welcome to Access Policies" page (Fabric > Access Policies > Quick Start), work pane now contains the following choices:

◦ Configure Interfaces: Used to configure the interfaces on a node.

◦ Breakout: Used to configure breakout ports on a node.

◦ Create a SPAN Source and Destination: Used to create a SPAN source group.

◦ Convert Interfaces: Used to convert interfaces on a node to uplink or downlink ports.

◦ Fabric Extender: Used to connect a node to a fabric extender (FEX).

● In the Cisco APIC GUI, the Admin > AAA pages have been modified. The Work panes of Authentication, Security, and Users have been enhanced for better functionality and ease of use.

● The hash result of symmetric EtherChannel could be different because of the fix for issue CSCwb93059. This change could cause asymmetric flow. For example, if the ingress leaf switch for the incoming traffic uses a prior release and the ingress leaf switch for the return traffic uses this release or later, the switches get different hash results for the incoming and return traffic.

● Transport Layer Security (TLS) version 1.0 and 1.1 are no longer supported.

● A leaf switch now supports only up to 56 uplinks. Prior to the 6.0(1) release, a leaf switch supported more than 56 uplinks. If your configuration has more than 56 uplinks, before you upgrade to the 6.0(1) release, reduce the number of uplinks to 56 or less otherwise you will lose any uplinks that are more than 56. If you upgrade to the 6.0(1) release and have more than 56 uplinks, Cisco APIC raises a fault similar to the following example:

[F2981][raised][portp-policy-limit-exceeded][warning][sys/ops/slot-lcslot-1/portpol-21/fault-F2981] PortP policy limit exceeded

Open Issues

Click the bug ID to access the Bug Search tool and see additional information about the bug. The "Exists In" column of the table specifies the 6.0(1) releases in which the bug exists. A bug might also exist in releases other than the 6.0(1) releases.

Bug ID

Description

Exists in

CSCvt99966

A SPAN session with the source type set to "Routed-Outside" goes down. The SPAN configuration is pushed to the anchor or non-anchor nodes, but the interfaces are not pushed due to the following fault: "Failed to configure SPAN with source SpanFL3out due to Source fvIfConn not available".

6.0(1g) and later

CSCvy40511

Traffic from an endpoint under a remote leaf switch to an external node and its attached external networks is dropped. This occurs if the external node is attached to an L3Out with a vPC and there is a redistribution configuration on the L3Out to advertise the reachability of the external nodes as direct-attached hosts.

6.0(1g) and later

CSCvz72941

While performing ID recovery, id-import gets timed out. Due to this, ID recovery fails.

6.0(1g) and later

CSCwc66053

Preconfiguration validations for L3Outs that occur whenever a new configuration is pushed to the Cisco APIC might not get triggered.

6.0(1g) and later

CSCwe19885

The Nexus Insights application cannot stream the telemetry data to NDI, even though the Cisco ACI site is registered and active.

6.0(1g) and later

CSCwf54771

User configuration is missing on APICs and switches following an ungraceful reload or power outage.

6.0(1g) and later

CSCwc36551

There are alarms raised on Cisco Nexus Dashboard that report memory leakage in the svc_ifc_ae process on the Cisco APICs.

On the Cisco Nexus Dashboard side:

'anomalyType': 'high_threshold', 'reason': '[ae] : mem usage above threshold (Usage: 5199.82 MB, High-Threshold: 2560.00 MB)

'mnemonicDescription': 'Memory usage above threshold', 'mnemonicNum': 100481, 'mnemonicTitle': 'ENVIRONMENTAL_MEMORY_HIGH_THRESHOLD',

6.0(1g)

Resolved Issues

Bug ID

Description

Fixed in

CSCwc36551

There are alarms raised on Cisco Nexus Dashboard that report memory leakage in the svc_ifc_ae process on the Cisco APICs.

On the Cisco Nexus Dashboard side:

'anomalyType': 'high_threshold', 'reason': '[ae] : mem usage above threshold (Usage: 5199.82 MB, High-Threshold: 2560.00 MB)

'mnemonicDescription': 'Memory usage above threshold', 'mnemonicNum': 100481, 'mnemonicTitle': 'ENVIRONMENTAL_MEMORY_HIGH_THRESHOLD',

6.0(1j)

CSCvy00746

A breakout parent port shows in the drop-down list for the SPAN source even after the port is broken out.

6.0(1g)

CSCvz83636

For a health record query using the last page and a time range, the GUI displays some health records with a creation time that are beyond the time range (such as 24h).

6.0(1g)

CSCwa53478

After migrating a VM between two hosts using VMware vMotion, EPG does not get deployed on the target leaf node. When affected, the fvIfConn managed object corresponding to the missing EPG can be seen on APIC, but it would be missing from the target leaf node when queried.

6.0(1g)

CSCwa58061

When there are more than 40 objects in the tree and you double click on an object in the BGP Peer table, then the tree does not expand because the tree does not have pagination. The APIC tries to load all objects in one query, which is drastically slows the GUI.

6.0(1g)

CSCwa78740

When HBR is enabled on a source EPG's bridge domain and the subnet is configured with the private scope (advertise externally = FALSE), if there is a shared service EPG contract with an L3Out, the L3Out will not publish the subnet or the corresponding /32 host routes because of this private scope.

In this scenario, if there is also an explicit ESG leakRoute configured for the same subnet across those VRF instances, the leakRoute is faulted because the route is already shared with an EPG contract, and the leakRoute is installed in the hardware along with a pcTag, then the leakRoute should not be processed and any flags under it should not be considered.

But, if this explicit leakRoute has a public scope, the /32 host routes are still published externally out of the L3Out, which should not happen as the leakRoute itself is faulted and bridge domain subnet scope is private.

6.0(1g)

CSCwa90058

When a VRF-level subnet <fvRtSummSubnet> and instP-level subnet <l3extSubnet> with a summary policy is configured for an overlapping subnet, the routes will get summarized by the configuration that was added first. But, the fault on the configuration that was added last will not be shown in the Cisco APIC GUI.

6.0(1g)

CSCwa95297

When a VRF-level subnet, fvRtSummSubnet, exists with a summary policy and an instP level subnet, <l3extSubnet>, with the same subnet as the VRF-level subnet is associated with summary policy, then there won't be any fault seen on the Cisco APIC. The summarization will be done according to the VRF-level subnet <fvRtSummSubnet>.

6.0(1g)

CSCwa99045

VMM domain attachments of floating SVIs configured for dual stack with the same encapsulation and the same VMM domain attachments are not being cleaned up after downgrading from 6.0(1) to an earlier release.

6.0(1g)

CSCwb00781

Importing the routing table of a remote site carries the wrong autonomous system number (ASN).

6.0(1g)

Known Issues

Click the bug ID to access the Bug Search tool and see additional information about the bug. The "Exists In" column of the table specifies the 6.0(1) releases in which the bug exists. A bug might also exist in releases other than the 6.0(1) releases.

Bug ID

Description

Exists in

CSCvj26666

The "show run leaf|spine <nodeId>" command might produce an error for scaled up configurations.

6.0(1g) and later

CSCvj90385

With a uniform distribution of EPs and traffic flows, a fabric module in slot 25 sometimes reports far less than 50% of the traffic compared to the traffic on fabric modules in non-FM25 slots.

6.0(1g) and later

CSCvm71833

Switch upgrades fail with the following error: Version not compatible.

6.0(1g) and later

CSCvq39764

When you click Restart for the Microsoft System Center Virtual Machine Manager (SCVMM) agent on a scaled-out setup, the service may stop. You can restart the agent by clicking Start.

6.0(1g) and later

CSCvq58953

One of the following symptoms occurs:

App installation/enable/disable takes a long time and does not complete.

Nomad leadership is lost. The output of the acidiag scheduler logs members command contains the following error:

Error querying node status: Unexpected response code: 500 (rpc error: No cluster leader)

6.0(1g) and later

CSCvr89603

The CRC and stomped CRC error values do not match when seen from the APIC CLI compared to the APIC GUI. This is expected behavior. The GUI values are from the history data, whereas the CLI values are from the current data.

6.0(1g) and later

CSCvs19322

Upgrading Cisco APIC from a 3.x release to a 4.x release causes Smart Licensing to lose its registration. Registering Smart Licensing again will clear the fault.

6.0(1g) and later

CSCvs77929

In the 4.x and later releases, if a firmware policy is created with different name than the maintenance policy, the firmware policy will be deleted and a new firmware policy gets created with the same name, which causes the upgrade process to fail.

6.0(1g) and later

CSCvx75380

svcredirDestmon objects get programmed in all of the leaf switches where the service L3Out is deployed, even though the service node may not be connected to some of the leaf switch.

There is no impact to traffic.

6.0(1g) and later

CSCvx78018

A remote leaf switch has momentary traffic loss for flushed endpoints as the traffic goes through the tglean path and does not directly go through the spine switch proxy path.

6.0(1g) and later

CSCvy07935

xR IP flush for all endpoints under the bridge domain subnets of the EPG being migrated to ESG. This will lead to a temporary traffic loss on remote leaf switch for all EPGs in the bridge domain. Traffic is expected to recover.

6.0(1g) and later

CSCvy10946

With the floating L3Out multipath recursive feature, if a static route with multipath is configured, not all paths are installed at the non-border leaf switch/non-anchor nodes.

6.0(1g) and later

CSCvy34357

Starting with the 6.0(1) release, the following apps built with the following non-compliant Docker versions cannot be installed nor run:

· ConnectivityCompliance 1.2

· SevOneAciMonitor 1.0

6.0(1g) and later

CSCvy45358

The file size mentioned in the status managed object for techsupport "dbgexpTechSupStatus" is wrong if the file size is larger than 4GB.

6.0(1g) and later

CSCvz06118

In the "Visibility and Troubleshooting Wizard," ERSPAN support for IPv6 traffic is not available.

6.0(1g) and later

CSCvz84444

While navigating to the last records in the various History sub tabs, it is possible to not see any results. The first, previous, next, and last buttons will then stop working too.

6.0(1g) and later

CSCvz85579

VMMmgr process experiences a very high load for an extended period of time that impacts other operations that involve it.

The process may consume excessive amount of memory and get aborted. This can be confirmed with the command "dmesg -T | grep oom_reaper" if messages such as the following are reported:

oom_reaper: reaped process 5578 (svc_ifc_vmmmgr.)

6.0(1g) and later

CSCwa78573

When the "BGP" branch is expanded in the Fabric > Inventory > POD 1 > Leaf > Protocols > BGP navigation path, the GUI freezes and you cannot navigate to any other page.

This occurs because the APIC gets large set of data in response, which cannot be handled by the browser for parts of the GUI that do not have the pagination.

6.0(1g) and later

N/A

If you are upgrading to Cisco APIC release 4.2(6o), 4.2(7l), 5.2(1g), or later, ensure that any VLAN encapsulation blocks that you are explicitly using for leaf switch front panel VLAN programming are set as "external (on the wire)." If these VLAN encapsulation blocks are instead set to "internal," the upgrade causes the front panel port VLAN to be removed, which can result in a datapath outage.

6.0(1g) and later

N/A

Beginning in Cisco APIC release 4.1(1), the IP SLA monitor policy validates the IP SLA port value. Because of the validation, when TCP is configured as the IP SLA type, Cisco APIC no longer accepts an IP SLA port value of 0, which was allowed in previous releases. An IP SLA monitor policy from a previous release that has an IP SLA port value of 0 becomes invalid if the Cisco APIC is upgraded to release 4.1(1) or later. This results in a failure for the configuration import or snapshot rollback.

The workaround is to configure a non-zero IP SLA port value before upgrading the Cisco APIC, and use the snapshot and configuration export that was taken after the IP SLA port change.

6.0(1g) and later

N/A

If you use the REST API to upgrade an app, you must create a new firmware.OSource to be able to download a new app image.

6.0(1g) and later

N/A

In a multipod configuration, before you make any changes to a spine switch, ensure that there is at least one operationally "up" external link that is participating in the multipod topology. Failure to do so could bring down the multipod connectivity. For more information about multipod, see the Cisco Application Centric Infrastructure Fundamentals document and the Cisco APIC Getting Started Guide.

6.0(1g) and later

N/A

With a non-english SCVMM 2012 R2 or SCVMM 2016 setup and where the virtual machine names are specified in non-english characters, if the host is removed and re-added to the host group, the GUID for all the virtual machines under that host changes. Therefore, if a user has created a micro segmentation endpoint group using "VM name" attribute specifying the GUID of respective virtual machine, then that micro segmentation endpoint group will not work if the host (hosting the virtual machines) is removed and re-added to the host group, as the GUID for all the virtual machines would have changed. This does not happen if the virtual name has name specified in all english characters.

6.0(1g) and later

N/A

A query of a configurable policy that does not have a subscription goes to the policy distributor. However, a query of a configurable policy that has a subscription goes to the policy manager. As a result, if the policy propagation from the policy distributor to the policy manager takes a prolonged amount of time, then in such cases the query with the subscription might not return the policy simply because it has not reached policy manager yet.

6.0(1g) and later

N/A

When there are silent hosts across sites, ARP glean messages might not be forwarded to remote sites if a leaf switch without -EX or a later designation in the product ID happens to be in the transit path and the VRF is deployed on that leaf switch, the switch does not forward the ARP glean packet back into the fabric to reach the remote site. This issue is specific to transit leaf switches without -EX or a later designation in the product ID and does not affect leaf switches that have -EX or a later designation in the product ID. This issue breaks the capability of discovering silent hosts.

6.0(1g) and later

N/A

Typically, faults are generally raised based on the presence of the BGP route target profile under the VRF table. However, if a BGP route target profile is configured without actual route targets (that is, the profile has empty policies), a fault will not be raised in this situation.

6.0(1g) and later

N/A

MPLS interface statistics shown in a switch's CLI get cleared after an admin or operational down event.

6.0(1g) and later

N/A

MPLS interface statistics in a switch's CLI are reported every 10 seconds. If, for example, an interface goes down 3 seconds after the collection of the statistics, the CLI reports only 3 seconds of the statistics and clears all of the other statistics.

6.0(1g) and later

Virtualization Compatibility Information

This section lists virtualization compatibility information for the Cisco APIC software.

· For a table that shows the supported virtualization products, see the ACI Virtualization Compatibility Matrix.

· For information about Cisco APIC compatibility with Cisco UCS Director, see the appropriate Cisco UCS Director Compatibility Matrix document.

· If you use Microsoft vSwitch and want to downgrade to Cisco APIC Release 2.3(1) from a later release, you first must delete any microsegment EPGs configured with the Match All filter.

· This release supports the following additional virtualization products:

Product

Supported Release

Information Location

Microsoft Hyper-V

2016 Update Rollup 1, 2, 2.1, and 3

N/A

VMM Integration and VMware Distributed Virtual Switch (DVS)

6.5, 6.7, and 7.0

Cisco ACI Virtualization Guide, Release 6.0(x)

Hardware Compatibility Information

This release supports the following Cisco APIC servers:

Product ID

Description

APIC-L2

Cisco APIC with large CPU, hard drive, and memory configurations (more than 1000 edge ports)

APIC-L3

Cisco APIC with large CPU, hard drive, and memory configurations (more than 1200 edge ports)

APIC-M2

Cisco APIC with medium-size CPU, hard drive, and memory configurations (up to 1000 edge ports)

APIC-M3

Cisco APIC with medium-size CPU, hard drive, and memory configurations (up to 1200 edge ports)


The following list includes general hardware compatibility information:

· For the supported hardware, see the Cisco Nexus 9000 ACI-Mode Switches Release Notes, Release 16.0(1).

· Contracts using matchDscp filters are only supported on switches with "EX" on the end of the switch name. For example, N9K-93108TC-EX.

· When the fabric node switch (spine or leaf) is out-of-fabric, the environmental sensor values, such as Current Temperature, Power Draw, and Power Consumption, might be reported as "N/A." A status might be reported as "Normal" even when the Current Temperature is "N/A."

· First generation switches (switches without -EX, -FX, -GX, or a later suffix in the product ID) do not support Contract filters with match type "IPv4" or "IPv6." Only match type "IP" is supported. Because of this, a contract will match both IPv4 and IPv6 traffic when the match type of "IP" is used.

The following table provides compatibility information for specific hardware:

Product ID

Description

Cisco UCS M4-based Cisco APIC

The Cisco UCS M4-based Cisco APIC and previous versions support only the 10G interface. Connecting the Cisco APIC to the Cisco ACI fabric requires a same speed interface on the Cisco ACI leaf switch. You cannot connect the Cisco APIC directly to the Cisco N9332PQ ACI leaf switch, unless you use a 40G to 10G converter (part number CVR-QSFP-SFP10G), in which case the port on the Cisco N9332PQ switch auto-negotiates to 10G without requiring any manual configuration.

Cisco UCS M5-based Cisco APIC

The Cisco UCS M5-based Cisco APIC supports dual speed 10G and 25G interfaces. Connecting the Cisco APIC to the Cisco ACI fabric requires a same speed interface on the Cisco ACI leaf switch. You cannot connect the Cisco APIC directly to the Cisco N9332PQ ACI leaf switch, unless you use a 40G to 10G converter (part number CVR-QSFP-SFP10G), in which case the port on the Cisco N9332PQ switch auto-negotiates to 10G without requiring any manual configuration.

N2348UPQ

To connect the N2348UPQ to Cisco ACI leaf switches, the following options are available:

Directly connect the 40G FEX ports on the N2348UPQ to the 40G switch ports on the Cisco ACI leaf switches

Break out the 40G FEX ports on the N2348UPQ to 4x10G ports and connect to the 10G ports on all other Cisco ACI leaf switches.

Note: A fabric uplink port cannot be used as a FEX fabric port.

N9K-C9348GC-FXP

This switch does not read SPROM information if the PSU is in a shut state. You might see an empty string in the Cisco APIC output.

N9K-C9364C-FX

Ports 49-64 do not support 1G SFPs with QSA.

N9K-C9508-FM-E

The Cisco N9K-C9508-FM-E2 and N9K-C9508-FM-E fabric modules in the mixed mode configuration are not supported on the same spine switch.

N9K-C9508-FM-E2

The Cisco N9K-C9508-FM-E2 and N9K-C9508-FM-E fabric modules in the mixed mode configuration are not supported on the same spine switch.

The locator LED enable/disable feature is supported in the GUI and not supported in the Cisco ACI NX-OS switch CLI.

N9K-C9508-FM-E2

This fabric module must be physically removed before downgrading to releases earlier than Cisco APIC 3.0(1).

N9K-X9736C-FX

The locator LED enable/disable feature is supported in the GUI and not supported in the Cisco ACI NX-OS Switch CLI.

N9K-X9736C-FX

Ports 29 to 36 do not support 1G SFPs with QSA.

Miscellaneous Compatibility Information

This release supports the following products:

Product

Supported Release

Cisco NX-OS

16.0(1)

Cisco UCS Manager

2.2(1c) or later is required for the Cisco UCS Fabric Interconnect and other components, including the BIOS, CIMC, and the adapter.

CIMC HUU ISO

Network Insights Base, Network Insights Advisor, and Network Insights for Resources

For the release information, documentation, and download links, see the Cisco Network Insights for Data Center page.

For the supported releases, see the Cisco Data Center Networking Applications Compatibility Matrix.


· This release supports the partner packages specified in the L4-L7 Compatibility List Solution Overview document.

· A known issue exists with the Safari browser and unsigned certificates, which applies when connecting to the Cisco APIC GUI. For more information, see the Cisco APIC Getting Started Guide, Release 6.0(x).

· For compatibility with Day-2 Operations apps, see the Cisco Data Center Networking Applications Compatibility Matrix.

· Cisco Nexus Dashboard Insights creates a user in Cisco APIC called cisco_SN_NI. This user is used when Nexus Dashboard Insights needs to make any changes or query any information from the Cisco APIC. In the Cisco APIC, navigate to the Audit Logs tab of the System > History page. The cisco_SN_NI user is displayed in the User column.

Related Content

See the Cisco Application Policy Infrastructure Controller (APIC) page for the documentation.

The documentation includes installation, upgrade, configuration, programming, and troubleshooting guides, technical references, release notes, and knowledge base (KB) articles, as well as other documentation. KB articles provide information about a specific use case or a specific topic.

By using the "Choose a topic" and "Choose a document type" fields of the APIC documentation website, you can narrow down the displayed documentation list to make it easier to find the desired document.

You can watch videos that demonstrate how to perform specific tasks in the Cisco APIC on the Cisco Data Center Networking YouTube channel.

Temporary licenses with an expiry date are available for evaluation and lab use purposes. They are strictly not allowed to be used in production. Use a permanent or subscription license that has been purchased through Cisco for production purposes. For more information, go to Cisco Data Center Networking Software Subscriptions.

The following table provides links to the release notes, verified scalability documentation, and new documentation:

Document

Description

Cisco Nexus 9000 ACI-Mode Switches Release Notes, Release 16.0(1)

The release notes for Cisco NX-OS for Cisco Nexus 9000 Series ACI-Mode Switches.

Verified Scalability Guide for Cisco APIC, Release 6.0(1) and Cisco Nexus 9000 Series ACI-Mode Switches, Release 16.0(1)

This guide contains the maximum verified scalability limits for Cisco Application Centric Infrastructure (ACI) parameters for Cisco APIC and Cisco Nexus 9000 Series ACI-Mode Switches.


Introduction

The Cisco Application Centric Infrastructure (ACI) is an architecture that allows the application to define the networking requirements in a programmatic way. This architecture simplifies, optimizes, and accelerates the entire application deployment lifecycle. Cisco Application Policy Infrastructure Controller (APIC) is the software, or operating system, that acts as the controller.

This document describes the features, issues, and limitations for the Cisco APIC software. For the features, issues, and limitations for the Cisco NX-OS software for the Cisco Nexus 9000 series switches, see the Cisco Nexus 9000 ACI-Mode Switches Release Notes, Release 15.2(8).

For more information about this product, see "Related Content."

Date

Description

August 9, 2023

Release 5.2(8e) became available; there are no changes to this document for this release. See the Cisco Nexus 9000 ACI-Mode Switches Release Notes, Release 15.2(8) for the changes in this release.

June 29, 2023

Release 5.2(8d) became available.

New Software Features

Product Impact

Feature

Description

Base Functionality

Source IP address in the audit log messages

The audit log messages now include the IP address of the source of the configuration change.

Base Functionality

Enhanced timestamp for UDP transport syslog messages

Cisco APIC supports a new enhanced timestamp mode for syslog to include the year in the timestamp for UDP transport.

Interoperability

Support for VMware vSphere 8.0

VMM domains now support VMware vSphere 8.0.

For more information, see the Cisco ACI vCenter Plug-in chapter in the Cisco ACI Virtualization Guide, 5.2(x) and Cisco ACI Virtual Edge Release Notes, Release 3.2(4).

New Hardware Features

For the new hardware features, see the Cisco Nexus 9000 ACI-Mode Switches Release Notes, Release 15.2(8).

Changes in Behavior

· There are no changes in behavior in this release.

Open Issues

Click the bug ID to access the Bug Search tool and see additional information about the bug. The "Exists In" column of the table specifies the 5.2(8) releases in which the bug exists. A bug might also exist in releases other than the 5.2(8) releases.

Bug ID

Description

Exists in

CSCvt99966

A SPAN session with the source type set to "Routed-Outside" goes down. The SPAN configuration is pushed to the anchor or non-anchor nodes, but the interfaces are not pushed due to the following fault: "Failed to configure SPAN with source SpanFL3out due to Source fvIfConn not available".

5.2(8d) and later

CSCvy40511

Traffic from an endpoint under a remote leaf switch to an external node and its attached external networks is dropped. This occurs if the external node is attached to an L3Out with a vPC and there is a redistribution configuration on the L3Out to advertise the reachability of the external nodes as direct-attached hosts.

5.2(8d) and later

CSCvz72941

While performing ID recovery, id-import gets timed out. Due to this, ID recovery fails.

5.2(8d) and later

CSCvz83636

For a health record query using the last page and a time range, the GUI displays some health records with a creation time that are beyond the time range (such as 24h).

5.2(8d) and later

CSCwa90058

When a VRF-level subnet <fvRtSummSubnet> and instP-level subnet <l3extSubnet> with a summary policy is configured for an overlapping subnet, the routes will get summarized by the configuration that was added first. But, the fault on the configuration that was added last will not be shown in the Cisco APIC GUI.

5.2(8d) and later

CSCwa90084

- Traffic disruption across a vPC pair on a given encapsulation.

OR

- EPG flood in encap blackholing on a given encapsulation.

OR

- STP packets received on an encapsulation on a given port are not forwarded on all the leaf switches where the same EPG/same encapsulation is deployed.

5.2(8d) and later

CSCwd26277

When deploying a service graph, the dialog does not list all bridge domains for the provider connector. This issue is observed when you enter or edit the bridge domain name in the consumer connector field. After this, the provider connector will only list the bridge domain that is selected by the consumer connector field.

5.2(8d) and later

CSCwe39988

The Cisco APIC GUI becomes unresponsive when there is large configuration for given tenant and VRF instance.

5.2(8d) and later

CSCwf04318

In a Cisco APIC 5.2 release, a user in a restricted security domain is allowed to create a VRF instance, but that same restricted mode user is not allowed to delete that VRF instance.

5.2(8d) and later

CSCwf54771

User configuration is missing on APICs and switches following an ungraceful reload or power outage.

5.2(8d) and later

Resolved Issues

Bug ID

Description

Fixed in

CSCwc11570

In certain configuration sequences, bridge domain routes (and consequently, host routes) are not advertised out of GOLF and ACI Anywhere L3Outs.

5.2(8d)

CSCwc66053

Preconfiguration validations for L3Outs that occur whenever a new configuration is pushed to the Cisco APIC might not get triggered.

5.2(8d)

CSCwd44827

APIC fails to update standalone Layer3 controller subnet on its fabric. The following fault is raised:

F609478 - [FSM:FAILED]: Update Standalone Controller Subnet(TASK:ifc:policymgr:FabricCtrlrConfigPUpdatePodConnPDef)

5.2(8d)

CSCwd88076

This issue is seen when Syslog is configured with CA signed certificates.The TLS connection was terminated due to certificate validation failure. This is not correct behavior and TLS connection should be established for CA signed certificates too.

5.2(8d)

CSCwd90130

After performing an interface migration from the old selector-based style to the new per-port configuration, an interface with an active override might not work as before the migration.

5.2(8d)

CSCwe09859

The OpFlex agent on a hypervisor randomly disconnects, impacting VM traffic.

5.2(8d)

CSCwe19885

The Nexus Insights application cannot stream the telemetry data to NDI, even though the Cisco ACI site is registered and active.

5.2(8d)

CSCwe25534

When an IPv6 address is added as the BGP peer address, the APIC does not validate the IPv6 address if the address contains any letters.

5.2(8d)

CSCwe64159

Podman is unstable, and gets restarted every 5 seconds, even after running the podman_cleanup.py script. The scheduler services show all the services are running on all the Cisco APICs.

5.2(8d)

CSCwe96230

The Appliance Element (AE) service gets restarted several times by the cgroup process (oom-killer).

5.2(8d)

CSCwf11826

Endpoints are moved from a Tag-based ESG to the base EPG. Tag-to-VM associations are lost and not refreshed for Tags used in ESG selectors.

5.2(8d)

CSCwf19660

Major fault F3083 ("IP detected on multiple MACs") is raised under a uEPG and ESG when an EPG selector is configured in the ESG.

5.2(8d)

CSCwf29094

The error "Error:Server did not return JSON response" occurs in Nexus Cloud Connectivity in any of the Cisco APICs.

5.2(8d)

Known Issues

Click the bug ID to access the Bug Search tool and see additional information about the bug. The "Exists In" column of the table specifies the 5.2(8) releases in which the bug exists. A bug might also exist in releases other than the 5.2(8) releases.

Bug ID

Description

Exists in

CSCuu11416

An endpoint-to-endpoint ACI policy that uses Layer 2 traffic with an IPv6 header does not get counted within or across ESGs/EPGs.

5.2(8d) and later

CSCvj26666

The "show run leaf|spine <nodeId>" command might produce an error for scaled up configurations.

5.2(8d) and later

CSCvj90385

With a uniform distribution of EPs and traffic flows, a fabric module in slot 25 sometimes reports far less than 50% of the traffic compared to the traffic on fabric modules in non-FM25 slots.

5.2(8d) and later

CSCvm71833

Switch upgrades fail with the following error: Version not compatible.

5.2(8d) and later

CSCvq39764

When you click Restart for the Microsoft System Center Virtual Machine Manager (SCVMM) agent on a scaled-out setup, the service may stop. You can restart the agent by clicking Start.

5.2(8d) and later

CSCvq58953

One of the following symptoms occurs:

App installation/enable/disable takes a long time and does not complete.

Nomad leadership is lost. The output of the acidiag scheduler logs members command contains the following error:

Error querying node status: Unexpected response code: 500 (rpc error: No cluster leader)

5.2(8d) and later

CSCvr89603

The CRC and stomped CRC error values do not match when seen from the APIC CLI compared to the APIC GUI. This is expected behavior. The GUI values are from the history data, whereas the CLI values are from the current data.

5.2(8d) and later

CSCvs19322

Upgrading Cisco APIC from a 3.x release to a 4.x or later release causes Smart Licensing to lose its registration. Registering Smart Licensing again will clear the fault.

5.2(8d) and later

CSCvs77929

In the 4.x and later releases, if a firmware policy is created with different name than the maintenance policy, the firmware policy will be deleted and a new firmware policy gets created with the same name, which causes the upgrade process to fail.

5.2(8d) and later

CSCvx75380

svcredirDestmon objects get programmed in all of the leaf switches where the service L3Out is deployed, even though the service node may not be connected to some of the leaf switch.

There is no impact to traffic.

5.2(8d) and later

CSCvx78018

A remote leaf switch has momentary traffic loss for flushed endpoints as the traffic goes through the tglean path and does not directly go through the spine switch proxy path.

5.2(8d) and later

CSCvy07935

xR IP flush for all endpoints under the bridge domain subnets of the EPG being migrated to ESG. This will lead to a temporary traffic loss on remote leaf switch for all EPGs in the bridge domain. Traffic is expected to recover.

5.2(8d) and later

CSCvy10946

With the floating L3Out multipath recursive feature, if a static route with multipath is configured, not all paths are installed at the non-border leaf switch/non-anchor nodes.

5.2(8d) and later

CSCvy34357

Starting with the 5.2(8) release, the following apps built with the following non-compliant Docker versions cannot be installed nor run:

· ConnectivityCompliance 1.2

· SevOneAciMonitor 1.0

5.2(8d) and later

CSCvy45358

The file size mentioned in the status managed object for techsupport "dbgexpTechSupStatus" is wrong if the file size is larger than 4GB.

5.2(8d) and later

CSCvz06118

In the "Visibility and Troubleshooting Wizard," ERSPAN support for IPv6 traffic is not available.

5.2(8d) and later

CSCvz84444

While navigating to the last records in the various History sub tabs, it is possible to not see any results. The first, previous, next, and last buttons will then stop working too.

5.2(8d) and later

CSCvz85579

VMMmgr process experiences a very high load for an extended period of time that impacts other operations that involve it.

The process may consume excessive amount of memory and get aborted. This can be confirmed with the command "dmesg -T | grep oom_reaper" if messages such as the following are reported:

oom_reaper: reaped process 5578 (svc_ifc_vmmmgr.)

5.2(8d) and later

CSCwa78573

When the "BGP" branch is expanded in the Fabric > Inventory > POD 1 > Leaf > Protocols > BGP navigation path, the GUI freezes and you cannot navigate to any other page.

This occurs because the APIC gets large set of data in response, which cannot be handled by the browser for parts of the GUI that do not have the pagination.

5.2(8d) and later

CSCwd45200

Hosting server details for AVE endpoints at the operational tab under the EPG is not updated after VM migration.

5.2(8d) and later

CSCwd51537

After changing a VM's name, the name does not get updated for endpoints in the Operational tab of an EPG.

5.2(8d) and later

N/A

If you are upgrading to Cisco APIC release 4.2(6o), 4.2(7l), 5.2(1g), or later, ensure that any VLAN encapsulation blocks that you are explicitly using for leaf switch front panel VLAN programming are set as "external (on the wire)." If these VLAN encapsulation blocks are instead set to "internal," the upgrade causes the front panel port VLAN to be removed, which can result in a datapath outage.

5.2(8d) and later

N/A

Beginning in Cisco APIC release 4.1(1), the IP SLA monitor policy validates the IP SLA port value. Because of the validation, when TCP is configured as the IP SLA type, Cisco APIC no longer accepts an IP SLA port value of 0, which was allowed in previous releases. An IP SLA monitor policy from a previous release that has an IP SLA port value of 0 becomes invalid if the Cisco APIC is upgraded to release 4.1(1) or later. This results in a failure for the configuration import or snapshot rollback.

The workaround is to configure a non-zero IP SLA port value before upgrading the Cisco APIC, and use the snapshot and configuration export that was taken after the IP SLA port change.

5.2(8d) and later

N/A

If you use the REST API to upgrade an app, you must create a new firmware.OSource to be able to download a new app image.

5.2(8d) and later

N/A

In a multipod configuration, before you make any changes to a spine switch, ensure that there is at least one operationally "up" external link that is participating in the multipod topology. Failure to do so could bring down the multipod connectivity. For more information about multipod, see the Cisco Application Centric Infrastructure Fundamentals document and the Cisco APIC Getting Started Guide.

5.2(8d) and later

N/A

With a non-english SCVMM 2012 R2 or SCVMM 2016 setup and where the virtual machine names are specified in non-english characters, if the host is removed and re-added to the host group, the GUID for all the virtual machines under that host changes. Therefore, if a user has created a micro segmentation endpoint group using "VM name" attribute specifying the GUID of respective virtual machine, then that micro segmentation endpoint group will not work if the host (hosting the virtual machines) is removed and re-added to the host group, as the GUID for all the virtual machines would have changed. This does not happen if the virtual name has name specified in all english characters.

5.2(8d) and later

N/A

A query of a configurable policy that does not have a subscription goes to the policy distributor. However, a query of a configurable policy that has a subscription goes to the policy manager. As a result, if the policy propagation from the policy distributor to the policy manager takes a prolonged amount of time, then in such cases the query with the subscription might not return the policy simply because it has not reached policy manager yet.

5.2(8d) and later

N/A

When there are silent hosts across sites, ARP glean messages might not be forwarded to remote sites if a leaf switch without -EX or a later designation in the product ID happens to be in the transit path and the VRF is deployed on that leaf switch, the switch does not forward the ARP glean packet back into the fabric to reach the remote site. This issue is specific to transit leaf switches without -EX or a later designation in the product ID and does not affect leaf switches that have -EX or a later designation in the product ID. This issue breaks the capability of discovering silent hosts.

5.2(8d) and later

N/A

Typically, faults are generally raised based on the presence of the BGP route target profile under the VRF table. However, if a BGP route target profile is configured without actual route targets (that is, the profile has empty policies), a fault will not be raised in this situation.

5.2(8d) and later

N/A

MPLS interface statistics shown in a switch's CLI get cleared after an admin or operational down event.

5.2(8d) and later

N/A

MPLS interface statistics in a switch's CLI are reported every 10 seconds. If, for example, an interface goes down 3 seconds after the collection of the statistics, the CLI reports only 3 seconds of the statistics and clears all of the other statistics.

5.2(8d) and later

Virtualization Compatibility Information

This section lists virtualization compatibility information for the Cisco APIC software.

· For a table that shows the supported virtualization products, see the ACI Virtualization Compatibility Matrix.

· For information about Cisco APIC compatibility with Cisco UCS Director, see the appropriate Cisco UCS Director Compatibility Matrix document.

· If you use Microsoft vSwitch and want to downgrade to Cisco APIC Release 2.3(1) from a later release, you first must delete any microsegment EPGs configured with the Match All filter.

· This release supports the following additional virtualization products:

Product

Supported Release

Information Location

Microsoft Hyper-V

2016 Update Rollup 1, 2, 2.1, and 3

N/A

VMM Integration and VMware Distributed Virtual Switch (DVS)

6.5, 6.7, 7.0, and 8.0

Cisco ACI Virtualization Guide, Release 5.2(x)

Hardware Compatibility Information

This release supports the following Cisco APIC servers:

Product ID

Description

APIC-L2

Cisco APIC with large CPU, hard drive, and memory configurations (more than 1000 edge ports)

APIC-L3

Cisco APIC with large CPU, hard drive, and memory configurations (more than 1200 edge ports)

APIC-M2

Cisco APIC with medium-size CPU, hard drive, and memory configurations (up to 1000 edge ports)

APIC-M3

Cisco APIC with medium-size CPU, hard drive, and memory configurations (up to 1200 edge ports)


The following list includes general hardware compatibility information:

· For the supported hardware, see the Cisco Nexus 9000 ACI-Mode Switches Release Notes, Release 15.2(8).

· Contracts using matchDscp filters are only supported on switches with "EX" on the end of the switch name. For example, N9K-93108TC-EX.

· When the fabric node switch (spine or leaf) is out-of-fabric, the environmental sensor values, such as Current Temperature, Power Draw, and Power Consumption, might be reported as "N/A." A status might be reported as "Normal" even when the Current Temperature is "N/A."

· First generation switches (switches without -EX, -FX, -GX, or a later suffix in the product ID) do not support Contract filters with match type "IPv4" or "IPv6." Only match type "IP" is supported. Because of this, a contract will match both IPv4 and IPv6 traffic when the match type of "IP" is used.

The following table provides compatibility information for specific hardware:

Product ID

Description

Cisco UCS M4-based Cisco APIC

The Cisco UCS M4-based Cisco APIC and previous versions support only the 10G interface. Connecting the Cisco APIC to the Cisco ACI fabric requires a same speed interface on the Cisco ACI leaf switch. You cannot connect the Cisco APIC directly to the Cisco N9332PQ ACI leaf switch, unless you use a 40G to 10G converter (part number CVR-QSFP-SFP10G), in which case the port on the Cisco N9332PQ switch auto-negotiates to 10G without requiring any manual configuration.

Cisco UCS M5-based Cisco APIC

The Cisco UCS M5-based Cisco APIC supports dual speed 10G and 25G interfaces. Connecting the Cisco APIC to the Cisco ACI fabric requires a same speed interface on the Cisco ACI leaf switch. You cannot connect the Cisco APIC directly to the Cisco N9332PQ ACI leaf switch, unless you use a 40G to 10G converter (part number CVR-QSFP-SFP10G), in which case the port on the Cisco N9332PQ switch auto-negotiates to 10G without requiring any manual configuration.

N2348UPQ

To connect the N2348UPQ to Cisco ACI leaf switches, the following options are available:

Directly connect the 40G FEX ports on the N2348UPQ to the 40G switch ports on the Cisco ACI leaf switches

Break out the 40G FEX ports on the N2348UPQ to 4x10G ports and connect to the 10G ports on all other Cisco ACI leaf switches.

Note: A fabric uplink port cannot be used as a FEX fabric port.

N9K-C9348GC-FXP

This switch does not read SPROM information if the PSU is in a shut state. You might see an empty string in the Cisco APIC output.

N9K-C9364C-FX

Ports 49-64 do not support 1G SFPs with QSA.

N9K-C9508-FM-E

The Cisco N9K-C9508-FM-E2 and N9K-C9508-FM-E fabric modules in the mixed mode configuration are not supported on the same spine switch.

N9K-C9508-FM-E2

The Cisco N9K-C9508-FM-E2 and N9K-C9508-FM-E fabric modules in the mixed mode configuration are not supported on the same spine switch.

The locator LED enable/disable feature is supported in the GUI and not supported in the Cisco ACI NX-OS switch CLI.

N9K-C9508-FM-E2

This fabric module must be physically removed before downgrading to releases earlier than Cisco APIC 3.0(1).

N9K-X9736C-FX

The locator LED enable/disable feature is supported in the GUI and not supported in the Cisco ACI NX-OS Switch CLI.

N9K-X9736C-FX

Ports 29 to 36 do not support 1G SFPs with QSA.

Miscellaneous Compatibility Information

This release supports the following products:

Product

Supported Release

Cisco NX-OS

15.2(8)

Cisco UCS Manager

2.2(1c) or later is required for the Cisco UCS Fabric Interconnect and other components, including the BIOS, CIMC, and the adapter.

CIMC HUU ISO

Network Insights Base, Network Insights Advisor, and Network Insights for Resources

For the release information, documentation, and download links, see the Cisco Network Insights for Data Center page.

For the supported releases, see the Cisco Data Center Networking Applications Compatibility Matrix.


· A known issue exists with the Safari browser and unsigned certificates, which applies when connecting to the Cisco APIC GUI. For more information, see the Cisco APIC Getting Started Guide, Release 5.2(x).

· For compatibility with Day-2 Operations apps, see the Cisco Data Center Networking Applications Compatibility Matrix.

· Cisco Nexus Dashboard Insights creates a user in Cisco APIC called cisco_SN_NI. This user is used when Nexus Dashboard Insights needs to make any changes or query any information from the Cisco APIC. In the Cisco APIC, navigate to the Audit Logs tab of the System > History page. The cisco_SN_NI user is displayed in the User column.

Related Content

See the Cisco Application Policy Infrastructure Controller (APIC) page for the documentation.

The documentation includes installation, upgrade, configuration, programming, and troubleshooting guides, technical references, release notes, and knowledge base (KB) articles, as well as other documentation. KB articles provide information about a specific use case or a specific topic.

By using the "Choose a topic" and "Choose a document type" fields of the APIC documentation website, you can narrow down the displayed documentation list to make it easier to find the desired document.

You can watch videos that demonstrate how to perform specific tasks in the Cisco APIC on the Cisco Data Center Networking YouTube channel.

Temporary licenses with an expiry date are available for evaluation and lab use purposes. They are strictly not allowed to be used in production. Use a permanent or subscription license that has been purchased through Cisco for production purposes. For more information, go to Cisco Data Center Networking Software Subscriptions.

The following table provides links to the release notes, verified scalability documentation, and new documentation:

Document

Description

Cisco Nexus 9000 ACI-Mode Switches Release Notes, Release 15.2(8)

The release notes for Cisco NX-OS for Cisco Nexus 9000 Series ACI-Mode Switches.

Verified Scalability Guide for Cisco APIC, Release 5.2(8) and Cisco Nexus 9000 Series ACI-Mode Switches, Release 15.2(8)

This guide contains the maximum verified scalability limits for Cisco Application Centric Infrastructure (ACI) parameters for Cisco APIC and Cisco Nexus 9000 Series ACI-Mode Switches.


Introduction

The Cisco Application Centric Infrastructure (ACI) is an architecture that allows the application to define the networking requirements in a programmatic way. This architecture simplifies, optimizes, and accelerates the entire application deployment lifecycle. Cisco Application Policy Infrastructure Controller (APIC) is the software, or operating system, that acts as the controller.

This document describes the features, issues, and limitations for the Cisco APIC software. For the features, issues, and limitations for the Cisco NX-OS software for the Cisco Nexus 9000 series switches, see the Cisco Nexus 9000 ACI-Mode Switches Release Notes, Release 15.2(8).

For more information about this product, see "Related Content."

Date

Description

August 9, 2023

Release 5.2(8e) became available; there are no changes to this document for this release. See the Cisco Nexus 9000 ACI-Mode Switches Release Notes, Release 15.2(8) for the changes in this release.

June 29, 2023

Release 5.2(8d) became available.

New Software Features

Product Impact

Feature

Description

Base Functionality

Source IP address in the audit log messages

The audit log messages now include the IP address of the source of the configuration change.

Base Functionality

Enhanced timestamp for UDP transport syslog messages

Cisco APIC supports a new enhanced timestamp mode for syslog to include the year in the timestamp for UDP transport.

Interoperability

Support for VMware vSphere 8.0

VMM domains now support VMware vSphere 8.0.

For more information, see the Cisco ACI vCenter Plug-in chapter in the Cisco ACI Virtualization Guide, 5.2(x) and Cisco ACI Virtual Edge Release Notes, Release 3.2(4).

New Hardware Features

For the new hardware features, see the Cisco Nexus 9000 ACI-Mode Switches Release Notes, Release 15.2(8).

Changes in Behavior

· There are no changes in behavior in this release.

Open Issues

Click the bug ID to access the Bug Search tool and see additional information about the bug. The "Exists In" column of the table specifies the 5.2(8) releases in which the bug exists. A bug might also exist in releases other than the 5.2(8) releases.

Bug ID

Description

Exists in

CSCvt99966

A SPAN session with the source type set to "Routed-Outside" goes down. The SPAN configuration is pushed to the anchor or non-anchor nodes, but the interfaces are not pushed due to the following fault: "Failed to configure SPAN with source SpanFL3out due to Source fvIfConn not available".

5.2(8d) and later

CSCvy40511

Traffic from an endpoint under a remote leaf switch to an external node and its attached external networks is dropped. This occurs if the external node is attached to an L3Out with a vPC and there is a redistribution configuration on the L3Out to advertise the reachability of the external nodes as direct-attached hosts.

5.2(8d) and later

CSCvz72941

While performing ID recovery, id-import gets timed out. Due to this, ID recovery fails.

5.2(8d) and later

CSCvz83636

For a health record query using the last page and a time range, the GUI displays some health records with a creation time that are beyond the time range (such as 24h).

5.2(8d) and later

CSCwa90058

When a VRF-level subnet <fvRtSummSubnet> and instP-level subnet <l3extSubnet> with a summary policy is configured for an overlapping subnet, the routes will get summarized by the configuration that was added first. But, the fault on the configuration that was added last will not be shown in the Cisco APIC GUI.

5.2(8d) and later

CSCwa90084

- Traffic disruption across a vPC pair on a given encapsulation.

OR

- EPG flood in encap blackholing on a given encapsulation.

OR

- STP packets received on an encapsulation on a given port are not forwarded on all the leaf switches where the same EPG/same encapsulation is deployed.

5.2(8d) and later

CSCwd26277

When deploying a service graph, the dialog does not list all bridge domains for the provider connector. This issue is observed when you enter or edit the bridge domain name in the consumer connector field. After this, the provider connector will only list the bridge domain that is selected by the consumer connector field.

5.2(8d) and later

CSCwe39988

The Cisco APIC GUI becomes unresponsive when there is large configuration for given tenant and VRF instance.

5.2(8d) and later

CSCwf04318

In a Cisco APIC 5.2 release, a user in a restricted security domain is allowed to create a VRF instance, but that same restricted mode user is not allowed to delete that VRF instance.

5.2(8d) and later

CSCwf54771

User configuration is missing on APICs and switches following an ungraceful reload or power outage.

5.2(8d) and later

Resolved Issues

Bug ID

Description

Fixed in

CSCwc11570

In certain configuration sequences, bridge domain routes (and consequently, host routes) are not advertised out of GOLF and ACI Anywhere L3Outs.

5.2(8d)

CSCwc66053

Preconfiguration validations for L3Outs that occur whenever a new configuration is pushed to the Cisco APIC might not get triggered.

5.2(8d)

CSCwd44827

APIC fails to update standalone Layer3 controller subnet on its fabric. The following fault is raised:

F609478 - [FSM:FAILED]: Update Standalone Controller Subnet(TASK:ifc:policymgr:FabricCtrlrConfigPUpdatePodConnPDef)

5.2(8d)

CSCwd88076

This issue is seen when Syslog is configured with CA signed certificates.The TLS connection was terminated due to certificate validation failure. This is not correct behavior and TLS connection should be established for CA signed certificates too.

5.2(8d)

CSCwd90130

After performing an interface migration from the old selector-based style to the new per-port configuration, an interface with an active override might not work as before the migration.

5.2(8d)

CSCwe09859

The OpFlex agent on a hypervisor randomly disconnects, impacting VM traffic.

5.2(8d)

CSCwe19885

The Nexus Insights application cannot stream the telemetry data to NDI, even though the Cisco ACI site is registered and active.

5.2(8d)

CSCwe25534

When an IPv6 address is added as the BGP peer address, the APIC does not validate the IPv6 address if the address contains any letters.

5.2(8d)

CSCwe64159

Podman is unstable, and gets restarted every 5 seconds, even after running the podman_cleanup.py script. The scheduler services show all the services are running on all the Cisco APICs.

5.2(8d)

CSCwe96230

The Appliance Element (AE) service gets restarted several times by the cgroup process (oom-killer).

5.2(8d)

CSCwf11826

Endpoints are moved from a Tag-based ESG to the base EPG. Tag-to-VM associations are lost and not refreshed for Tags used in ESG selectors.

5.2(8d)

CSCwf19660

Major fault F3083 ("IP detected on multiple MACs") is raised under a uEPG and ESG when an EPG selector is configured in the ESG.

5.2(8d)

CSCwf29094

The error "Error:Server did not return JSON response" occurs in Nexus Cloud Connectivity in any of the Cisco APICs.

5.2(8d)

Known Issues

Click the bug ID to access the Bug Search tool and see additional information about the bug. The "Exists In" column of the table specifies the 5.2(8) releases in which the bug exists. A bug might also exist in releases other than the 5.2(8) releases.

Bug ID

Description

Exists in

CSCuu11416

An endpoint-to-endpoint ACI policy that uses Layer 2 traffic with an IPv6 header does not get counted within or across ESGs/EPGs.

5.2(8d) and later

CSCvj26666

The "show run leaf|spine <nodeId>" command might produce an error for scaled up configurations.

5.2(8d) and later

CSCvj90385

With a uniform distribution of EPs and traffic flows, a fabric module in slot 25 sometimes reports far less than 50% of the traffic compared to the traffic on fabric modules in non-FM25 slots.

5.2(8d) and later

CSCvm71833

Switch upgrades fail with the following error: Version not compatible.

5.2(8d) and later

CSCvq39764

When you click Restart for the Microsoft System Center Virtual Machine Manager (SCVMM) agent on a scaled-out setup, the service may stop. You can restart the agent by clicking Start.

5.2(8d) and later

CSCvq58953

One of the following symptoms occurs:

App installation/enable/disable takes a long time and does not complete.

Nomad leadership is lost. The output of the acidiag scheduler logs members command contains the following error:

Error querying node status: Unexpected response code: 500 (rpc error: No cluster leader)

5.2(8d) and later

CSCvr89603

The CRC and stomped CRC error values do not match when seen from the APIC CLI compared to the APIC GUI. This is expected behavior. The GUI values are from the history data, whereas the CLI values are from the current data.

5.2(8d) and later

CSCvs19322

Upgrading Cisco APIC from a 3.x release to a 4.x or later release causes Smart Licensing to lose its registration. Registering Smart Licensing again will clear the fault.

5.2(8d) and later

CSCvs77929

In the 4.x and later releases, if a firmware policy is created with different name than the maintenance policy, the firmware policy will be deleted and a new firmware policy gets created with the same name, which causes the upgrade process to fail.

5.2(8d) and later

CSCvx75380

svcredirDestmon objects get programmed in all of the leaf switches where the service L3Out is deployed, even though the service node may not be connected to some of the leaf switch.

There is no impact to traffic.

5.2(8d) and later

CSCvx78018

A remote leaf switch has momentary traffic loss for flushed endpoints as the traffic goes through the tglean path and does not directly go through the spine switch proxy path.

5.2(8d) and later

CSCvy07935

xR IP flush for all endpoints under the bridge domain subnets of the EPG being migrated to ESG. This will lead to a temporary traffic loss on remote leaf switch for all EPGs in the bridge domain. Traffic is expected to recover.

5.2(8d) and later

CSCvy10946

With the floating L3Out multipath recursive feature, if a static route with multipath is configured, not all paths are installed at the non-border leaf switch/non-anchor nodes.

5.2(8d) and later

CSCvy34357

Starting with the 5.2(8) release, the following apps built with the following non-compliant Docker versions cannot be installed nor run:

· ConnectivityCompliance 1.2

· SevOneAciMonitor 1.0

5.2(8d) and later

CSCvy45358

The file size mentioned in the status managed object for techsupport "dbgexpTechSupStatus" is wrong if the file size is larger than 4GB.

5.2(8d) and later

CSCvz06118

In the "Visibility and Troubleshooting Wizard," ERSPAN support for IPv6 traffic is not available.

5.2(8d) and later

CSCvz84444

While navigating to the last records in the various History sub tabs, it is possible to not see any results. The first, previous, next, and last buttons will then stop working too.

5.2(8d) and later

CSCvz85579

VMMmgr process experiences a very high load for an extended period of time that impacts other operations that involve it.

The process may consume excessive amount of memory and get aborted. This can be confirmed with the command "dmesg -T | grep oom_reaper" if messages such as the following are reported:

oom_reaper: reaped process 5578 (svc_ifc_vmmmgr.)

5.2(8d) and later

CSCwa78573

When the "BGP" branch is expanded in the Fabric > Inventory > POD 1 > Leaf > Protocols > BGP navigation path, the GUI freezes and you cannot navigate to any other page.

This occurs because the APIC gets large set of data in response, which cannot be handled by the browser for parts of the GUI that do not have the pagination.

5.2(8d) and later

CSCwd45200

Hosting server details for AVE endpoints at the operational tab under the EPG is not updated after VM migration.

5.2(8d) and later

CSCwd51537

After changing a VM's name, the name does not get updated for endpoints in the Operational tab of an EPG.

5.2(8d) and later

N/A

If you are upgrading to Cisco APIC release 4.2(6o), 4.2(7l), 5.2(1g), or later, ensure that any VLAN encapsulation blocks that you are explicitly using for leaf switch front panel VLAN programming are set as "external (on the wire)." If these VLAN encapsulation blocks are instead set to "internal," the upgrade causes the front panel port VLAN to be removed, which can result in a datapath outage.

5.2(8d) and later

N/A

Beginning in Cisco APIC release 4.1(1), the IP SLA monitor policy validates the IP SLA port value. Because of the validation, when TCP is configured as the IP SLA type, Cisco APIC no longer accepts an IP SLA port value of 0, which was allowed in previous releases. An IP SLA monitor policy from a previous release that has an IP SLA port value of 0 becomes invalid if the Cisco APIC is upgraded to release 4.1(1) or later. This results in a failure for the configuration import or snapshot rollback.

The workaround is to configure a non-zero IP SLA port value before upgrading the Cisco APIC, and use the snapshot and configuration export that was taken after the IP SLA port change.

5.2(8d) and later

N/A

If you use the REST API to upgrade an app, you must create a new firmware.OSource to be able to download a new app image.

5.2(8d) and later

N/A

In a multipod configuration, before you make any changes to a spine switch, ensure that there is at least one operationally "up" external link that is participating in the multipod topology. Failure to do so could bring down the multipod connectivity. For more information about multipod, see the Cisco Application Centric Infrastructure Fundamentals document and the Cisco APIC Getting Started Guide.

5.2(8d) and later

N/A

With a non-english SCVMM 2012 R2 or SCVMM 2016 setup and where the virtual machine names are specified in non-english characters, if the host is removed and re-added to the host group, the GUID for all the virtual machines under that host changes. Therefore, if a user has created a micro segmentation endpoint group using "VM name" attribute specifying the GUID of respective virtual machine, then that micro segmentation endpoint group will not work if the host (hosting the virtual machines) is removed and re-added to the host group, as the GUID for all the virtual machines would have changed. This does not happen if the virtual name has name specified in all english characters.

5.2(8d) and later

N/A

A query of a configurable policy that does not have a subscription goes to the policy distributor. However, a query of a configurable policy that has a subscription goes to the policy manager. As a result, if the policy propagation from the policy distributor to the policy manager takes a prolonged amount of time, then in such cases the query with the subscription might not return the policy simply because it has not reached policy manager yet.

5.2(8d) and later

N/A

When there are silent hosts across sites, ARP glean messages might not be forwarded to remote sites if a leaf switch without -EX or a later designation in the product ID happens to be in the transit path and the VRF is deployed on that leaf switch, the switch does not forward the ARP glean packet back into the fabric to reach the remote site. This issue is specific to transit leaf switches without -EX or a later designation in the product ID and does not affect leaf switches that have -EX or a later designation in the product ID. This issue breaks the capability of discovering silent hosts.

5.2(8d) and later

N/A

Typically, faults are generally raised based on the presence of the BGP route target profile under the VRF table. However, if a BGP route target profile is configured without actual route targets (that is, the profile has empty policies), a fault will not be raised in this situation.

5.2(8d) and later

N/A

MPLS interface statistics shown in a switch's CLI get cleared after an admin or operational down event.

5.2(8d) and later

N/A

MPLS interface statistics in a switch's CLI are reported every 10 seconds. If, for example, an interface goes down 3 seconds after the collection of the statistics, the CLI reports only 3 seconds of the statistics and clears all of the other statistics.

5.2(8d) and later

Virtualization Compatibility Information

This section lists virtualization compatibility information for the Cisco APIC software.

· For a table that shows the supported virtualization products, see the ACI Virtualization Compatibility Matrix.

· For information about Cisco APIC compatibility with Cisco UCS Director, see the appropriate Cisco UCS Director Compatibility Matrix document.

· If you use Microsoft vSwitch and want to downgrade to Cisco APIC Release 2.3(1) from a later release, you first must delete any microsegment EPGs configured with the Match All filter.

· This release supports the following additional virtualization products:

Product

Supported Release

Information Location

Microsoft Hyper-V

2016 Update Rollup 1, 2, 2.1, and 3

N/A

VMM Integration and VMware Distributed Virtual Switch (DVS)

6.5, 6.7, 7.0, and 8.0

Cisco ACI Virtualization Guide, Release 5.2(x)

Hardware Compatibility Information

This release supports the following Cisco APIC servers:

Product ID

Description

APIC-L2

Cisco APIC with large CPU, hard drive, and memory configurations (more than 1000 edge ports)

APIC-L3

Cisco APIC with large CPU, hard drive, and memory configurations (more than 1200 edge ports)

APIC-M2

Cisco APIC with medium-size CPU, hard drive, and memory configurations (up to 1000 edge ports)

APIC-M3

Cisco APIC with medium-size CPU, hard drive, and memory configurations (up to 1200 edge ports)


The following list includes general hardware compatibility information:

· For the supported hardware, see the Cisco Nexus 9000 ACI-Mode Switches Release Notes, Release 15.2(8).

· Contracts using matchDscp filters are only supported on switches with "EX" on the end of the switch name. For example, N9K-93108TC-EX.

· When the fabric node switch (spine or leaf) is out-of-fabric, the environmental sensor values, such as Current Temperature, Power Draw, and Power Consumption, might be reported as "N/A." A status might be reported as "Normal" even when the Current Temperature is "N/A."

· First generation switches (switches without -EX, -FX, -GX, or a later suffix in the product ID) do not support Contract filters with match type "IPv4" or "IPv6." Only match type "IP" is supported. Because of this, a contract will match both IPv4 and IPv6 traffic when the match type of "IP" is used.

The following table provides compatibility information for specific hardware:

Product ID

Description

Cisco UCS M4-based Cisco APIC

The Cisco UCS M4-based Cisco APIC and previous versions support only the 10G interface. Connecting the Cisco APIC to the Cisco ACI fabric requires a same speed interface on the Cisco ACI leaf switch. You cannot connect the Cisco APIC directly to the Cisco N9332PQ ACI leaf switch, unless you use a 40G to 10G converter (part number CVR-QSFP-SFP10G), in which case the port on the Cisco N9332PQ switch auto-negotiates to 10G without requiring any manual configuration.

Cisco UCS M5-based Cisco APIC

The Cisco UCS M5-based Cisco APIC supports dual speed 10G and 25G interfaces. Connecting the Cisco APIC to the Cisco ACI fabric requires a same speed interface on the Cisco ACI leaf switch. You cannot connect the Cisco APIC directly to the Cisco N9332PQ ACI leaf switch, unless you use a 40G to 10G converter (part number CVR-QSFP-SFP10G), in which case the port on the Cisco N9332PQ switch auto-negotiates to 10G without requiring any manual configuration.

N2348UPQ

To connect the N2348UPQ to Cisco ACI leaf switches, the following options are available:

Directly connect the 40G FEX ports on the N2348UPQ to the 40G switch ports on the Cisco ACI leaf switches

Break out the 40G FEX ports on the N2348UPQ to 4x10G ports and connect to the 10G ports on all other Cisco ACI leaf switches.

Note: A fabric uplink port cannot be used as a FEX fabric port.

N9K-C9348GC-FXP

This switch does not read SPROM information if the PSU is in a shut state. You might see an empty string in the Cisco APIC output.

N9K-C9364C-FX

Ports 49-64 do not support 1G SFPs with QSA.

N9K-C9508-FM-E

The Cisco N9K-C9508-FM-E2 and N9K-C9508-FM-E fabric modules in the mixed mode configuration are not supported on the same spine switch.

N9K-C9508-FM-E2

The Cisco N9K-C9508-FM-E2 and N9K-C9508-FM-E fabric modules in the mixed mode configuration are not supported on the same spine switch.

The locator LED enable/disable feature is supported in the GUI and not supported in the Cisco ACI NX-OS switch CLI.

N9K-C9508-FM-E2

This fabric module must be physically removed before downgrading to releases earlier than Cisco APIC 3.0(1).

N9K-X9736C-FX

The locator LED enable/disable feature is supported in the GUI and not supported in the Cisco ACI NX-OS Switch CLI.

N9K-X9736C-FX

Ports 29 to 36 do not support 1G SFPs with QSA.

Miscellaneous Compatibility Information

This release supports the following products:

Product

Supported Release

Cisco NX-OS

15.2(8)

Cisco UCS Manager

2.2(1c) or later is required for the Cisco UCS Fabric Interconnect and other components, including the BIOS, CIMC, and the adapter.

CIMC HUU ISO

Network Insights Base, Network Insights Advisor, and Network Insights for Resources

For the release information, documentation, and download links, see the Cisco Network Insights for Data Center page.

For the supported releases, see the Cisco Data Center Networking Applications Compatibility Matrix.


· A known issue exists with the Safari browser and unsigned certificates, which applies when connecting to the Cisco APIC GUI. For more information, see the Cisco APIC Getting Started Guide, Release 5.2(x).

· For compatibility with Day-2 Operations apps, see the Cisco Data Center Networking Applications Compatibility Matrix.

· Cisco Nexus Dashboard Insights creates a user in Cisco APIC called cisco_SN_NI. This user is used when Nexus Dashboard Insights needs to make any changes or query any information from the Cisco APIC. In the Cisco APIC, navigate to the Audit Logs tab of the System > History page. The cisco_SN_NI user is displayed in the User column.

Related Content

See the Cisco Application Policy Infrastructure Controller (APIC) page for the documentation.

The documentation includes installation, upgrade, configuration, programming, and troubleshooting guides, technical references, release notes, and knowledge base (KB) articles, as well as other documentation. KB articles provide information about a specific use case or a specific topic.

By using the "Choose a topic" and "Choose a document type" fields of the APIC documentation website, you can narrow down the displayed documentation list to make it easier to find the desired document.

You can watch videos that demonstrate how to perform specific tasks in the Cisco APIC on the Cisco Data Center Networking YouTube channel.

Temporary licenses with an expiry date are available for evaluation and lab use purposes. They are strictly not allowed to be used in production. Use a permanent or subscription license that has been purchased through Cisco for production purposes. For more information, go to Cisco Data Center Networking Software Subscriptions.

The following table provides links to the release notes, verified scalability documentation, and new documentation:

Document

Description

Cisco Nexus 9000 ACI-Mode Switches Release Notes, Release 15.2(8)

The release notes for Cisco NX-OS for Cisco Nexus 9000 Series ACI-Mode Switches.

Verified Scalability Guide for Cisco APIC, Release 5.2(8) and Cisco Nexus 9000 Series ACI-Mode Switches, Release 15.2(8)

This guide contains the maximum verified scalability limits for Cisco Application Centric Infrastructure (ACI) parameters for Cisco APIC and Cisco Nexus 9000 Series ACI-Mode Switches.

Documentation Feedback

To provide technical feedback on this document, or to report an error or omission, send your comments to apic-docfeedback@cisco.com. We appreciate your feedback.


Introduction

The Cisco Application Centric Infrastructure (ACI) is an architecture that allows the application to define the networking requirements in a programmatic way. This architecture simplifies, optimizes, and accelerates the entire application deployment lifecycle. Cisco Application Policy Infrastructure Controller (APIC) is the software, or operating system, that acts as the controller.

This document describes the features, issues, and limitations for the Cisco APIC software. For the features, issues, and limitations for the Cisco NX-OS software for the Cisco Nexus 9000 series switches, see the Cisco Nexus 9000 ACI-Mode Switches Release Notes, Release 15.2(8).

For more information about this product, see "Related Content."

Date

Description

August 9, 2023

Release 5.2(8e) became available; there are no changes to this document for this release. See the Cisco Nexus 9000 ACI-Mode Switches Release Notes, Release 15.2(8) for the changes in this release.

June 29, 2023

Release 5.2(8d) became available.

New Software Features

Product Impact

Feature

Description

Base Functionality

Source IP address in the audit log messages

The audit log messages now include the IP address of the source of the configuration change.

Base Functionality

Enhanced timestamp for UDP transport syslog messages

Cisco APIC supports a new enhanced timestamp mode for syslog to include the year in the timestamp for UDP transport.

Interoperability

Support for VMware vSphere 8.0

VMM domains now support VMware vSphere 8.0.

For more information, see the Cisco ACI vCenter Plug-in chapter in the Cisco ACI Virtualization Guide, 5.2(x) and Cisco ACI Virtual Edge Release Notes, Release 3.2(4).

New Hardware Features

For the new hardware features, see the Cisco Nexus 9000 ACI-Mode Switches Release Notes, Release 15.2(8).

Changes in Behavior

· There are no changes in behavior in this release.

Open Issues

Click the bug ID to access the Bug Search tool and see additional information about the bug. The "Exists In" column of the table specifies the 5.2(8) releases in which the bug exists. A bug might also exist in releases other than the 5.2(8) releases.

Bug ID

Description

Exists in

CSCvt99966

A SPAN session with the source type set to "Routed-Outside" goes down. The SPAN configuration is pushed to the anchor or non-anchor nodes, but the interfaces are not pushed due to the following fault: "Failed to configure SPAN with source SpanFL3out due to Source fvIfConn not available".

5.2(8d) and later

CSCvy40511

Traffic from an endpoint under a remote leaf switch to an external node and its attached external networks is dropped. This occurs if the external node is attached to an L3Out with a vPC and there is a redistribution configuration on the L3Out to advertise the reachability of the external nodes as direct-attached hosts.

5.2(8d) and later

CSCvz72941

While performing ID recovery, id-import gets timed out. Due to this, ID recovery fails.

5.2(8d) and later

CSCvz83636

For a health record query using the last page and a time range, the GUI displays some health records with a creation time that are beyond the time range (such as 24h).

5.2(8d) and later

CSCwa90058

When a VRF-level subnet <fvRtSummSubnet> and instP-level subnet <l3extSubnet> with a summary policy is configured for an overlapping subnet, the routes will get summarized by the configuration that was added first. But, the fault on the configuration that was added last will not be shown in the Cisco APIC GUI.

5.2(8d) and later

CSCwa90084

- Traffic disruption across a vPC pair on a given encapsulation.

OR

- EPG flood in encap blackholing on a given encapsulation.

OR

- STP packets received on an encapsulation on a given port are not forwarded on all the leaf switches where the same EPG/same encapsulation is deployed.

5.2(8d) and later

CSCwd26277

When deploying a service graph, the dialog does not list all bridge domains for the provider connector. This issue is observed when you enter or edit the bridge domain name in the consumer connector field. After this, the provider connector will only list the bridge domain that is selected by the consumer connector field.

5.2(8d) and later

CSCwe39988

The Cisco APIC GUI becomes unresponsive when there is large configuration for given tenant and VRF instance.

5.2(8d) and later

CSCwf04318

In a Cisco APIC 5.2 release, a user in a restricted security domain is allowed to create a VRF instance, but that same restricted mode user is not allowed to delete that VRF instance.

5.2(8d) and later

CSCwf54771

User configuration is missing on APICs and switches following an ungraceful reload or power outage.

5.2(8d) and later

Resolved Issues

Bug ID

Description

Fixed in

CSCwc11570

In certain configuration sequences, bridge domain routes (and consequently, host routes) are not advertised out of GOLF and ACI Anywhere L3Outs.

5.2(8d)

CSCwc66053

Preconfiguration validations for L3Outs that occur whenever a new configuration is pushed to the Cisco APIC might not get triggered.

5.2(8d)

CSCwd44827

APIC fails to update standalone Layer3 controller subnet on its fabric. The following fault is raised:

F609478 - [FSM:FAILED]: Update Standalone Controller Subnet(TASK:ifc:policymgr:FabricCtrlrConfigPUpdatePodConnPDef)

5.2(8d)

CSCwd88076

This issue is seen when Syslog is configured with CA signed certificates.The TLS connection was terminated due to certificate validation failure. This is not correct behavior and TLS connection should be established for CA signed certificates too.

5.2(8d)

CSCwd90130

After performing an interface migration from the old selector-based style to the new per-port configuration, an interface with an active override might not work as before the migration.

5.2(8d)

CSCwe09859

The OpFlex agent on a hypervisor randomly disconnects, impacting VM traffic.

5.2(8d)

CSCwe19885

The Nexus Insights application cannot stream the telemetry data to NDI, even though the Cisco ACI site is registered and active.

5.2(8d)

CSCwe25534

When an IPv6 address is added as the BGP peer address, the APIC does not validate the IPv6 address if the address contains any letters.

5.2(8d)

CSCwe64159

Podman is unstable, and gets restarted every 5 seconds, even after running the podman_cleanup.py script. The scheduler services show all the services are running on all the Cisco APICs.

5.2(8d)

CSCwe96230

The Appliance Element (AE) service gets restarted several times by the cgroup process (oom-killer).

5.2(8d)

CSCwf11826

Endpoints are moved from a Tag-based ESG to the base EPG. Tag-to-VM associations are lost and not refreshed for Tags used in ESG selectors.

5.2(8d)

CSCwf19660

Major fault F3083 ("IP detected on multiple MACs") is raised under a uEPG and ESG when an EPG selector is configured in the ESG.

5.2(8d)

CSCwf29094

The error "Error:Server did not return JSON response" occurs in Nexus Cloud Connectivity in any of the Cisco APICs.

5.2(8d)

Known Issues

Click the bug ID to access the Bug Search tool and see additional information about the bug. The "Exists In" column of the table specifies the 5.2(8) releases in which the bug exists. A bug might also exist in releases other than the 5.2(8) releases.

Bug ID

Description

Exists in

CSCuu11416

An endpoint-to-endpoint ACI policy that uses Layer 2 traffic with an IPv6 header does not get counted within or across ESGs/EPGs.

5.2(8d) and later

CSCvj26666

The "show run leaf|spine <nodeId>" command might produce an error for scaled up configurations.

5.2(8d) and later

CSCvj90385

With a uniform distribution of EPs and traffic flows, a fabric module in slot 25 sometimes reports far less than 50% of the traffic compared to the traffic on fabric modules in non-FM25 slots.

5.2(8d) and later

CSCvm71833

Switch upgrades fail with the following error: Version not compatible.

5.2(8d) and later

CSCvq39764

When you click Restart for the Microsoft System Center Virtual Machine Manager (SCVMM) agent on a scaled-out setup, the service may stop. You can restart the agent by clicking Start.

5.2(8d) and later

CSCvq58953

One of the following symptoms occurs:

App installation/enable/disable takes a long time and does not complete.

Nomad leadership is lost. The output of the acidiag scheduler logs members command contains the following error:

Error querying node status: Unexpected response code: 500 (rpc error: No cluster leader)

5.2(8d) and later

CSCvr89603

The CRC and stomped CRC error values do not match when seen from the APIC CLI compared to the APIC GUI. This is expected behavior. The GUI values are from the history data, whereas the CLI values are from the current data.

5.2(8d) and later

CSCvs19322

Upgrading Cisco APIC from a 3.x release to a 4.x or later release causes Smart Licensing to lose its registration. Registering Smart Licensing again will clear the fault.

5.2(8d) and later

CSCvs77929

In the 4.x and later releases, if a firmware policy is created with different name than the maintenance policy, the firmware policy will be deleted and a new firmware policy gets created with the same name, which causes the upgrade process to fail.

5.2(8d) and later

CSCvx75380

svcredirDestmon objects get programmed in all of the leaf switches where the service L3Out is deployed, even though the service node may not be connected to some of the leaf switch.

There is no impact to traffic.

5.2(8d) and later

CSCvx78018

A remote leaf switch has momentary traffic loss for flushed endpoints as the traffic goes through the tglean path and does not directly go through the spine switch proxy path.

5.2(8d) and later

CSCvy07935

xR IP flush for all endpoints under the bridge domain subnets of the EPG being migrated to ESG. This will lead to a temporary traffic loss on remote leaf switch for all EPGs in the bridge domain. Traffic is expected to recover.

5.2(8d) and later

CSCvy10946

With the floating L3Out multipath recursive feature, if a static route with multipath is configured, not all paths are installed at the non-border leaf switch/non-anchor nodes.

5.2(8d) and later

CSCvy34357

Starting with the 5.2(8) release, the following apps built with the following non-compliant Docker versions cannot be installed nor run:

· ConnectivityCompliance 1.2

· SevOneAciMonitor 1.0

5.2(8d) and later

CSCvy45358

The file size mentioned in the status managed object for techsupport "dbgexpTechSupStatus" is wrong if the file size is larger than 4GB.

5.2(8d) and later

CSCvz06118

In the "Visibility and Troubleshooting Wizard," ERSPAN support for IPv6 traffic is not available.

5.2(8d) and later

CSCvz84444

While navigating to the last records in the various History sub tabs, it is possible to not see any results. The first, previous, next, and last buttons will then stop working too.

5.2(8d) and later

CSCvz85579

VMMmgr process experiences a very high load for an extended period of time that impacts other operations that involve it.

The process may consume excessive amount of memory and get aborted. This can be confirmed with the command "dmesg -T | grep oom_reaper" if messages such as the following are reported:

oom_reaper: reaped process 5578 (svc_ifc_vmmmgr.)

5.2(8d) and later

CSCwa78573

When the "BGP" branch is expanded in the Fabric > Inventory > POD 1 > Leaf > Protocols > BGP navigation path, the GUI freezes and you cannot navigate to any other page.

This occurs because the APIC gets large set of data in response, which cannot be handled by the browser for parts of the GUI that do not have the pagination.

5.2(8d) and later

CSCwd45200

Hosting server details for AVE endpoints at the operational tab under the EPG is not updated after VM migration.

5.2(8d) and later

CSCwd51537

After changing a VM's name, the name does not get updated for endpoints in the Operational tab of an EPG.

5.2(8d) and later

N/A

If you are upgrading to Cisco APIC release 4.2(6o), 4.2(7l), 5.2(1g), or later, ensure that any VLAN encapsulation blocks that you are explicitly using for leaf switch front panel VLAN programming are set as "external (on the wire)." If these VLAN encapsulation blocks are instead set to "internal," the upgrade causes the front panel port VLAN to be removed, which can result in a datapath outage.

5.2(8d) and later

N/A

Beginning in Cisco APIC release 4.1(1), the IP SLA monitor policy validates the IP SLA port value. Because of the validation, when TCP is configured as the IP SLA type, Cisco APIC no longer accepts an IP SLA port value of 0, which was allowed in previous releases. An IP SLA monitor policy from a previous release that has an IP SLA port value of 0 becomes invalid if the Cisco APIC is upgraded to release 4.1(1) or later. This results in a failure for the configuration import or snapshot rollback.

The workaround is to configure a non-zero IP SLA port value before upgrading the Cisco APIC, and use the snapshot and configuration export that was taken after the IP SLA port change.

5.2(8d) and later

N/A

If you use the REST API to upgrade an app, you must create a new firmware.OSource to be able to download a new app image.

5.2(8d) and later

N/A

In a multipod configuration, before you make any changes to a spine switch, ensure that there is at least one operationally "up" external link that is participating in the multipod topology. Failure to do so could bring down the multipod connectivity. For more information about multipod, see the Cisco Application Centric Infrastructure Fundamentals document and the Cisco APIC Getting Started Guide.

5.2(8d) and later

N/A

With a non-english SCVMM 2012 R2 or SCVMM 2016 setup and where the virtual machine names are specified in non-english characters, if the host is removed and re-added to the host group, the GUID for all the virtual machines under that host changes. Therefore, if a user has created a micro segmentation endpoint group using "VM name" attribute specifying the GUID of respective virtual machine, then that micro segmentation endpoint group will not work if the host (hosting the virtual machines) is removed and re-added to the host group, as the GUID for all the virtual machines would have changed. This does not happen if the virtual name has name specified in all english characters.

5.2(8d) and later

N/A

A query of a configurable policy that does not have a subscription goes to the policy distributor. However, a query of a configurable policy that has a subscription goes to the policy manager. As a result, if the policy propagation from the policy distributor to the policy manager takes a prolonged amount of time, then in such cases the query with the subscription might not return the policy simply because it has not reached policy manager yet.

5.2(8d) and later

N/A

When there are silent hosts across sites, ARP glean messages might not be forwarded to remote sites if a leaf switch without -EX or a later designation in the product ID happens to be in the transit path and the VRF is deployed on that leaf switch, the switch does not forward the ARP glean packet back into the fabric to reach the remote site. This issue is specific to transit leaf switches without -EX or a later designation in the product ID and does not affect leaf switches that have -EX or a later designation in the product ID. This issue breaks the capability of discovering silent hosts.

5.2(8d) and later

N/A

Typically, faults are generally raised based on the presence of the BGP route target profile under the VRF table. However, if a BGP route target profile is configured without actual route targets (that is, the profile has empty policies), a fault will not be raised in this situation.

5.2(8d) and later

N/A

MPLS interface statistics shown in a switch's CLI get cleared after an admin or operational down event.

5.2(8d) and later

N/A

MPLS interface statistics in a switch's CLI are reported every 10 seconds. If, for example, an interface goes down 3 seconds after the collection of the statistics, the CLI reports only 3 seconds of the statistics and clears all of the other statistics.

5.2(8d) and later

Virtualization Compatibility Information

This section lists virtualization compatibility information for the Cisco APIC software.

· For a table that shows the supported virtualization products, see the ACI Virtualization Compatibility Matrix.

· For information about Cisco APIC compatibility with Cisco UCS Director, see the appropriate Cisco UCS Director Compatibility Matrix document.

· If you use Microsoft vSwitch and want to downgrade to Cisco APIC Release 2.3(1) from a later release, you first must delete any microsegment EPGs configured with the Match All filter.

· This release supports the following additional virtualization products:

Product

Supported Release

Information Location

Microsoft Hyper-V

2016 Update Rollup 1, 2, 2.1, and 3

N/A

VMM Integration and VMware Distributed Virtual Switch (DVS)

6.5, 6.7, 7.0, and 8.0

Cisco ACI Virtualization Guide, Release 5.2(x)

Hardware Compatibility Information

This release supports the following Cisco APIC servers:

Product ID

Description

APIC-L2

Cisco APIC with large CPU, hard drive, and memory configurations (more than 1000 edge ports)

APIC-L3

Cisco APIC with large CPU, hard drive, and memory configurations (more than 1200 edge ports)

APIC-M2

Cisco APIC with medium-size CPU, hard drive, and memory configurations (up to 1000 edge ports)

APIC-M3

Cisco APIC with medium-size CPU, hard drive, and memory configurations (up to 1200 edge ports)


The following list includes general hardware compatibility information:

· For the supported hardware, see the Cisco Nexus 9000 ACI-Mode Switches Release Notes, Release 15.2(8).

· Contracts using matchDscp filters are only supported on switches with "EX" on the end of the switch name. For example, N9K-93108TC-EX.

· When the fabric node switch (spine or leaf) is out-of-fabric, the environmental sensor values, such as Current Temperature, Power Draw, and Power Consumption, might be reported as "N/A." A status might be reported as "Normal" even when the Current Temperature is "N/A."

· First generation switches (switches without -EX, -FX, -GX, or a later suffix in the product ID) do not support Contract filters with match type "IPv4" or "IPv6." Only match type "IP" is supported. Because of this, a contract will match both IPv4 and IPv6 traffic when the match type of "IP" is used.

The following table provides compatibility information for specific hardware:

Product ID

Description

Cisco UCS M4-based Cisco APIC

The Cisco UCS M4-based Cisco APIC and previous versions support only the 10G interface. Connecting the Cisco APIC to the Cisco ACI fabric requires a same speed interface on the Cisco ACI leaf switch. You cannot connect the Cisco APIC directly to the Cisco N9332PQ ACI leaf switch, unless you use a 40G to 10G converter (part number CVR-QSFP-SFP10G), in which case the port on the Cisco N9332PQ switch auto-negotiates to 10G without requiring any manual configuration.

Cisco UCS M5-based Cisco APIC

The Cisco UCS M5-based Cisco APIC supports dual speed 10G and 25G interfaces. Connecting the Cisco APIC to the Cisco ACI fabric requires a same speed interface on the Cisco ACI leaf switch. You cannot connect the Cisco APIC directly to the Cisco N9332PQ ACI leaf switch, unless you use a 40G to 10G converter (part number CVR-QSFP-SFP10G), in which case the port on the Cisco N9332PQ switch auto-negotiates to 10G without requiring any manual configuration.

N2348UPQ

To connect the N2348UPQ to Cisco ACI leaf switches, the following options are available:

Directly connect the 40G FEX ports on the N2348UPQ to the 40G switch ports on the Cisco ACI leaf switches

Break out the 40G FEX ports on the N2348UPQ to 4x10G ports and connect to the 10G ports on all other Cisco ACI leaf switches.

Note: A fabric uplink port cannot be used as a FEX fabric port.

N9K-C9348GC-FXP

This switch does not read SPROM information if the PSU is in a shut state. You might see an empty string in the Cisco APIC output.

N9K-C9364C-FX

Ports 49-64 do not support 1G SFPs with QSA.

N9K-C9508-FM-E

The Cisco N9K-C9508-FM-E2 and N9K-C9508-FM-E fabric modules in the mixed mode configuration are not supported on the same spine switch.

N9K-C9508-FM-E2

The Cisco N9K-C9508-FM-E2 and N9K-C9508-FM-E fabric modules in the mixed mode configuration are not supported on the same spine switch.

The locator LED enable/disable feature is supported in the GUI and not supported in the Cisco ACI NX-OS switch CLI.

N9K-C9508-FM-E2

This fabric module must be physically removed before downgrading to releases earlier than Cisco APIC 3.0(1).

N9K-X9736C-FX

The locator LED enable/disable feature is supported in the GUI and not supported in the Cisco ACI NX-OS Switch CLI.

N9K-X9736C-FX

Ports 29 to 36 do not support 1G SFPs with QSA.

Miscellaneous Compatibility Information

This release supports the following products:

Product

Supported Release

Cisco NX-OS

15.2(8)

Cisco UCS Manager

2.2(1c) or later is required for the Cisco UCS Fabric Interconnect and other components, including the BIOS, CIMC, and the adapter.

CIMC HUU ISO

Network Insights Base, Network Insights Advisor, and Network Insights for Resources

For the release information, documentation, and download links, see the Cisco Network Insights for Data Center page.

For the supported releases, see the Cisco Data Center Networking Applications Compatibility Matrix.


· A known issue exists with the Safari browser and unsigned certificates, which applies when connecting to the Cisco APIC GUI. For more information, see the Cisco APIC Getting Started Guide, Release 5.2(x).

· For compatibility with Day-2 Operations apps, see the Cisco Data Center Networking Applications Compatibility Matrix.

· Cisco Nexus Dashboard Insights creates a user in Cisco APIC called cisco_SN_NI. This user is used when Nexus Dashboard Insights needs to make any changes or query any information from the Cisco APIC. In the Cisco APIC, navigate to the Audit Logs tab of the System > History page. The cisco_SN_NI user is displayed in the User column.

Related Content

See the Cisco Application Policy Infrastructure Controller (APIC) page for the documentation.

The documentation includes installation, upgrade, configuration, programming, and troubleshooting guides, technical references, release notes, and knowledge base (KB) articles, as well as other documentation. KB articles provide information about a specific use case or a specific topic.

By using the "Choose a topic" and "Choose a document type" fields of the APIC documentation website, you can narrow down the displayed documentation list to make it easier to find the desired document.

You can watch videos that demonstrate how to perform specific tasks in the Cisco APIC on the Cisco Data Center Networking YouTube channel.

Temporary licenses with an expiry date are available for evaluation and lab use purposes. They are strictly not allowed to be used in production. Use a permanent or subscription license that has been purchased through Cisco for production purposes. For more information, go to Cisco Data Center Networking Software Subscriptions.

The following table provides links to the release notes, verified scalability documentation, and new documentation:

Document

Description

Cisco Nexus 9000 ACI-Mode Switches Release Notes, Release 15.2(8)

The release notes for Cisco NX-OS for Cisco Nexus 9000 Series ACI-Mode Switches.

Verified Scalability Guide for Cisco APIC, Release 5.2(8) and Cisco Nexus 9000 Series ACI-Mode Switches, Release 15.2(8)

This guide contains the maximum verified scalability limits for Cisco Application Centric Infrastructure (ACI) parameters for Cisco APIC and Cisco Nexus 9000 Series ACI-Mode Switches.


Introduction

The Cisco Application Centric Infrastructure (ACI) is an architecture that allows the application to define the networking requirements in a programmatic way. This architecture simplifies, optimizes, and accelerates the entire application deployment lifecycle. Cisco Application Policy Infrastructure Controller (APIC) is the software, or operating system, that acts as the controller.

This document describes the features, issues, and limitations for the Cisco APIC software. For the features, issues, and limitations for the Cisco NX-OS software for the Cisco Nexus 9000 series switches, see the Cisco Nexus 9000 ACI-Mode Switches Release Notes, Release 15.2(8).

For more information about this product, see "Related Content."

Date

Description

August 9, 2023

Release 5.2(8e) became available; there are no changes to this document for this release. See the Cisco Nexus 9000 ACI-Mode Switches Release Notes, Release 15.2(8) for the changes in this release.

June 29, 2023

Release 5.2(8d) became available.

New Software Features

Product Impact

Feature

Description

Base Functionality

Source IP address in the audit log messages

The audit log messages now include the IP address of the source of the configuration change.

Base Functionality

Enhanced timestamp for UDP transport syslog messages

Cisco APIC supports a new enhanced timestamp mode for syslog to include the year in the timestamp for UDP transport.

Interoperability

Support for VMware vSphere 8.0

VMM domains now support VMware vSphere 8.0.

For more information, see the Cisco ACI vCenter Plug-in chapter in the Cisco ACI Virtualization Guide, 5.2(x) and Cisco ACI Virtual Edge Release Notes, Release 3.2(4).

New Hardware Features

For the new hardware features, see the Cisco Nexus 9000 ACI-Mode Switches Release Notes, Release 15.2(8).

Changes in Behavior

· There are no changes in behavior in this release.

Open Issues

Click the bug ID to access the Bug Search tool and see additional information about the bug. The "Exists In" column of the table specifies the 5.2(8) releases in which the bug exists. A bug might also exist in releases other than the 5.2(8) releases.

Bug ID

Description

Exists in

CSCvt99966

A SPAN session with the source type set to "Routed-Outside" goes down. The SPAN configuration is pushed to the anchor or non-anchor nodes, but the interfaces are not pushed due to the following fault: "Failed to configure SPAN with source SpanFL3out due to Source fvIfConn not available".

5.2(8d) and later

CSCvy40511

Traffic from an endpoint under a remote leaf switch to an external node and its attached external networks is dropped. This occurs if the external node is attached to an L3Out with a vPC and there is a redistribution configuration on the L3Out to advertise the reachability of the external nodes as direct-attached hosts.

5.2(8d) and later

CSCvz72941

While performing ID recovery, id-import gets timed out. Due to this, ID recovery fails.

5.2(8d) and later

CSCvz83636

For a health record query using the last page and a time range, the GUI displays some health records with a creation time that are beyond the time range (such as 24h).

5.2(8d) and later

CSCwa90058

When a VRF-level subnet <fvRtSummSubnet> and instP-level subnet <l3extSubnet> with a summary policy is configured for an overlapping subnet, the routes will get summarized by the configuration that was added first. But, the fault on the configuration that was added last will not be shown in the Cisco APIC GUI.

5.2(8d) and later

CSCwa90084

- Traffic disruption across a vPC pair on a given encapsulation.

OR

- EPG flood in encap blackholing on a given encapsulation.

OR

- STP packets received on an encapsulation on a given port are not forwarded on all the leaf switches where the same EPG/same encapsulation is deployed.

5.2(8d) and later

CSCwd26277

When deploying a service graph, the dialog does not list all bridge domains for the provider connector. This issue is observed when you enter or edit the bridge domain name in the consumer connector field. After this, the provider connector will only list the bridge domain that is selected by the consumer connector field.

5.2(8d) and later

CSCwe39988

The Cisco APIC GUI becomes unresponsive when there is large configuration for given tenant and VRF instance.

5.2(8d) and later

CSCwf04318

In a Cisco APIC 5.2 release, a user in a restricted security domain is allowed to create a VRF instance, but that same restricted mode user is not allowed to delete that VRF instance.

5.2(8d) and later

CSCwf54771

User configuration is missing on APICs and switches following an ungraceful reload or power outage.

5.2(8d) and later

Resolved Issues

Bug ID

Description

Fixed in

CSCwc11570

In certain configuration sequences, bridge domain routes (and consequently, host routes) are not advertised out of GOLF and ACI Anywhere L3Outs.

5.2(8d)

CSCwc66053

Preconfiguration validations for L3Outs that occur whenever a new configuration is pushed to the Cisco APIC might not get triggered.

5.2(8d)

CSCwd44827

APIC fails to update standalone Layer3 controller subnet on its fabric. The following fault is raised:

F609478 - [FSM:FAILED]: Update Standalone Controller Subnet(TASK:ifc:policymgr:FabricCtrlrConfigPUpdatePodConnPDef)

5.2(8d)

CSCwd88076

This issue is seen when Syslog is configured with CA signed certificates.The TLS connection was terminated due to certificate validation failure. This is not correct behavior and TLS connection should be established for CA signed certificates too.

5.2(8d)

CSCwd90130

After performing an interface migration from the old selector-based style to the new per-port configuration, an interface with an active override might not work as before the migration.

5.2(8d)

CSCwe09859

The OpFlex agent on a hypervisor randomly disconnects, impacting VM traffic.

5.2(8d)

CSCwe19885

The Nexus Insights application cannot stream the telemetry data to NDI, even though the Cisco ACI site is registered and active.

5.2(8d)

CSCwe25534

When an IPv6 address is added as the BGP peer address, the APIC does not validate the IPv6 address if the address contains any letters.

5.2(8d)

CSCwe64159

Podman is unstable, and gets restarted every 5 seconds, even after running the podman_cleanup.py script. The scheduler services show all the services are running on all the Cisco APICs.

5.2(8d)

CSCwe96230

The Appliance Element (AE) service gets restarted several times by the cgroup process (oom-killer).

5.2(8d)

CSCwf11826

Endpoints are moved from a Tag-based ESG to the base EPG. Tag-to-VM associations are lost and not refreshed for Tags used in ESG selectors.

5.2(8d)

CSCwf19660

Major fault F3083 ("IP detected on multiple MACs") is raised under a uEPG and ESG when an EPG selector is configured in the ESG.

5.2(8d)

CSCwf29094

The error "Error:Server did not return JSON response" occurs in Nexus Cloud Connectivity in any of the Cisco APICs.

5.2(8d)

Known Issues

Click the bug ID to access the Bug Search tool and see additional information about the bug. The "Exists In" column of the table specifies the 5.2(8) releases in which the bug exists. A bug might also exist in releases other than the 5.2(8) releases.

Bug ID

Description

Exists in

CSCuu11416

An endpoint-to-endpoint ACI policy that uses Layer 2 traffic with an IPv6 header does not get counted within or across ESGs/EPGs.

5.2(8d) and later

CSCvj26666

The "show run leaf|spine <nodeId>" command might produce an error for scaled up configurations.

5.2(8d) and later

CSCvj90385

With a uniform distribution of EPs and traffic flows, a fabric module in slot 25 sometimes reports far less than 50% of the traffic compared to the traffic on fabric modules in non-FM25 slots.

5.2(8d) and later

CSCvm71833

Switch upgrades fail with the following error: Version not compatible.

5.2(8d) and later

CSCvq39764

When you click Restart for the Microsoft System Center Virtual Machine Manager (SCVMM) agent on a scaled-out setup, the service may stop. You can restart the agent by clicking Start.

5.2(8d) and later

CSCvq58953

One of the following symptoms occurs:

App installation/enable/disable takes a long time and does not complete.

Nomad leadership is lost. The output of the acidiag scheduler logs members command contains the following error:

Error querying node status: Unexpected response code: 500 (rpc error: No cluster leader)

5.2(8d) and later

CSCvr89603

The CRC and stomped CRC error values do not match when seen from the APIC CLI compared to the APIC GUI. This is expected behavior. The GUI values are from the history data, whereas the CLI values are from the current data.

5.2(8d) and later

CSCvs19322

Upgrading Cisco APIC from a 3.x release to a 4.x or later release causes Smart Licensing to lose its registration. Registering Smart Licensing again will clear the fault.

5.2(8d) and later

CSCvs77929

In the 4.x and later releases, if a firmware policy is created with different name than the maintenance policy, the firmware policy will be deleted and a new firmware policy gets created with the same name, which causes the upgrade process to fail.

5.2(8d) and later

CSCvx75380

svcredirDestmon objects get programmed in all of the leaf switches where the service L3Out is deployed, even though the service node may not be connected to some of the leaf switch.

There is no impact to traffic.

5.2(8d) and later

CSCvx78018

A remote leaf switch has momentary traffic loss for flushed endpoints as the traffic goes through the tglean path and does not directly go through the spine switch proxy path.

5.2(8d) and later

CSCvy07935

xR IP flush for all endpoints under the bridge domain subnets of the EPG being migrated to ESG. This will lead to a temporary traffic loss on remote leaf switch for all EPGs in the bridge domain. Traffic is expected to recover.

5.2(8d) and later

CSCvy10946

With the floating L3Out multipath recursive feature, if a static route with multipath is configured, not all paths are installed at the non-border leaf switch/non-anchor nodes.

5.2(8d) and later

CSCvy34357

Starting with the 5.2(8) release, the following apps built with the following non-compliant Docker versions cannot be installed nor run:

· ConnectivityCompliance 1.2

· SevOneAciMonitor 1.0

5.2(8d) and later

CSCvy45358

The file size mentioned in the status managed object for techsupport "dbgexpTechSupStatus" is wrong if the file size is larger than 4GB.

5.2(8d) and later

CSCvz06118

In the "Visibility and Troubleshooting Wizard," ERSPAN support for IPv6 traffic is not available.

5.2(8d) and later

CSCvz84444

While navigating to the last records in the various History sub tabs, it is possible to not see any results. The first, previous, next, and last buttons will then stop working too.

5.2(8d) and later

CSCvz85579

VMMmgr process experiences a very high load for an extended period of time that impacts other operations that involve it.

The process may consume excessive amount of memory and get aborted. This can be confirmed with the command "dmesg -T | grep oom_reaper" if messages such as the following are reported:

oom_reaper: reaped process 5578 (svc_ifc_vmmmgr.)

5.2(8d) and later

CSCwa78573

When the "BGP" branch is expanded in the Fabric > Inventory > POD 1 > Leaf > Protocols > BGP navigation path, the GUI freezes and you cannot navigate to any other page.

This occurs because the APIC gets large set of data in response, which cannot be handled by the browser for parts of the GUI that do not have the pagination.

5.2(8d) and later

CSCwd45200

Hosting server details for AVE endpoints at the operational tab under the EPG is not updated after VM migration.

5.2(8d) and later

CSCwd51537

After changing a VM's name, the name does not get updated for endpoints in the Operational tab of an EPG.

5.2(8d) and later

N/A

If you are upgrading to Cisco APIC release 4.2(6o), 4.2(7l), 5.2(1g), or later, ensure that any VLAN encapsulation blocks that you are explicitly using for leaf switch front panel VLAN programming are set as "external (on the wire)." If these VLAN encapsulation blocks are instead set to "internal," the upgrade causes the front panel port VLAN to be removed, which can result in a datapath outage.

5.2(8d) and later

N/A

Beginning in Cisco APIC release 4.1(1), the IP SLA monitor policy validates the IP SLA port value. Because of the validation, when TCP is configured as the IP SLA type, Cisco APIC no longer accepts an IP SLA port value of 0, which was allowed in previous releases. An IP SLA monitor policy from a previous release that has an IP SLA port value of 0 becomes invalid if the Cisco APIC is upgraded to release 4.1(1) or later. This results in a failure for the configuration import or snapshot rollback.

The workaround is to configure a non-zero IP SLA port value before upgrading the Cisco APIC, and use the snapshot and configuration export that was taken after the IP SLA port change.

5.2(8d) and later

N/A

If you use the REST API to upgrade an app, you must create a new firmware.OSource to be able to download a new app image.

5.2(8d) and later

N/A

In a multipod configuration, before you make any changes to a spine switch, ensure that there is at least one operationally "up" external link that is participating in the multipod topology. Failure to do so could bring down the multipod connectivity. For more information about multipod, see the Cisco Application Centric Infrastructure Fundamentals document and the Cisco APIC Getting Started Guide.

5.2(8d) and later

N/A

With a non-english SCVMM 2012 R2 or SCVMM 2016 setup and where the virtual machine names are specified in non-english characters, if the host is removed and re-added to the host group, the GUID for all the virtual machines under that host changes. Therefore, if a user has created a micro segmentation endpoint group using "VM name" attribute specifying the GUID of respective virtual machine, then that micro segmentation endpoint group will not work if the host (hosting the virtual machines) is removed and re-added to the host group, as the GUID for all the virtual machines would have changed. This does not happen if the virtual name has name specified in all english characters.

5.2(8d) and later

N/A

A query of a configurable policy that does not have a subscription goes to the policy distributor. However, a query of a configurable policy that has a subscription goes to the policy manager. As a result, if the policy propagation from the policy distributor to the policy manager takes a prolonged amount of time, then in such cases the query with the subscription might not return the policy simply because it has not reached policy manager yet.

5.2(8d) and later

N/A

When there are silent hosts across sites, ARP glean messages might not be forwarded to remote sites if a leaf switch without -EX or a later designation in the product ID happens to be in the transit path and the VRF is deployed on that leaf switch, the switch does not forward the ARP glean packet back into the fabric to reach the remote site. This issue is specific to transit leaf switches without -EX or a later designation in the product ID and does not affect leaf switches that have -EX or a later designation in the product ID. This issue breaks the capability of discovering silent hosts.

5.2(8d) and later

N/A

Typically, faults are generally raised based on the presence of the BGP route target profile under the VRF table. However, if a BGP route target profile is configured without actual route targets (that is, the profile has empty policies), a fault will not be raised in this situation.

5.2(8d) and later

N/A

MPLS interface statistics shown in a switch's CLI get cleared after an admin or operational down event.

5.2(8d) and later

N/A

MPLS interface statistics in a switch's CLI are reported every 10 seconds. If, for example, an interface goes down 3 seconds after the collection of the statistics, the CLI reports only 3 seconds of the statistics and clears all of the other statistics.

5.2(8d) and later

Virtualization Compatibility Information

This section lists virtualization compatibility information for the Cisco APIC software.

· For a table that shows the supported virtualization products, see the ACI Virtualization Compatibility Matrix.

· For information about Cisco APIC compatibility with Cisco UCS Director, see the appropriate Cisco UCS Director Compatibility Matrix document.

· If you use Microsoft vSwitch and want to downgrade to Cisco APIC Release 2.3(1) from a later release, you first must delete any microsegment EPGs configured with the Match All filter.

· This release supports the following additional virtualization products:

Product

Supported Release

Information Location

Microsoft Hyper-V

2016 Update Rollup 1, 2, 2.1, and 3

N/A

VMM Integration and VMware Distributed Virtual Switch (DVS)

6.5, 6.7, 7.0, and 8.0

Cisco ACI Virtualization Guide, Release 5.2(x)

Hardware Compatibility Information

This release supports the following Cisco APIC servers:

Product ID

Description

APIC-L2

Cisco APIC with large CPU, hard drive, and memory configurations (more than 1000 edge ports)

APIC-L3

Cisco APIC with large CPU, hard drive, and memory configurations (more than 1200 edge ports)

APIC-M2

Cisco APIC with medium-size CPU, hard drive, and memory configurations (up to 1000 edge ports)

APIC-M3

Cisco APIC with medium-size CPU, hard drive, and memory configurations (up to 1200 edge ports)


The following list includes general hardware compatibility information:

· For the supported hardware, see the Cisco Nexus 9000 ACI-Mode Switches Release Notes, Release 15.2(8).

· Contracts using matchDscp filters are only supported on switches with "EX" on the end of the switch name. For example, N9K-93108TC-EX.

· When the fabric node switch (spine or leaf) is out-of-fabric, the environmental sensor values, such as Current Temperature, Power Draw, and Power Consumption, might be reported as "N/A." A status might be reported as "Normal" even when the Current Temperature is "N/A."

· First generation switches (switches without -EX, -FX, -GX, or a later suffix in the product ID) do not support Contract filters with match type "IPv4" or "IPv6." Only match type "IP" is supported. Because of this, a contract will match both IPv4 and IPv6 traffic when the match type of "IP" is used.

The following table provides compatibility information for specific hardware:

Product ID

Description

Cisco UCS M4-based Cisco APIC

The Cisco UCS M4-based Cisco APIC and previous versions support only the 10G interface. Connecting the Cisco APIC to the Cisco ACI fabric requires a same speed interface on the Cisco ACI leaf switch. You cannot connect the Cisco APIC directly to the Cisco N9332PQ ACI leaf switch, unless you use a 40G to 10G converter (part number CVR-QSFP-SFP10G), in which case the port on the Cisco N9332PQ switch auto-negotiates to 10G without requiring any manual configuration.

Cisco UCS M5-based Cisco APIC

The Cisco UCS M5-based Cisco APIC supports dual speed 10G and 25G interfaces. Connecting the Cisco APIC to the Cisco ACI fabric requires a same speed interface on the Cisco ACI leaf switch. You cannot connect the Cisco APIC directly to the Cisco N9332PQ ACI leaf switch, unless you use a 40G to 10G converter (part number CVR-QSFP-SFP10G), in which case the port on the Cisco N9332PQ switch auto-negotiates to 10G without requiring any manual configuration.

N2348UPQ

To connect the N2348UPQ to Cisco ACI leaf switches, the following options are available:

Directly connect the 40G FEX ports on the N2348UPQ to the 40G switch ports on the Cisco ACI leaf switches

Break out the 40G FEX ports on the N2348UPQ to 4x10G ports and connect to the 10G ports on all other Cisco ACI leaf switches.

Note: A fabric uplink port cannot be used as a FEX fabric port.

N9K-C9348GC-FXP

This switch does not read SPROM information if the PSU is in a shut state. You might see an empty string in the Cisco APIC output.

N9K-C9364C-FX

Ports 49-64 do not support 1G SFPs with QSA.

N9K-C9508-FM-E

The Cisco N9K-C9508-FM-E2 and N9K-C9508-FM-E fabric modules in the mixed mode configuration are not supported on the same spine switch.

N9K-C9508-FM-E2

The Cisco N9K-C9508-FM-E2 and N9K-C9508-FM-E fabric modules in the mixed mode configuration are not supported on the same spine switch.

The locator LED enable/disable feature is supported in the GUI and not supported in the Cisco ACI NX-OS switch CLI.

N9K-C9508-FM-E2

This fabric module must be physically removed before downgrading to releases earlier than Cisco APIC 3.0(1).

N9K-X9736C-FX

The locator LED enable/disable feature is supported in the GUI and not supported in the Cisco ACI NX-OS Switch CLI.

N9K-X9736C-FX

Ports 29 to 36 do not support 1G SFPs with QSA.

Miscellaneous Compatibility Information

This release supports the following products:

Product

Supported Release

Cisco NX-OS

15.2(8)

Cisco UCS Manager

2.2(1c) or later is required for the Cisco UCS Fabric Interconnect and other components, including the BIOS, CIMC, and the adapter.

CIMC HUU ISO

Network Insights Base, Network Insights Advisor, and Network Insights for Resources

For the release information, documentation, and download links, see the Cisco Network Insights for Data Center page.

For the supported releases, see the Cisco Data Center Networking Applications Compatibility Matrix.


· A known issue exists with the Safari browser and unsigned certificates, which applies when connecting to the Cisco APIC GUI. For more information, see the Cisco APIC Getting Started Guide, Release 5.2(x).

· For compatibility with Day-2 Operations apps, see the Cisco Data Center Networking Applications Compatibility Matrix.

· Cisco Nexus Dashboard Insights creates a user in Cisco APIC called cisco_SN_NI. This user is used when Nexus Dashboard Insights needs to make any changes or query any information from the Cisco APIC. In the Cisco APIC, navigate to the Audit Logs tab of the System > History page. The cisco_SN_NI user is displayed in the User column.

Related Content

See the Cisco Application Policy Infrastructure Controller (APIC) page for the documentation.

The documentation includes installation, upgrade, configuration, programming, and troubleshooting guides, technical references, release notes, and knowledge base (KB) articles, as well as other documentation. KB articles provide information about a specific use case or a specific topic.

By using the "Choose a topic" and "Choose a document type" fields of the APIC documentation website, you can narrow down the displayed documentation list to make it easier to find the desired document.

You can watch videos that demonstrate how to perform specific tasks in the Cisco APIC on the Cisco Data Center Networking YouTube channel.

Temporary licenses with an expiry date are available for evaluation and lab use purposes. They are strictly not allowed to be used in production. Use a permanent or subscription license that has been purchased through Cisco for production purposes. For more information, go to Cisco Data Center Networking Software Subscriptions.

The following table provides links to the release notes, verified scalability documentation, and new documentation:

Document

Description

Cisco Nexus 9000 ACI-Mode Switches Release Notes, Release 15.2(8)

The release notes for Cisco NX-OS for Cisco Nexus 9000 Series ACI-Mode Switches.

Verified Scalability Guide for Cisco APIC, Release 5.2(8) and Cisco Nexus 9000 Series ACI-Mode Switches, Release 15.2(8)

This guide contains the maximum verified scalability limits for Cisco Application Centric Infrastructure (ACI) parameters for Cisco APIC and Cisco Nexus 9000 Series ACI-Mode Switches.


Introduction

The Cisco Application Centric Infrastructure (ACI) is an architecture that allows the application to define the networking requirements in a programmatic way. This architecture simplifies, optimizes, and accelerates the entire application deployment lifecycle. Cisco Application Policy Infrastructure Controller (APIC) is the software, or operating system, that acts as the controller.

This document describes the features, issues, and limitations for the Cisco APIC software. For the features, issues, and limitations for the Cisco NX-OS software for the Cisco Nexus 9000 series switches, see the Cisco Nexus 9000 ACI-Mode Switches Release Notes, Release 15.2(8).

For more information about this product, see "Related Content."

Date

Description

August 9, 2023

Release 5.2(8e) became available; there are no changes to this document for this release. See the Cisco Nexus 9000 ACI-Mode Switches Release Notes, Release 15.2(8) for the changes in this release.

June 29, 2023

Release 5.2(8d) became available.

New Software Features

Product Impact

Feature

Description

Base Functionality

Source IP address in the audit log messages

The audit log messages now include the IP address of the source of the configuration change.

Base Functionality

Enhanced timestamp for UDP transport syslog messages

Cisco APIC supports a new enhanced timestamp mode for syslog to include the year in the timestamp for UDP transport.

Interoperability

Support for VMware vSphere 8.0

VMM domains now support VMware vSphere 8.0.

For more information, see the Cisco ACI vCenter Plug-in chapter in the Cisco ACI Virtualization Guide, 5.2(x) and Cisco ACI Virtual Edge Release Notes, Release 3.2(4).

New Hardware Features

For the new hardware features, see the Cisco Nexus 9000 ACI-Mode Switches Release Notes, Release 15.2(8).

Changes in Behavior

· There are no changes in behavior in this release.

Open Issues

Click the bug ID to access the Bug Search tool and see additional information about the bug. The "Exists In" column of the table specifies the 5.2(8) releases in which the bug exists. A bug might also exist in releases other than the 5.2(8) releases.

Bug ID

Description

Exists in

CSCvt99966

A SPAN session with the source type set to "Routed-Outside" goes down. The SPAN configuration is pushed to the anchor or non-anchor nodes, but the interfaces are not pushed due to the following fault: "Failed to configure SPAN with source SpanFL3out due to Source fvIfConn not available".

5.2(8d) and later

CSCvy40511

Traffic from an endpoint under a remote leaf switch to an external node and its attached external networks is dropped. This occurs if the external node is attached to an L3Out with a vPC and there is a redistribution configuration on the L3Out to advertise the reachability of the external nodes as direct-attached hosts.

5.2(8d) and later

CSCvz72941

While performing ID recovery, id-import gets timed out. Due to this, ID recovery fails.

5.2(8d) and later

CSCvz83636

For a health record query using the last page and a time range, the GUI displays some health records with a creation time that are beyond the time range (such as 24h).

5.2(8d) and later

CSCwa90058

When a VRF-level subnet <fvRtSummSubnet> and instP-level subnet <l3extSubnet> with a summary policy is configured for an overlapping subnet, the routes will get summarized by the configuration that was added first. But, the fault on the configuration that was added last will not be shown in the Cisco APIC GUI.

5.2(8d) and later

CSCwa90084

- Traffic disruption across a vPC pair on a given encapsulation.

OR

- EPG flood in encap blackholing on a given encapsulation.

OR

- STP packets received on an encapsulation on a given port are not forwarded on all the leaf switches where the same EPG/same encapsulation is deployed.

5.2(8d) and later

CSCwd26277

When deploying a service graph, the dialog does not list all bridge domains for the provider connector. This issue is observed when you enter or edit the bridge domain name in the consumer connector field. After this, the provider connector will only list the bridge domain that is selected by the consumer connector field.

5.2(8d) and later

CSCwe39988

The Cisco APIC GUI becomes unresponsive when there is large configuration for given tenant and VRF instance.

5.2(8d) and later

CSCwf04318

In a Cisco APIC 5.2 release, a user in a restricted security domain is allowed to create a VRF instance, but that same restricted mode user is not allowed to delete that VRF instance.

5.2(8d) and later

CSCwf54771

User configuration is missing on APICs and switches following an ungraceful reload or power outage.

5.2(8d) and later

Resolved Issues

Bug ID

Description

Fixed in

CSCwc11570

In certain configuration sequences, bridge domain routes (and consequently, host routes) are not advertised out of GOLF and ACI Anywhere L3Outs.

5.2(8d)

CSCwc66053

Preconfiguration validations for L3Outs that occur whenever a new configuration is pushed to the Cisco APIC might not get triggered.

5.2(8d)

CSCwd44827

APIC fails to update standalone Layer3 controller subnet on its fabric. The following fault is raised:

F609478 - [FSM:FAILED]: Update Standalone Controller Subnet(TASK:ifc:policymgr:FabricCtrlrConfigPUpdatePodConnPDef)

5.2(8d)

CSCwd88076

This issue is seen when Syslog is configured with CA signed certificates.The TLS connection was terminated due to certificate validation failure. This is not correct behavior and TLS connection should be established for CA signed certificates too.

5.2(8d)

CSCwd90130

After performing an interface migration from the old selector-based style to the new per-port configuration, an interface with an active override might not work as before the migration.

5.2(8d)

CSCwe09859

The OpFlex agent on a hypervisor randomly disconnects, impacting VM traffic.

5.2(8d)

CSCwe19885

The Nexus Insights application cannot stream the telemetry data to NDI, even though the Cisco ACI site is registered and active.

5.2(8d)

CSCwe25534

When an IPv6 address is added as the BGP peer address, the APIC does not validate the IPv6 address if the address contains any letters.

5.2(8d)

CSCwe64159

Podman is unstable, and gets restarted every 5 seconds, even after running the podman_cleanup.py script. The scheduler services show all the services are running on all the Cisco APICs.

5.2(8d)

CSCwe96230

The Appliance Element (AE) service gets restarted several times by the cgroup process (oom-killer).

5.2(8d)

CSCwf11826

Endpoints are moved from a Tag-based ESG to the base EPG. Tag-to-VM associations are lost and not refreshed for Tags used in ESG selectors.

5.2(8d)

CSCwf19660

Major fault F3083 ("IP detected on multiple MACs") is raised under a uEPG and ESG when an EPG selector is configured in the ESG.

5.2(8d)

CSCwf29094

The error "Error:Server did not return JSON response" occurs in Nexus Cloud Connectivity in any of the Cisco APICs.

5.2(8d)

Known Issues

Click the bug ID to access the Bug Search tool and see additional information about the bug. The "Exists In" column of the table specifies the 5.2(8) releases in which the bug exists. A bug might also exist in releases other than the 5.2(8) releases.

Bug ID

Description

Exists in

CSCuu11416

An endpoint-to-endpoint ACI policy that uses Layer 2 traffic with an IPv6 header does not get counted within or across ESGs/EPGs.

5.2(8d) and later

CSCvj26666

The "show run leaf|spine <nodeId>" command might produce an error for scaled up configurations.

5.2(8d) and later

CSCvj90385

With a uniform distribution of EPs and traffic flows, a fabric module in slot 25 sometimes reports far less than 50% of the traffic compared to the traffic on fabric modules in non-FM25 slots.

5.2(8d) and later

CSCvm71833

Switch upgrades fail with the following error: Version not compatible.

5.2(8d) and later

CSCvq39764

When you click Restart for the Microsoft System Center Virtual Machine Manager (SCVMM) agent on a scaled-out setup, the service may stop. You can restart the agent by clicking Start.

5.2(8d) and later

CSCvq58953

One of the following symptoms occurs:

App installation/enable/disable takes a long time and does not complete.

Nomad leadership is lost. The output of the acidiag scheduler logs members command contains the following error:

Error querying node status: Unexpected response code: 500 (rpc error: No cluster leader)

5.2(8d) and later

CSCvr89603

The CRC and stomped CRC error values do not match when seen from the APIC CLI compared to the APIC GUI. This is expected behavior. The GUI values are from the history data, whereas the CLI values are from the current data.

5.2(8d) and later

CSCvs19322

Upgrading Cisco APIC from a 3.x release to a 4.x or later release causes Smart Licensing to lose its registration. Registering Smart Licensing again will clear the fault.

5.2(8d) and later

CSCvs77929

In the 4.x and later releases, if a firmware policy is created with different name than the maintenance policy, the firmware policy will be deleted and a new firmware policy gets created with the same name, which causes the upgrade process to fail.

5.2(8d) and later

CSCvx75380

svcredirDestmon objects get programmed in all of the leaf switches where the service L3Out is deployed, even though the service node may not be connected to some of the leaf switch.

There is no impact to traffic.

5.2(8d) and later

CSCvx78018

A remote leaf switch has momentary traffic loss for flushed endpoints as the traffic goes through the tglean path and does not directly go through the spine switch proxy path.

5.2(8d) and later

CSCvy07935

xR IP flush for all endpoints under the bridge domain subnets of the EPG being migrated to ESG. This will lead to a temporary traffic loss on remote leaf switch for all EPGs in the bridge domain. Traffic is expected to recover.

5.2(8d) and later

CSCvy10946

With the floating L3Out multipath recursive feature, if a static route with multipath is configured, not all paths are installed at the non-border leaf switch/non-anchor nodes.

5.2(8d) and later

CSCvy34357

Starting with the 5.2(8) release, the following apps built with the following non-compliant Docker versions cannot be installed nor run:

· ConnectivityCompliance 1.2

· SevOneAciMonitor 1.0

5.2(8d) and later

CSCvy45358

The file size mentioned in the status managed object for techsupport "dbgexpTechSupStatus" is wrong if the file size is larger than 4GB.

5.2(8d) and later

CSCvz06118

In the "Visibility and Troubleshooting Wizard," ERSPAN support for IPv6 traffic is not available.

5.2(8d) and later

CSCvz84444

While navigating to the last records in the various History sub tabs, it is possible to not see any results. The first, previous, next, and last buttons will then stop working too.

5.2(8d) and later

CSCvz85579

VMMmgr process experiences a very high load for an extended period of time that impacts other operations that involve it.

The process may consume excessive amount of memory and get aborted. This can be confirmed with the command "dmesg -T | grep oom_reaper" if messages such as the following are reported:

oom_reaper: reaped process 5578 (svc_ifc_vmmmgr.)

5.2(8d) and later

CSCwa78573

When the "BGP" branch is expanded in the Fabric > Inventory > POD 1 > Leaf > Protocols > BGP navigation path, the GUI freezes and you cannot navigate to any other page.

This occurs because the APIC gets large set of data in response, which cannot be handled by the browser for parts of the GUI that do not have the pagination.

5.2(8d) and later

CSCwd45200

Hosting server details for AVE endpoints at the operational tab under the EPG is not updated after VM migration.

5.2(8d) and later

CSCwd51537

After changing a VM's name, the name does not get updated for endpoints in the Operational tab of an EPG.

5.2(8d) and later

N/A

If you are upgrading to Cisco APIC release 4.2(6o), 4.2(7l), 5.2(1g), or later, ensure that any VLAN encapsulation blocks that you are explicitly using for leaf switch front panel VLAN programming are set as "external (on the wire)." If these VLAN encapsulation blocks are instead set to "internal," the upgrade causes the front panel port VLAN to be removed, which can result in a datapath outage.

5.2(8d) and later

N/A

Beginning in Cisco APIC release 4.1(1), the IP SLA monitor policy validates the IP SLA port value. Because of the validation, when TCP is configured as the IP SLA type, Cisco APIC no longer accepts an IP SLA port value of 0, which was allowed in previous releases. An IP SLA monitor policy from a previous release that has an IP SLA port value of 0 becomes invalid if the Cisco APIC is upgraded to release 4.1(1) or later. This results in a failure for the configuration import or snapshot rollback.

The workaround is to configure a non-zero IP SLA port value before upgrading the Cisco APIC, and use the snapshot and configuration export that was taken after the IP SLA port change.

5.2(8d) and later

N/A

If you use the REST API to upgrade an app, you must create a new firmware.OSource to be able to download a new app image.

5.2(8d) and later

N/A

In a multipod configuration, before you make any changes to a spine switch, ensure that there is at least one operationally "up" external link that is participating in the multipod topology. Failure to do so could bring down the multipod connectivity. For more information about multipod, see the Cisco Application Centric Infrastructure Fundamentals document and the Cisco APIC Getting Started Guide.

5.2(8d) and later

N/A

With a non-english SCVMM 2012 R2 or SCVMM 2016 setup and where the virtual machine names are specified in non-english characters, if the host is removed and re-added to the host group, the GUID for all the virtual machines under that host changes. Therefore, if a user has created a micro segmentation endpoint group using "VM name" attribute specifying the GUID of respective virtual machine, then that micro segmentation endpoint group will not work if the host (hosting the virtual machines) is removed and re-added to the host group, as the GUID for all the virtual machines would have changed. This does not happen if the virtual name has name specified in all english characters.

5.2(8d) and later

N/A

A query of a configurable policy that does not have a subscription goes to the policy distributor. However, a query of a configurable policy that has a subscription goes to the policy manager. As a result, if the policy propagation from the policy distributor to the policy manager takes a prolonged amount of time, then in such cases the query with the subscription might not return the policy simply because it has not reached policy manager yet.

5.2(8d) and later

N/A

When there are silent hosts across sites, ARP glean messages might not be forwarded to remote sites if a leaf switch without -EX or a later designation in the product ID happens to be in the transit path and the VRF is deployed on that leaf switch, the switch does not forward the ARP glean packet back into the fabric to reach the remote site. This issue is specific to transit leaf switches without -EX or a later designation in the product ID and does not affect leaf switches that have -EX or a later designation in the product ID. This issue breaks the capability of discovering silent hosts.

5.2(8d) and later

N/A

Typically, faults are generally raised based on the presence of the BGP route target profile under the VRF table. However, if a BGP route target profile is configured without actual route targets (that is, the profile has empty policies), a fault will not be raised in this situation.

5.2(8d) and later

N/A

MPLS interface statistics shown in a switch's CLI get cleared after an admin or operational down event.

5.2(8d) and later

N/A

MPLS interface statistics in a switch's CLI are reported every 10 seconds. If, for example, an interface goes down 3 seconds after the collection of the statistics, the CLI reports only 3 seconds of the statistics and clears all of the other statistics.

5.2(8d) and later

Virtualization Compatibility Information

This section lists virtualization compatibility information for the Cisco APIC software.

· For a table that shows the supported virtualization products, see the ACI Virtualization Compatibility Matrix.

· For information about Cisco APIC compatibility with Cisco UCS Director, see the appropriate Cisco UCS Director Compatibility Matrix document.

· If you use Microsoft vSwitch and want to downgrade to Cisco APIC Release 2.3(1) from a later release, you first must delete any microsegment EPGs configured with the Match All filter.

· This release supports the following additional virtualization products:

Product

Supported Release

Information Location

Microsoft Hyper-V

2016 Update Rollup 1, 2, 2.1, and 3

N/A

VMM Integration and VMware Distributed Virtual Switch (DVS)

6.5, 6.7, 7.0, and 8.0

Cisco ACI Virtualization Guide, Release 5.2(x)

Hardware Compatibility Information

This release supports the following Cisco APIC servers:

Product ID

Description

APIC-L2

Cisco APIC with large CPU, hard drive, and memory configurations (more than 1000 edge ports)

APIC-L3

Cisco APIC with large CPU, hard drive, and memory configurations (more than 1200 edge ports)

APIC-M2

Cisco APIC with medium-size CPU, hard drive, and memory configurations (up to 1000 edge ports)

APIC-M3

Cisco APIC with medium-size CPU, hard drive, and memory configurations (up to 1200 edge ports)


The following list includes general hardware compatibility information:

· For the supported hardware, see the Cisco Nexus 9000 ACI-Mode Switches Release Notes, Release 15.2(8).

· Contracts using matchDscp filters are only supported on switches with "EX" on the end of the switch name. For example, N9K-93108TC-EX.

· When the fabric node switch (spine or leaf) is out-of-fabric, the environmental sensor values, such as Current Temperature, Power Draw, and Power Consumption, might be reported as "N/A." A status might be reported as "Normal" even when the Current Temperature is "N/A."

· First generation switches (switches without -EX, -FX, -GX, or a later suffix in the product ID) do not support Contract filters with match type "IPv4" or "IPv6." Only match type "IP" is supported. Because of this, a contract will match both IPv4 and IPv6 traffic when the match type of "IP" is used.

The following table provides compatibility information for specific hardware:

Product ID

Description

Cisco UCS M4-based Cisco APIC

The Cisco UCS M4-based Cisco APIC and previous versions support only the 10G interface. Connecting the Cisco APIC to the Cisco ACI fabric requires a same speed interface on the Cisco ACI leaf switch. You cannot connect the Cisco APIC directly to the Cisco N9332PQ ACI leaf switch, unless you use a 40G to 10G converter (part number CVR-QSFP-SFP10G), in which case the port on the Cisco N9332PQ switch auto-negotiates to 10G without requiring any manual configuration.

Cisco UCS M5-based Cisco APIC

The Cisco UCS M5-based Cisco APIC supports dual speed 10G and 25G interfaces. Connecting the Cisco APIC to the Cisco ACI fabric requires a same speed interface on the Cisco ACI leaf switch. You cannot connect the Cisco APIC directly to the Cisco N9332PQ ACI leaf switch, unless you use a 40G to 10G converter (part number CVR-QSFP-SFP10G), in which case the port on the Cisco N9332PQ switch auto-negotiates to 10G without requiring any manual configuration.

N2348UPQ

To connect the N2348UPQ to Cisco ACI leaf switches, the following options are available:

Directly connect the 40G FEX ports on the N2348UPQ to the 40G switch ports on the Cisco ACI leaf switches

Break out the 40G FEX ports on the N2348UPQ to 4x10G ports and connect to the 10G ports on all other Cisco ACI leaf switches.

Note: A fabric uplink port cannot be used as a FEX fabric port.

N9K-C9348GC-FXP

This switch does not read SPROM information if the PSU is in a shut state. You might see an empty string in the Cisco APIC output.

N9K-C9364C-FX

Ports 49-64 do not support 1G SFPs with QSA.

N9K-C9508-FM-E

The Cisco N9K-C9508-FM-E2 and N9K-C9508-FM-E fabric modules in the mixed mode configuration are not supported on the same spine switch.

N9K-C9508-FM-E2

The Cisco N9K-C9508-FM-E2 and N9K-C9508-FM-E fabric modules in the mixed mode configuration are not supported on the same spine switch.

The locator LED enable/disable feature is supported in the GUI and not supported in the Cisco ACI NX-OS switch CLI.

N9K-C9508-FM-E2

This fabric module must be physically removed before downgrading to releases earlier than Cisco APIC 3.0(1).

N9K-X9736C-FX

The locator LED enable/disable feature is supported in the GUI and not supported in the Cisco ACI NX-OS Switch CLI.

N9K-X9736C-FX

Ports 29 to 36 do not support 1G SFPs with QSA.

Miscellaneous Compatibility Information

This release supports the following products:

Product

Supported Release

Cisco NX-OS

15.2(8)

Cisco UCS Manager

2.2(1c) or later is required for the Cisco UCS Fabric Interconnect and other components, including the BIOS, CIMC, and the adapter.

CIMC HUU ISO

Network Insights Base, Network Insights Advisor, and Network Insights for Resources

For the release information, documentation, and download links, see the Cisco Network Insights for Data Center page.

For the supported releases, see the Cisco Data Center Networking Applications Compatibility Matrix.


· A known issue exists with the Safari browser and unsigned certificates, which applies when connecting to the Cisco APIC GUI. For more information, see the Cisco APIC Getting Started Guide, Release 5.2(x).

· For compatibility with Day-2 Operations apps, see the Cisco Data Center Networking Applications Compatibility Matrix.

· Cisco Nexus Dashboard Insights creates a user in Cisco APIC called cisco_SN_NI. This user is used when Nexus Dashboard Insights needs to make any changes or query any information from the Cisco APIC. In the Cisco APIC, navigate to the Audit Logs tab of the System > History page. The cisco_SN_NI user is displayed in the User column.

Related Content

See the Cisco Application Policy Infrastructure Controller (APIC) page for the documentation.

The documentation includes installation, upgrade, configuration, programming, and troubleshooting guides, technical references, release notes, and knowledge base (KB) articles, as well as other documentation. KB articles provide information about a specific use case or a specific topic.

By using the "Choose a topic" and "Choose a document type" fields of the APIC documentation website, you can narrow down the displayed documentation list to make it easier to find the desired document.

You can watch videos that demonstrate how to perform specific tasks in the Cisco APIC on the Cisco Data Center Networking YouTube channel.

Temporary licenses with an expiry date are available for evaluation and lab use purposes. They are strictly not allowed to be used in production. Use a permanent or subscription license that has been purchased through Cisco for production purposes. For more information, go to Cisco Data Center Networking Software Subscriptions.

The following table provides links to the release notes, verified scalability documentation, and new documentation:

Document

Description

Cisco Nexus 9000 ACI-Mode Switches Release Notes, Release 15.2(8)

The release notes for Cisco NX-OS for Cisco Nexus 9000 Series ACI-Mode Switches.

Verified Scalability Guide for Cisco APIC, Release 5.2(8) and Cisco Nexus 9000 Series ACI-Mode Switches, Release 15.2(8)

This guide contains the maximum verified scalability limits for Cisco Application Centric Infrastructure (ACI) parameters for Cisco APIC and Cisco Nexus 9000 Series ACI-Mode Switches.


Introduction

The Cisco Application Centric Infrastructure (ACI) is an architecture that allows the application to define the networking requirements in a programmatic way. This architecture simplifies, optimizes, and accelerates the entire application deployment lifecycle. Cisco Application Policy Infrastructure Controller (APIC) is the software, or operating system, that acts as the controller.

This document describes the features, issues, and limitations for the Cisco APIC software. For the features, issues, and limitations for the Cisco NX-OS software for the Cisco Nexus 9000 series switches, see the Cisco Nexus 9000 ACI-Mode Switches Release Notes, Release 15.2(8).

For more information about this product, see "Related Content."

Date

Description

August 9, 2023

Release 5.2(8e) became available; there are no changes to this document for this release. See the Cisco Nexus 9000 ACI-Mode Switches Release Notes, Release 15.2(8) for the changes in this release.

June 29, 2023

Release 5.2(8d) became available.

New Software Features

Product Impact

Feature

Description

Base Functionality

Source IP address in the audit log messages

The audit log messages now include the IP address of the source of the configuration change.

Base Functionality

Enhanced timestamp for UDP transport syslog messages

Cisco APIC supports a new enhanced timestamp mode for syslog to include the year in the timestamp for UDP transport.

Interoperability

Support for VMware vSphere 8.0

VMM domains now support VMware vSphere 8.0.

For more information, see the Cisco ACI vCenter Plug-in chapter in the Cisco ACI Virtualization Guide, 5.2(x) and Cisco ACI Virtual Edge Release Notes, Release 3.2(4).

New Hardware Features

For the new hardware features, see the Cisco Nexus 9000 ACI-Mode Switches Release Notes, Release 15.2(8).

Changes in Behavior

· There are no changes in behavior in this release.

Open Issues

Click the bug ID to access the Bug Search tool and see additional information about the bug. The "Exists In" column of the table specifies the 5.2(8) releases in which the bug exists. A bug might also exist in releases other than the 5.2(8) releases.

Bug ID

Description

Exists in

CSCvt99966

A SPAN session with the source type set to "Routed-Outside" goes down. The SPAN configuration is pushed to the anchor or non-anchor nodes, but the interfaces are not pushed due to the following fault: "Failed to configure SPAN with source SpanFL3out due to Source fvIfConn not available".

5.2(8d) and later

CSCvy40511

Traffic from an endpoint under a remote leaf switch to an external node and its attached external networks is dropped. This occurs if the external node is attached to an L3Out with a vPC and there is a redistribution configuration on the L3Out to advertise the reachability of the external nodes as direct-attached hosts.

5.2(8d) and later

CSCvz72941

While performing ID recovery, id-import gets timed out. Due to this, ID recovery fails.

5.2(8d) and later

CSCvz83636

For a health record query using the last page and a time range, the GUI displays some health records with a creation time that are beyond the time range (such as 24h).

5.2(8d) and later

CSCwa90058

When a VRF-level subnet <fvRtSummSubnet> and instP-level subnet <l3extSubnet> with a summary policy is configured for an overlapping subnet, the routes will get summarized by the configuration that was added first. But, the fault on the configuration that was added last will not be shown in the Cisco APIC GUI.

5.2(8d) and later

CSCwa90084

- Traffic disruption across a vPC pair on a given encapsulation.

OR

- EPG flood in encap blackholing on a given encapsulation.

OR

- STP packets received on an encapsulation on a given port are not forwarded on all the leaf switches where the same EPG/same encapsulation is deployed.

5.2(8d) and later

CSCwd26277

When deploying a service graph, the dialog does not list all bridge domains for the provider connector. This issue is observed when you enter or edit the bridge domain name in the consumer connector field. After this, the provider connector will only list the bridge domain that is selected by the consumer connector field.

5.2(8d) and later

CSCwe39988

The Cisco APIC GUI becomes unresponsive when there is large configuration for given tenant and VRF instance.

5.2(8d) and later

CSCwf04318

In a Cisco APIC 5.2 release, a user in a restricted security domain is allowed to create a VRF instance, but that same restricted mode user is not allowed to delete that VRF instance.

5.2(8d) and later

CSCwf54771

User configuration is missing on APICs and switches following an ungraceful reload or power outage.

5.2(8d) and later

Resolved Issues

Bug ID

Description

Fixed in

CSCwc11570

In certain configuration sequences, bridge domain routes (and consequently, host routes) are not advertised out of GOLF and ACI Anywhere L3Outs.

5.2(8d)

CSCwc66053

Preconfiguration validations for L3Outs that occur whenever a new configuration is pushed to the Cisco APIC might not get triggered.

5.2(8d)

CSCwd44827

APIC fails to update standalone Layer3 controller subnet on its fabric. The following fault is raised:

F609478 - [FSM:FAILED]: Update Standalone Controller Subnet(TASK:ifc:policymgr:FabricCtrlrConfigPUpdatePodConnPDef)

5.2(8d)

CSCwd88076

This issue is seen when Syslog is configured with CA signed certificates.The TLS connection was terminated due to certificate validation failure. This is not correct behavior and TLS connection should be established for CA signed certificates too.

5.2(8d)

CSCwd90130

After performing an interface migration from the old selector-based style to the new per-port configuration, an interface with an active override might not work as before the migration.

5.2(8d)

CSCwe09859

The OpFlex agent on a hypervisor randomly disconnects, impacting VM traffic.

5.2(8d)

CSCwe19885

The Nexus Insights application cannot stream the telemetry data to NDI, even though the Cisco ACI site is registered and active.

5.2(8d)

CSCwe25534

When an IPv6 address is added as the BGP peer address, the APIC does not validate the IPv6 address if the address contains any letters.

5.2(8d)

CSCwe64159

Podman is unstable, and gets restarted every 5 seconds, even after running the podman_cleanup.py script. The scheduler services show all the services are running on all the Cisco APICs.

5.2(8d)

CSCwe96230

The Appliance Element (AE) service gets restarted several times by the cgroup process (oom-killer).

5.2(8d)

CSCwf11826

Endpoints are moved from a Tag-based ESG to the base EPG. Tag-to-VM associations are lost and not refreshed for Tags used in ESG selectors.

5.2(8d)

CSCwf19660

Major fault F3083 ("IP detected on multiple MACs") is raised under a uEPG and ESG when an EPG selector is configured in the ESG.

5.2(8d)

CSCwf29094

The error "Error:Server did not return JSON response" occurs in Nexus Cloud Connectivity in any of the Cisco APICs.

5.2(8d)

Known Issues

Click the bug ID to access the Bug Search tool and see additional information about the bug. The "Exists In" column of the table specifies the 5.2(8) releases in which the bug exists. A bug might also exist in releases other than the 5.2(8) releases.

Bug ID

Description

Exists in

CSCuu11416

An endpoint-to-endpoint ACI policy that uses Layer 2 traffic with an IPv6 header does not get counted within or across ESGs/EPGs.

5.2(8d) and later

CSCvj26666

The "show run leaf|spine <nodeId>" command might produce an error for scaled up configurations.

5.2(8d) and later

CSCvj90385

With a uniform distribution of EPs and traffic flows, a fabric module in slot 25 sometimes reports far less than 50% of the traffic compared to the traffic on fabric modules in non-FM25 slots.

5.2(8d) and later

CSCvm71833

Switch upgrades fail with the following error: Version not compatible.

5.2(8d) and later

CSCvq39764

When you click Restart for the Microsoft System Center Virtual Machine Manager (SCVMM) agent on a scaled-out setup, the service may stop. You can restart the agent by clicking Start.

5.2(8d) and later

CSCvq58953

One of the following symptoms occurs:

App installation/enable/disable takes a long time and does not complete.

Nomad leadership is lost. The output of the acidiag scheduler logs members command contains the following error:

Error querying node status: Unexpected response code: 500 (rpc error: No cluster leader)

5.2(8d) and later

CSCvr89603

The CRC and stomped CRC error values do not match when seen from the APIC CLI compared to the APIC GUI. This is expected behavior. The GUI values are from the history data, whereas the CLI values are from the current data.

5.2(8d) and later

CSCvs19322

Upgrading Cisco APIC from a 3.x release to a 4.x or later release causes Smart Licensing to lose its registration. Registering Smart Licensing again will clear the fault.

5.2(8d) and later

CSCvs77929

In the 4.x and later releases, if a firmware policy is created with different name than the maintenance policy, the firmware policy will be deleted and a new firmware policy gets created with the same name, which causes the upgrade process to fail.

5.2(8d) and later

CSCvx75380

svcredirDestmon objects get programmed in all of the leaf switches where the service L3Out is deployed, even though the service node may not be connected to some of the leaf switch.

There is no impact to traffic.

5.2(8d) and later

CSCvx78018

A remote leaf switch has momentary traffic loss for flushed endpoints as the traffic goes through the tglean path and does not directly go through the spine switch proxy path.

5.2(8d) and later

CSCvy07935

xR IP flush for all endpoints under the bridge domain subnets of the EPG being migrated to ESG. This will lead to a temporary traffic loss on remote leaf switch for all EPGs in the bridge domain. Traffic is expected to recover.

5.2(8d) and later

CSCvy10946

With the floating L3Out multipath recursive feature, if a static route with multipath is configured, not all paths are installed at the non-border leaf switch/non-anchor nodes.

5.2(8d) and later

CSCvy34357

Starting with the 5.2(8) release, the following apps built with the following non-compliant Docker versions cannot be installed nor run:

· ConnectivityCompliance 1.2

· SevOneAciMonitor 1.0

5.2(8d) and later

CSCvy45358

The file size mentioned in the status managed object for techsupport "dbgexpTechSupStatus" is wrong if the file size is larger than 4GB.

5.2(8d) and later

CSCvz06118

In the "Visibility and Troubleshooting Wizard," ERSPAN support for IPv6 traffic is not available.

5.2(8d) and later

CSCvz84444

While navigating to the last records in the various History sub tabs, it is possible to not see any results. The first, previous, next, and last buttons will then stop working too.

5.2(8d) and later

CSCvz85579

VMMmgr process experiences a very high load for an extended period of time that impacts other operations that involve it.

The process may consume excessive amount of memory and get aborted. This can be confirmed with the command "dmesg -T | grep oom_reaper" if messages such as the following are reported:

oom_reaper: reaped process 5578 (svc_ifc_vmmmgr.)

5.2(8d) and later

CSCwa78573

When the "BGP" branch is expanded in the Fabric > Inventory > POD 1 > Leaf > Protocols > BGP navigation path, the GUI freezes and you cannot navigate to any other page.

This occurs because the APIC gets large set of data in response, which cannot be handled by the browser for parts of the GUI that do not have the pagination.

5.2(8d) and later

CSCwd45200

Hosting server details for AVE endpoints at the operational tab under the EPG is not updated after VM migration.

5.2(8d) and later

CSCwd51537

After changing a VM's name, the name does not get updated for endpoints in the Operational tab of an EPG.

5.2(8d) and later

N/A

If you are upgrading to Cisco APIC release 4.2(6o), 4.2(7l), 5.2(1g), or later, ensure that any VLAN encapsulation blocks that you are explicitly using for leaf switch front panel VLAN programming are set as "external (on the wire)." If these VLAN encapsulation blocks are instead set to "internal," the upgrade causes the front panel port VLAN to be removed, which can result in a datapath outage.

5.2(8d) and later

N/A

Beginning in Cisco APIC release 4.1(1), the IP SLA monitor policy validates the IP SLA port value. Because of the validation, when TCP is configured as the IP SLA type, Cisco APIC no longer accepts an IP SLA port value of 0, which was allowed in previous releases. An IP SLA monitor policy from a previous release that has an IP SLA port value of 0 becomes invalid if the Cisco APIC is upgraded to release 4.1(1) or later. This results in a failure for the configuration import or snapshot rollback.

The workaround is to configure a non-zero IP SLA port value before upgrading the Cisco APIC, and use the snapshot and configuration export that was taken after the IP SLA port change.

5.2(8d) and later

N/A

If you use the REST API to upgrade an app, you must create a new firmware.OSource to be able to download a new app image.

5.2(8d) and later

N/A

In a multipod configuration, before you make any changes to a spine switch, ensure that there is at least one operationally "up" external link that is participating in the multipod topology. Failure to do so could bring down the multipod connectivity. For more information about multipod, see the Cisco Application Centric Infrastructure Fundamentals document and the Cisco APIC Getting Started Guide.

5.2(8d) and later

N/A

With a non-english SCVMM 2012 R2 or SCVMM 2016 setup and where the virtual machine names are specified in non-english characters, if the host is removed and re-added to the host group, the GUID for all the virtual machines under that host changes. Therefore, if a user has created a micro segmentation endpoint group using "VM name" attribute specifying the GUID of respective virtual machine, then that micro segmentation endpoint group will not work if the host (hosting the virtual machines) is removed and re-added to the host group, as the GUID for all the virtual machines would have changed. This does not happen if the virtual name has name specified in all english characters.

5.2(8d) and later

N/A

A query of a configurable policy that does not have a subscription goes to the policy distributor. However, a query of a configurable policy that has a subscription goes to the policy manager. As a result, if the policy propagation from the policy distributor to the policy manager takes a prolonged amount of time, then in such cases the query with the subscription might not return the policy simply because it has not reached policy manager yet.

5.2(8d) and later

N/A

When there are silent hosts across sites, ARP glean messages might not be forwarded to remote sites if a leaf switch without -EX or a later designation in the product ID happens to be in the transit path and the VRF is deployed on that leaf switch, the switch does not forward the ARP glean packet back into the fabric to reach the remote site. This issue is specific to transit leaf switches without -EX or a later designation in the product ID and does not affect leaf switches that have -EX or a later designation in the product ID. This issue breaks the capability of discovering silent hosts.

5.2(8d) and later

N/A

Typically, faults are generally raised based on the presence of the BGP route target profile under the VRF table. However, if a BGP route target profile is configured without actual route targets (that is, the profile has empty policies), a fault will not be raised in this situation.

5.2(8d) and later

N/A

MPLS interface statistics shown in a switch's CLI get cleared after an admin or operational down event.

5.2(8d) and later

N/A

MPLS interface statistics in a switch's CLI are reported every 10 seconds. If, for example, an interface goes down 3 seconds after the collection of the statistics, the CLI reports only 3 seconds of the statistics and clears all of the other statistics.

5.2(8d) and later

Virtualization Compatibility Information

This section lists virtualization compatibility information for the Cisco APIC software.

· For a table that shows the supported virtualization products, see the ACI Virtualization Compatibility Matrix.

· For information about Cisco APIC compatibility with Cisco UCS Director, see the appropriate Cisco UCS Director Compatibility Matrix document.

· If you use Microsoft vSwitch and want to downgrade to Cisco APIC Release 2.3(1) from a later release, you first must delete any microsegment EPGs configured with the Match All filter.

· This release supports the following additional virtualization products:

Product

Supported Release

Information Location

Microsoft Hyper-V

2016 Update Rollup 1, 2, 2.1, and 3

N/A

VMM Integration and VMware Distributed Virtual Switch (DVS)

6.5, 6.7, 7.0, and 8.0

Cisco ACI Virtualization Guide, Release 5.2(x)

Hardware Compatibility Information

This release supports the following Cisco APIC servers:

Product ID

Description

APIC-L2

Cisco APIC with large CPU, hard drive, and memory configurations (more than 1000 edge ports)

APIC-L3

Cisco APIC with large CPU, hard drive, and memory configurations (more than 1200 edge ports)

APIC-M2

Cisco APIC with medium-size CPU, hard drive, and memory configurations (up to 1000 edge ports)

APIC-M3

Cisco APIC with medium-size CPU, hard drive, and memory configurations (up to 1200 edge ports)


The following list includes general hardware compatibility information:

· For the supported hardware, see the Cisco Nexus 9000 ACI-Mode Switches Release Notes, Release 15.2(8).

· Contracts using matchDscp filters are only supported on switches with "EX" on the end of the switch name. For example, N9K-93108TC-EX.

· When the fabric node switch (spine or leaf) is out-of-fabric, the environmental sensor values, such as Current Temperature, Power Draw, and Power Consumption, might be reported as "N/A." A status might be reported as "Normal" even when the Current Temperature is "N/A."

· First generation switches (switches without -EX, -FX, -GX, or a later suffix in the product ID) do not support Contract filters with match type "IPv4" or "IPv6." Only match type "IP" is supported. Because of this, a contract will match both IPv4 and IPv6 traffic when the match type of "IP" is used.

The following table provides compatibility information for specific hardware:

Product ID

Description

Cisco UCS M4-based Cisco APIC

The Cisco UCS M4-based Cisco APIC and previous versions support only the 10G interface. Connecting the Cisco APIC to the Cisco ACI fabric requires a same speed interface on the Cisco ACI leaf switch. You cannot connect the Cisco APIC directly to the Cisco N9332PQ ACI leaf switch, unless you use a 40G to 10G converter (part number CVR-QSFP-SFP10G), in which case the port on the Cisco N9332PQ switch auto-negotiates to 10G without requiring any manual configuration.

Cisco UCS M5-based Cisco APIC

The Cisco UCS M5-based Cisco APIC supports dual speed 10G and 25G interfaces. Connecting the Cisco APIC to the Cisco ACI fabric requires a same speed interface on the Cisco ACI leaf switch. You cannot connect the Cisco APIC directly to the Cisco N9332PQ ACI leaf switch, unless you use a 40G to 10G converter (part number CVR-QSFP-SFP10G), in which case the port on the Cisco N9332PQ switch auto-negotiates to 10G without requiring any manual configuration.

N2348UPQ

To connect the N2348UPQ to Cisco ACI leaf switches, the following options are available:

Directly connect the 40G FEX ports on the N2348UPQ to the 40G switch ports on the Cisco ACI leaf switches

Break out the 40G FEX ports on the N2348UPQ to 4x10G ports and connect to the 10G ports on all other Cisco ACI leaf switches.

Note: A fabric uplink port cannot be used as a FEX fabric port.

N9K-C9348GC-FXP

This switch does not read SPROM information if the PSU is in a shut state. You might see an empty string in the Cisco APIC output.

N9K-C9364C-FX

Ports 49-64 do not support 1G SFPs with QSA.

N9K-C9508-FM-E

The Cisco N9K-C9508-FM-E2 and N9K-C9508-FM-E fabric modules in the mixed mode configuration are not supported on the same spine switch.

N9K-C9508-FM-E2

The Cisco N9K-C9508-FM-E2 and N9K-C9508-FM-E fabric modules in the mixed mode configuration are not supported on the same spine switch.

The locator LED enable/disable feature is supported in the GUI and not supported in the Cisco ACI NX-OS switch CLI.

N9K-C9508-FM-E2

This fabric module must be physically removed before downgrading to releases earlier than Cisco APIC 3.0(1).

N9K-X9736C-FX

The locator LED enable/disable feature is supported in the GUI and not supported in the Cisco ACI NX-OS Switch CLI.

N9K-X9736C-FX

Ports 29 to 36 do not support 1G SFPs with QSA.

Miscellaneous Compatibility Information

This release supports the following products:

Product

Supported Release

Cisco NX-OS

15.2(8)

Cisco UCS Manager

2.2(1c) or later is required for the Cisco UCS Fabric Interconnect and other components, including the BIOS, CIMC, and the adapter.

CIMC HUU ISO

Network Insights Base, Network Insights Advisor, and Network Insights for Resources

For the release information, documentation, and download links, see the Cisco Network Insights for Data Center page.

For the supported releases, see the Cisco Data Center Networking Applications Compatibility Matrix.


· A known issue exists with the Safari browser and unsigned certificates, which applies when connecting to the Cisco APIC GUI. For more information, see the Cisco APIC Getting Started Guide, Release 5.2(x).

· For compatibility with Day-2 Operations apps, see the Cisco Data Center Networking Applications Compatibility Matrix.

· Cisco Nexus Dashboard Insights creates a user in Cisco APIC called cisco_SN_NI. This user is used when Nexus Dashboard Insights needs to make any changes or query any information from the Cisco APIC. In the Cisco APIC, navigate to the Audit Logs tab of the System > History page. The cisco_SN_NI user is displayed in the User column.

Related Content

See the Cisco Application Policy Infrastructure Controller (APIC) page for the documentation.

The documentation includes installation, upgrade, configuration, programming, and troubleshooting guides, technical references, release notes, and knowledge base (KB) articles, as well as other documentation. KB articles provide information about a specific use case or a specific topic.

By using the "Choose a topic" and "Choose a document type" fields of the APIC documentation website, you can narrow down the displayed documentation list to make it easier to find the desired document.

You can watch videos that demonstrate how to perform specific tasks in the Cisco APIC on the Cisco Data Center Networking YouTube channel.

Temporary licenses with an expiry date are available for evaluation and lab use purposes. They are strictly not allowed to be used in production. Use a permanent or subscription license that has been purchased through Cisco for production purposes. For more information, go to Cisco Data Center Networking Software Subscriptions.

The following table provides links to the release notes, verified scalability documentation, and new documentation:

Document

Description

Cisco Nexus 9000 ACI-Mode Switches Release Notes, Release 15.2(8)

The release notes for Cisco NX-OS for Cisco Nexus 9000 Series ACI-Mode Switches.

Verified Scalability Guide for Cisco APIC, Release 5.2(8) and Cisco Nexus 9000 Series ACI-Mode Switches, Release 15.2(8)

This guide contains the maximum verified scalability limits for Cisco Application Centric Infrastructure (ACI) parameters for Cisco APIC and Cisco Nexus 9000 Series ACI-Mode Switches.


Introduction

The Cisco Application Centric Infrastructure (ACI) is an architecture that allows the application to define the networking requirements in a programmatic way. This architecture simplifies, optimizes, and accelerates the entire application deployment lifecycle. Cisco Application Policy Infrastructure Controller (APIC) is the software, or operating system, that acts as the controller.

This document describes the features, issues, and limitations for the Cisco APIC software. For the features, issues, and limitations for the Cisco NX-OS software for the Cisco Nexus 9000 series switches, see the Cisco Nexus 9000 ACI-Mode Switches Release Notes, Release 15.2(8).

For more information about this product, see "Related Content."

Date

Description

August 9, 2023

Release 5.2(8e) became available; there are no changes to this document for this release. See the Cisco Nexus 9000 ACI-Mode Switches Release Notes, Release 15.2(8) for the changes in this release.

June 29, 2023

Release 5.2(8d) became available.

New Software Features

Product Impact

Feature

Description

Base Functionality

Source IP address in the audit log messages

The audit log messages now include the IP address of the source of the configuration change.

Base Functionality

Enhanced timestamp for UDP transport syslog messages

Cisco APIC supports a new enhanced timestamp mode for syslog to include the year in the timestamp for UDP transport.

Interoperability

Support for VMware vSphere 8.0

VMM domains now support VMware vSphere 8.0.

For more information, see the Cisco ACI vCenter Plug-in chapter in the Cisco ACI Virtualization Guide, 5.2(x) and Cisco ACI Virtual Edge Release Notes, Release 3.2(4).

New Hardware Features

For the new hardware features, see the Cisco Nexus 9000 ACI-Mode Switches Release Notes, Release 15.2(8).

Changes in Behavior

· There are no changes in behavior in this release.

Open Issues

Click the bug ID to access the Bug Search tool and see additional information about the bug. The "Exists In" column of the table specifies the 5.2(8) releases in which the bug exists. A bug might also exist in releases other than the 5.2(8) releases.

Bug ID

Description

Exists in

CSCvt99966

A SPAN session with the source type set to "Routed-Outside" goes down. The SPAN configuration is pushed to the anchor or non-anchor nodes, but the interfaces are not pushed due to the following fault: "Failed to configure SPAN with source SpanFL3out due to Source fvIfConn not available".

5.2(8d) and later

CSCvy40511

Traffic from an endpoint under a remote leaf switch to an external node and its attached external networks is dropped. This occurs if the external node is attached to an L3Out with a vPC and there is a redistribution configuration on the L3Out to advertise the reachability of the external nodes as direct-attached hosts.

5.2(8d) and later

CSCvz72941

While performing ID recovery, id-import gets timed out. Due to this, ID recovery fails.

5.2(8d) and later

CSCvz83636

For a health record query using the last page and a time range, the GUI displays some health records with a creation time that are beyond the time range (such as 24h).

5.2(8d) and later

CSCwa90058

When a VRF-level subnet <fvRtSummSubnet> and instP-level subnet <l3extSubnet> with a summary policy is configured for an overlapping subnet, the routes will get summarized by the configuration that was added first. But, the fault on the configuration that was added last will not be shown in the Cisco APIC GUI.

5.2(8d) and later

CSCwa90084

- Traffic disruption across a vPC pair on a given encapsulation.

OR

- EPG flood in encap blackholing on a given encapsulation.

OR

- STP packets received on an encapsulation on a given port are not forwarded on all the leaf switches where the same EPG/same encapsulation is deployed.

5.2(8d) and later

CSCwd26277

When deploying a service graph, the dialog does not list all bridge domains for the provider connector. This issue is observed when you enter or edit the bridge domain name in the consumer connector field. After this, the provider connector will only list the bridge domain that is selected by the consumer connector field.

5.2(8d) and later

CSCwe39988

The Cisco APIC GUI becomes unresponsive when there is large configuration for given tenant and VRF instance.

5.2(8d) and later

CSCwf04318

In a Cisco APIC 5.2 release, a user in a restricted security domain is allowed to create a VRF instance, but that same restricted mode user is not allowed to delete that VRF instance.

5.2(8d) and later

CSCwf54771

User configuration is missing on APICs and switches following an ungraceful reload or power outage.

5.2(8d) and later

Resolved Issues

Bug ID

Description

Fixed in

CSCwc11570

In certain configuration sequences, bridge domain routes (and consequently, host routes) are not advertised out of GOLF and ACI Anywhere L3Outs.

5.2(8d)

CSCwc66053

Preconfiguration validations for L3Outs that occur whenever a new configuration is pushed to the Cisco APIC might not get triggered.

5.2(8d)

CSCwd44827

APIC fails to update standalone Layer3 controller subnet on its fabric. The following fault is raised:

F609478 - [FSM:FAILED]: Update Standalone Controller Subnet(TASK:ifc:policymgr:FabricCtrlrConfigPUpdatePodConnPDef)

5.2(8d)

CSCwd88076

This issue is seen when Syslog is configured with CA signed certificates.The TLS connection was terminated due to certificate validation failure. This is not correct behavior and TLS connection should be established for CA signed certificates too.

5.2(8d)

CSCwd90130

After performing an interface migration from the old selector-based style to the new per-port configuration, an interface with an active override might not work as before the migration.

5.2(8d)

CSCwe09859

The OpFlex agent on a hypervisor randomly disconnects, impacting VM traffic.

5.2(8d)

CSCwe19885

The Nexus Insights application cannot stream the telemetry data to NDI, even though the Cisco ACI site is registered and active.

5.2(8d)

CSCwe25534

When an IPv6 address is added as the BGP peer address, the APIC does not validate the IPv6 address if the address contains any letters.

5.2(8d)

CSCwe64159

Podman is unstable, and gets restarted every 5 seconds, even after running the podman_cleanup.py script. The scheduler services show all the services are running on all the Cisco APICs.

5.2(8d)

CSCwe96230

The Appliance Element (AE) service gets restarted several times by the cgroup process (oom-killer).

5.2(8d)

CSCwf11826

Endpoints are moved from a Tag-based ESG to the base EPG. Tag-to-VM associations are lost and not refreshed for Tags used in ESG selectors.

5.2(8d)

CSCwf19660

Major fault F3083 ("IP detected on multiple MACs") is raised under a uEPG and ESG when an EPG selector is configured in the ESG.

5.2(8d)

CSCwf29094

The error "Error:Server did not return JSON response" occurs in Nexus Cloud Connectivity in any of the Cisco APICs.

5.2(8d)

Known Issues

Click the bug ID to access the Bug Search tool and see additional information about the bug. The "Exists In" column of the table specifies the 5.2(8) releases in which the bug exists. A bug might also exist in releases other than the 5.2(8) releases.

Bug ID

Description

Exists in

CSCuu11416

An endpoint-to-endpoint ACI policy that uses Layer 2 traffic with an IPv6 header does not get counted within or across ESGs/EPGs.

5.2(8d) and later

CSCvj26666

The "show run leaf|spine <nodeId>" command might produce an error for scaled up configurations.

5.2(8d) and later

CSCvj90385

With a uniform distribution of EPs and traffic flows, a fabric module in slot 25 sometimes reports far less than 50% of the traffic compared to the traffic on fabric modules in non-FM25 slots.

5.2(8d) and later

CSCvm71833

Switch upgrades fail with the following error: Version not compatible.

5.2(8d) and later

CSCvq39764

When you click Restart for the Microsoft System Center Virtual Machine Manager (SCVMM) agent on a scaled-out setup, the service may stop. You can restart the agent by clicking Start.

5.2(8d) and later

CSCvq58953

One of the following symptoms occurs:

App installation/enable/disable takes a long time and does not complete.

Nomad leadership is lost. The output of the acidiag scheduler logs members command contains the following error:

Error querying node status: Unexpected response code: 500 (rpc error: No cluster leader)

5.2(8d) and later

CSCvr89603

The CRC and stomped CRC error values do not match when seen from the APIC CLI compared to the APIC GUI. This is expected behavior. The GUI values are from the history data, whereas the CLI values are from the current data.

5.2(8d) and later

CSCvs19322

Upgrading Cisco APIC from a 3.x release to a 4.x or later release causes Smart Licensing to lose its registration. Registering Smart Licensing again will clear the fault.

5.2(8d) and later

CSCvs77929

In the 4.x and later releases, if a firmware policy is created with different name than the maintenance policy, the firmware policy will be deleted and a new firmware policy gets created with the same name, which causes the upgrade process to fail.

5.2(8d) and later

CSCvx75380

svcredirDestmon objects get programmed in all of the leaf switches where the service L3Out is deployed, even though the service node may not be connected to some of the leaf switch.

There is no impact to traffic.

5.2(8d) and later

CSCvx78018

A remote leaf switch has momentary traffic loss for flushed endpoints as the traffic goes through the tglean path and does not directly go through the spine switch proxy path.

5.2(8d) and later

CSCvy07935

xR IP flush for all endpoints under the bridge domain subnets of the EPG being migrated to ESG. This will lead to a temporary traffic loss on remote leaf switch for all EPGs in the bridge domain. Traffic is expected to recover.

5.2(8d) and later

CSCvy10946

With the floating L3Out multipath recursive feature, if a static route with multipath is configured, not all paths are installed at the non-border leaf switch/non-anchor nodes.

5.2(8d) and later

CSCvy34357

Starting with the 5.2(8) release, the following apps built with the following non-compliant Docker versions cannot be installed nor run:

· ConnectivityCompliance 1.2

· SevOneAciMonitor 1.0

5.2(8d) and later

CSCvy45358

The file size mentioned in the status managed object for techsupport "dbgexpTechSupStatus" is wrong if the file size is larger than 4GB.

5.2(8d) and later

CSCvz06118

In the "Visibility and Troubleshooting Wizard," ERSPAN support for IPv6 traffic is not available.

5.2(8d) and later

CSCvz84444

While navigating to the last records in the various History sub tabs, it is possible to not see any results. The first, previous, next, and last buttons will then stop working too.

5.2(8d) and later

CSCvz85579

VMMmgr process experiences a very high load for an extended period of time that impacts other operations that involve it.

The process may consume excessive amount of memory and get aborted. This can be confirmed with the command "dmesg -T | grep oom_reaper" if messages such as the following are reported:

oom_reaper: reaped process 5578 (svc_ifc_vmmmgr.)

5.2(8d) and later

CSCwa78573

When the "BGP" branch is expanded in the Fabric > Inventory > POD 1 > Leaf > Protocols > BGP navigation path, the GUI freezes and you cannot navigate to any other page.

This occurs because the APIC gets large set of data in response, which cannot be handled by the browser for parts of the GUI that do not have the pagination.

5.2(8d) and later

CSCwd45200

Hosting server details for AVE endpoints at the operational tab under the EPG is not updated after VM migration.

5.2(8d) and later

CSCwd51537

After changing a VM's name, the name does not get updated for endpoints in the Operational tab of an EPG.

5.2(8d) and later

N/A

If you are upgrading to Cisco APIC release 4.2(6o), 4.2(7l), 5.2(1g), or later, ensure that any VLAN encapsulation blocks that you are explicitly using for leaf switch front panel VLAN programming are set as "external (on the wire)." If these VLAN encapsulation blocks are instead set to "internal," the upgrade causes the front panel port VLAN to be removed, which can result in a datapath outage.

5.2(8d) and later

N/A

Beginning in Cisco APIC release 4.1(1), the IP SLA monitor policy validates the IP SLA port value. Because of the validation, when TCP is configured as the IP SLA type, Cisco APIC no longer accepts an IP SLA port value of 0, which was allowed in previous releases. An IP SLA monitor policy from a previous release that has an IP SLA port value of 0 becomes invalid if the Cisco APIC is upgraded to release 4.1(1) or later. This results in a failure for the configuration import or snapshot rollback.

The workaround is to configure a non-zero IP SLA port value before upgrading the Cisco APIC, and use the snapshot and configuration export that was taken after the IP SLA port change.

5.2(8d) and later

N/A

If you use the REST API to upgrade an app, you must create a new firmware.OSource to be able to download a new app image.

5.2(8d) and later

N/A

In a multipod configuration, before you make any changes to a spine switch, ensure that there is at least one operationally "up" external link that is participating in the multipod topology. Failure to do so could bring down the multipod connectivity. For more information about multipod, see the Cisco Application Centric Infrastructure Fundamentals document and the Cisco APIC Getting Started Guide.

5.2(8d) and later

N/A

With a non-english SCVMM 2012 R2 or SCVMM 2016 setup and where the virtual machine names are specified in non-english characters, if the host is removed and re-added to the host group, the GUID for all the virtual machines under that host changes. Therefore, if a user has created a micro segmentation endpoint group using "VM name" attribute specifying the GUID of respective virtual machine, then that micro segmentation endpoint group will not work if the host (hosting the virtual machines) is removed and re-added to the host group, as the GUID for all the virtual machines would have changed. This does not happen if the virtual name has name specified in all english characters.

5.2(8d) and later

N/A

A query of a configurable policy that does not have a subscription goes to the policy distributor. However, a query of a configurable policy that has a subscription goes to the policy manager. As a result, if the policy propagation from the policy distributor to the policy manager takes a prolonged amount of time, then in such cases the query with the subscription might not return the policy simply because it has not reached policy manager yet.

5.2(8d) and later

N/A

When there are silent hosts across sites, ARP glean messages might not be forwarded to remote sites if a leaf switch without -EX or a later designation in the product ID happens to be in the transit path and the VRF is deployed on that leaf switch, the switch does not forward the ARP glean packet back into the fabric to reach the remote site. This issue is specific to transit leaf switches without -EX or a later designation in the product ID and does not affect leaf switches that have -EX or a later designation in the product ID. This issue breaks the capability of discovering silent hosts.

5.2(8d) and later

N/A

Typically, faults are generally raised based on the presence of the BGP route target profile under the VRF table. However, if a BGP route target profile is configured without actual route targets (that is, the profile has empty policies), a fault will not be raised in this situation.

5.2(8d) and later

N/A

MPLS interface statistics shown in a switch's CLI get cleared after an admin or operational down event.

5.2(8d) and later

N/A

MPLS interface statistics in a switch's CLI are reported every 10 seconds. If, for example, an interface goes down 3 seconds after the collection of the statistics, the CLI reports only 3 seconds of the statistics and clears all of the other statistics.

5.2(8d) and later

Virtualization Compatibility Information

This section lists virtualization compatibility information for the Cisco APIC software.

· For a table that shows the supported virtualization products, see the ACI Virtualization Compatibility Matrix.

· For information about Cisco APIC compatibility with Cisco UCS Director, see the appropriate Cisco UCS Director Compatibility Matrix document.

· If you use Microsoft vSwitch and want to downgrade to Cisco APIC Release 2.3(1) from a later release, you first must delete any microsegment EPGs configured with the Match All filter.

· This release supports the following additional virtualization products:

Product

Supported Release

Information Location

Microsoft Hyper-V

2016 Update Rollup 1, 2, 2.1, and 3

N/A

VMM Integration and VMware Distributed Virtual Switch (DVS)

6.5, 6.7, 7.0, and 8.0

Cisco ACI Virtualization Guide, Release 5.2(x)

Hardware Compatibility Information

This release supports the following Cisco APIC servers:

Product ID

Description

APIC-L2

Cisco APIC with large CPU, hard drive, and memory configurations (more than 1000 edge ports)

APIC-L3

Cisco APIC with large CPU, hard drive, and memory configurations (more than 1200 edge ports)

APIC-M2

Cisco APIC with medium-size CPU, hard drive, and memory configurations (up to 1000 edge ports)

APIC-M3

Cisco APIC with medium-size CPU, hard drive, and memory configurations (up to 1200 edge ports)


The following list includes general hardware compatibility information:

· For the supported hardware, see the Cisco Nexus 9000 ACI-Mode Switches Release Notes, Release 15.2(8).

· Contracts using matchDscp filters are only supported on switches with "EX" on the end of the switch name. For example, N9K-93108TC-EX.

· When the fabric node switch (spine or leaf) is out-of-fabric, the environmental sensor values, such as Current Temperature, Power Draw, and Power Consumption, might be reported as "N/A." A status might be reported as "Normal" even when the Current Temperature is "N/A."

· First generation switches (switches without -EX, -FX, -GX, or a later suffix in the product ID) do not support Contract filters with match type "IPv4" or "IPv6." Only match type "IP" is supported. Because of this, a contract will match both IPv4 and IPv6 traffic when the match type of "IP" is used.

The following table provides compatibility information for specific hardware:

Product ID

Description

Cisco UCS M4-based Cisco APIC

The Cisco UCS M4-based Cisco APIC and previous versions support only the 10G interface. Connecting the Cisco APIC to the Cisco ACI fabric requires a same speed interface on the Cisco ACI leaf switch. You cannot connect the Cisco APIC directly to the Cisco N9332PQ ACI leaf switch, unless you use a 40G to 10G converter (part number CVR-QSFP-SFP10G), in which case the port on the Cisco N9332PQ switch auto-negotiates to 10G without requiring any manual configuration.

Cisco UCS M5-based Cisco APIC

The Cisco UCS M5-based Cisco APIC supports dual speed 10G and 25G interfaces. Connecting the Cisco APIC to the Cisco ACI fabric requires a same speed interface on the Cisco ACI leaf switch. You cannot connect the Cisco APIC directly to the Cisco N9332PQ ACI leaf switch, unless you use a 40G to 10G converter (part number CVR-QSFP-SFP10G), in which case the port on the Cisco N9332PQ switch auto-negotiates to 10G without requiring any manual configuration.

N2348UPQ

To connect the N2348UPQ to Cisco ACI leaf switches, the following options are available:

Directly connect the 40G FEX ports on the N2348UPQ to the 40G switch ports on the Cisco ACI leaf switches

Break out the 40G FEX ports on the N2348UPQ to 4x10G ports and connect to the 10G ports on all other Cisco ACI leaf switches.

Note: A fabric uplink port cannot be used as a FEX fabric port.

N9K-C9348GC-FXP

This switch does not read SPROM information if the PSU is in a shut state. You might see an empty string in the Cisco APIC output.

N9K-C9364C-FX

Ports 49-64 do not support 1G SFPs with QSA.

N9K-C9508-FM-E

The Cisco N9K-C9508-FM-E2 and N9K-C9508-FM-E fabric modules in the mixed mode configuration are not supported on the same spine switch.

N9K-C9508-FM-E2

The Cisco N9K-C9508-FM-E2 and N9K-C9508-FM-E fabric modules in the mixed mode configuration are not supported on the same spine switch.

The locator LED enable/disable feature is supported in the GUI and not supported in the Cisco ACI NX-OS switch CLI.

N9K-C9508-FM-E2

This fabric module must be physically removed before downgrading to releases earlier than Cisco APIC 3.0(1).

N9K-X9736C-FX

The locator LED enable/disable feature is supported in the GUI and not supported in the Cisco ACI NX-OS Switch CLI.

N9K-X9736C-FX

Ports 29 to 36 do not support 1G SFPs with QSA.

Miscellaneous Compatibility Information

This release supports the following products:

Product

Supported Release

Cisco NX-OS

15.2(8)

Cisco UCS Manager

2.2(1c) or later is required for the Cisco UCS Fabric Interconnect and other components, including the BIOS, CIMC, and the adapter.

CIMC HUU ISO

Network Insights Base, Network Insights Advisor, and Network Insights for Resources

For the release information, documentation, and download links, see the Cisco Network Insights for Data Center page.

For the supported releases, see the Cisco Data Center Networking Applications Compatibility Matrix.


· A known issue exists with the Safari browser and unsigned certificates, which applies when connecting to the Cisco APIC GUI. For more information, see the Cisco APIC Getting Started Guide, Release 5.2(x).

· For compatibility with Day-2 Operations apps, see the Cisco Data Center Networking Applications Compatibility Matrix.

· Cisco Nexus Dashboard Insights creates a user in Cisco APIC called cisco_SN_NI. This user is used when Nexus Dashboard Insights needs to make any changes or query any information from the Cisco APIC. In the Cisco APIC, navigate to the Audit Logs tab of the System > History page. The cisco_SN_NI user is displayed in the User column.

Related Content

See the Cisco Application Policy Infrastructure Controller (APIC) page for the documentation.

The documentation includes installation, upgrade, configuration, programming, and troubleshooting guides, technical references, release notes, and knowledge base (KB) articles, as well as other documentation. KB articles provide information about a specific use case or a specific topic.

By using the "Choose a topic" and "Choose a document type" fields of the APIC documentation website, you can narrow down the displayed documentation list to make it easier to find the desired document.

You can watch videos that demonstrate how to perform specific tasks in the Cisco APIC on the Cisco Data Center Networking YouTube channel.

Temporary licenses with an expiry date are available for evaluation and lab use purposes. They are strictly not allowed to be used in production. Use a permanent or subscription license that has been purchased through Cisco for production purposes. For more information, go to Cisco Data Center Networking Software Subscriptions.

The following table provides links to the release notes, verified scalability documentation, and new documentation:

Document

Description

Cisco Nexus 9000 ACI-Mode Switches Release Notes, Release 15.2(8)

The release notes for Cisco NX-OS for Cisco Nexus 9000 Series ACI-Mode Switches.

Verified Scalability Guide for Cisco APIC, Release 5.2(8) and Cisco Nexus 9000 Series ACI-Mode Switches, Release 15.2(8)

This guide contains the maximum verified scalability limits for Cisco Application Centric Infrastructure (ACI) parameters for Cisco APIC and Cisco Nexus 9000 Series ACI-Mode Switches.


Introduction

The Cisco Application Centric Infrastructure (ACI) is an architecture that allows the application to define the networking requirements in a programmatic way. This architecture simplifies, optimizes, and accelerates the entire application deployment lifecycle. Cisco Application Policy Infrastructure Controller (APIC) is the software, or operating system, that acts as the controller.

This document describes the features, issues, and limitations for the Cisco APIC software. For the features, issues, and limitations for the Cisco NX-OS software for the Cisco Nexus 9000 series switches, see the Cisco Nexus 9000 ACI-Mode Switches Release Notes, Release 15.2(8).

For more information about this product, see "Related Content."

Date

Description

August 9, 2023

Release 5.2(8e) became available; there are no changes to this document for this release. See the Cisco Nexus 9000 ACI-Mode Switches Release Notes, Release 15.2(8) for the changes in this release.

June 29, 2023

Release 5.2(8d) became available.

New Software Features

Product Impact

Feature

Description

Base Functionality

Source IP address in the audit log messages

The audit log messages now include the IP address of the source of the configuration change.

Base Functionality

Enhanced timestamp for UDP transport syslog messages

Cisco APIC supports a new enhanced timestamp mode for syslog to include the year in the timestamp for UDP transport.

Interoperability

Support for VMware vSphere 8.0

VMM domains now support VMware vSphere 8.0.

For more information, see the Cisco ACI vCenter Plug-in chapter in the Cisco ACI Virtualization Guide, 5.2(x) and Cisco ACI Virtual Edge Release Notes, Release 3.2(4).

New Hardware Features

For the new hardware features, see the Cisco Nexus 9000 ACI-Mode Switches Release Notes, Release 15.2(8).

Changes in Behavior

· There are no changes in behavior in this release.

Open Issues

Click the bug ID to access the Bug Search tool and see additional information about the bug. The "Exists In" column of the table specifies the 5.2(8) releases in which the bug exists. A bug might also exist in releases other than the 5.2(8) releases.

Bug ID

Description

Exists in

CSCvt99966

A SPAN session with the source type set to "Routed-Outside" goes down. The SPAN configuration is pushed to the anchor or non-anchor nodes, but the interfaces are not pushed due to the following fault: "Failed to configure SPAN with source SpanFL3out due to Source fvIfConn not available".

5.2(8d) and later

CSCvy40511

Traffic from an endpoint under a remote leaf switch to an external node and its attached external networks is dropped. This occurs if the external node is attached to an L3Out with a vPC and there is a redistribution configuration on the L3Out to advertise the reachability of the external nodes as direct-attached hosts.

5.2(8d) and later

CSCvz72941

While performing ID recovery, id-import gets timed out. Due to this, ID recovery fails.

5.2(8d) and later

CSCvz83636

For a health record query using the last page and a time range, the GUI displays some health records with a creation time that are beyond the time range (such as 24h).

5.2(8d) and later

CSCwa90058

When a VRF-level subnet <fvRtSummSubnet> and instP-level subnet <l3extSubnet> with a summary policy is configured for an overlapping subnet, the routes will get summarized by the configuration that was added first. But, the fault on the configuration that was added last will not be shown in the Cisco APIC GUI.

5.2(8d) and later

CSCwa90084

- Traffic disruption across a vPC pair on a given encapsulation.

OR

- EPG flood in encap blackholing on a given encapsulation.

OR

- STP packets received on an encapsulation on a given port are not forwarded on all the leaf switches where the same EPG/same encapsulation is deployed.

5.2(8d) and later

CSCwd26277

When deploying a service graph, the dialog does not list all bridge domains for the provider connector. This issue is observed when you enter or edit the bridge domain name in the consumer connector field. After this, the provider connector will only list the bridge domain that is selected by the consumer connector field.

5.2(8d) and later

CSCwe39988

The Cisco APIC GUI becomes unresponsive when there is large configuration for given tenant and VRF instance.

5.2(8d) and later

CSCwf04318

In a Cisco APIC 5.2 release, a user in a restricted security domain is allowed to create a VRF instance, but that same restricted mode user is not allowed to delete that VRF instance.

5.2(8d) and later

CSCwf54771

User configuration is missing on APICs and switches following an ungraceful reload or power outage.

5.2(8d) and later

Resolved Issues

Bug ID

Description

Fixed in

CSCwc11570

In certain configuration sequences, bridge domain routes (and consequently, host routes) are not advertised out of GOLF and ACI Anywhere L3Outs.

5.2(8d)

CSCwc66053

Preconfiguration validations for L3Outs that occur whenever a new configuration is pushed to the Cisco APIC might not get triggered.

5.2(8d)

CSCwd44827

APIC fails to update standalone Layer3 controller subnet on its fabric. The following fault is raised:

F609478 - [FSM:FAILED]: Update Standalone Controller Subnet(TASK:ifc:policymgr:FabricCtrlrConfigPUpdatePodConnPDef)

5.2(8d)

CSCwd88076

This issue is seen when Syslog is configured with CA signed certificates.The TLS connection was terminated due to certificate validation failure. This is not correct behavior and TLS connection should be established for CA signed certificates too.

5.2(8d)

CSCwd90130

After performing an interface migration from the old selector-based style to the new per-port configuration, an interface with an active override might not work as before the migration.

5.2(8d)

CSCwe09859

The OpFlex agent on a hypervisor randomly disconnects, impacting VM traffic.

5.2(8d)

CSCwe19885

The Nexus Insights application cannot stream the telemetry data to NDI, even though the Cisco ACI site is registered and active.

5.2(8d)

CSCwe25534

When an IPv6 address is added as the BGP peer address, the APIC does not validate the IPv6 address if the address contains any letters.

5.2(8d)

CSCwe64159

Podman is unstable, and gets restarted every 5 seconds, even after running the podman_cleanup.py script. The scheduler services show all the services are running on all the Cisco APICs.

5.2(8d)

CSCwe96230

The Appliance Element (AE) service gets restarted several times by the cgroup process (oom-killer).

5.2(8d)

CSCwf11826

Endpoints are moved from a Tag-based ESG to the base EPG. Tag-to-VM associations are lost and not refreshed for Tags used in ESG selectors.

5.2(8d)

CSCwf19660

Major fault F3083 ("IP detected on multiple MACs") is raised under a uEPG and ESG when an EPG selector is configured in the ESG.

5.2(8d)

CSCwf29094

The error "Error:Server did not return JSON response" occurs in Nexus Cloud Connectivity in any of the Cisco APICs.

5.2(8d)

Known Issues

Click the bug ID to access the Bug Search tool and see additional information about the bug. The "Exists In" column of the table specifies the 5.2(8) releases in which the bug exists. A bug might also exist in releases other than the 5.2(8) releases.

Bug ID

Description

Exists in

CSCuu11416

An endpoint-to-endpoint ACI policy that uses Layer 2 traffic with an IPv6 header does not get counted within or across ESGs/EPGs.

5.2(8d) and later

CSCvj26666

The "show run leaf|spine <nodeId>" command might produce an error for scaled up configurations.

5.2(8d) and later

CSCvj90385

With a uniform distribution of EPs and traffic flows, a fabric module in slot 25 sometimes reports far less than 50% of the traffic compared to the traffic on fabric modules in non-FM25 slots.

5.2(8d) and later

CSCvm71833

Switch upgrades fail with the following error: Version not compatible.

5.2(8d) and later

CSCvq39764

When you click Restart for the Microsoft System Center Virtual Machine Manager (SCVMM) agent on a scaled-out setup, the service may stop. You can restart the agent by clicking Start.

5.2(8d) and later

CSCvq58953

One of the following symptoms occurs:

App installation/enable/disable takes a long time and does not complete.

Nomad leadership is lost. The output of the acidiag scheduler logs members command contains the following error:

Error querying node status: Unexpected response code: 500 (rpc error: No cluster leader)

5.2(8d) and later

CSCvr89603

The CRC and stomped CRC error values do not match when seen from the APIC CLI compared to the APIC GUI. This is expected behavior. The GUI values are from the history data, whereas the CLI values are from the current data.

5.2(8d) and later

CSCvs19322

Upgrading Cisco APIC from a 3.x release to a 4.x or later release causes Smart Licensing to lose its registration. Registering Smart Licensing again will clear the fault.

5.2(8d) and later

CSCvs77929

In the 4.x and later releases, if a firmware policy is created with different name than the maintenance policy, the firmware policy will be deleted and a new firmware policy gets created with the same name, which causes the upgrade process to fail.

5.2(8d) and later

CSCvx75380

svcredirDestmon objects get programmed in all of the leaf switches where the service L3Out is deployed, even though the service node may not be connected to some of the leaf switch.

There is no impact to traffic.

5.2(8d) and later

CSCvx78018

A remote leaf switch has momentary traffic loss for flushed endpoints as the traffic goes through the tglean path and does not directly go through the spine switch proxy path.

5.2(8d) and later

CSCvy07935

xR IP flush for all endpoints under the bridge domain subnets of the EPG being migrated to ESG. This will lead to a temporary traffic loss on remote leaf switch for all EPGs in the bridge domain. Traffic is expected to recover.

5.2(8d) and later

CSCvy10946

With the floating L3Out multipath recursive feature, if a static route with multipath is configured, not all paths are installed at the non-border leaf switch/non-anchor nodes.

5.2(8d) and later

CSCvy34357

Starting with the 5.2(8) release, the following apps built with the following non-compliant Docker versions cannot be installed nor run:

· ConnectivityCompliance 1.2

· SevOneAciMonitor 1.0

5.2(8d) and later

CSCvy45358

The file size mentioned in the status managed object for techsupport "dbgexpTechSupStatus" is wrong if the file size is larger than 4GB.

5.2(8d) and later

CSCvz06118

In the "Visibility and Troubleshooting Wizard," ERSPAN support for IPv6 traffic is not available.

5.2(8d) and later

CSCvz84444

While navigating to the last records in the various History sub tabs, it is possible to not see any results. The first, previous, next, and last buttons will then stop working too.

5.2(8d) and later

CSCvz85579

VMMmgr process experiences a very high load for an extended period of time that impacts other operations that involve it.

The process may consume excessive amount of memory and get aborted. This can be confirmed with the command "dmesg -T | grep oom_reaper" if messages such as the following are reported:

oom_reaper: reaped process 5578 (svc_ifc_vmmmgr.)

5.2(8d) and later

CSCwa78573

When the "BGP" branch is expanded in the Fabric > Inventory > POD 1 > Leaf > Protocols > BGP navigation path, the GUI freezes and you cannot navigate to any other page.

This occurs because the APIC gets large set of data in response, which cannot be handled by the browser for parts of the GUI that do not have the pagination.

5.2(8d) and later

CSCwd45200

Hosting server details for AVE endpoints at the operational tab under the EPG is not updated after VM migration.

5.2(8d) and later

CSCwd51537

After changing a VM's name, the name does not get updated for endpoints in the Operational tab of an EPG.

5.2(8d) and later

N/A

If you are upgrading to Cisco APIC release 4.2(6o), 4.2(7l), 5.2(1g), or later, ensure that any VLAN encapsulation blocks that you are explicitly using for leaf switch front panel VLAN programming are set as "external (on the wire)." If these VLAN encapsulation blocks are instead set to "internal," the upgrade causes the front panel port VLAN to be removed, which can result in a datapath outage.

5.2(8d) and later

N/A

Beginning in Cisco APIC release 4.1(1), the IP SLA monitor policy validates the IP SLA port value. Because of the validation, when TCP is configured as the IP SLA type, Cisco APIC no longer accepts an IP SLA port value of 0, which was allowed in previous releases. An IP SLA monitor policy from a previous release that has an IP SLA port value of 0 becomes invalid if the Cisco APIC is upgraded to release 4.1(1) or later. This results in a failure for the configuration import or snapshot rollback.

The workaround is to configure a non-zero IP SLA port value before upgrading the Cisco APIC, and use the snapshot and configuration export that was taken after the IP SLA port change.

5.2(8d) and later

N/A

If you use the REST API to upgrade an app, you must create a new firmware.OSource to be able to download a new app image.

5.2(8d) and later

N/A

In a multipod configuration, before you make any changes to a spine switch, ensure that there is at least one operationally "up" external link that is participating in the multipod topology. Failure to do so could bring down the multipod connectivity. For more information about multipod, see the Cisco Application Centric Infrastructure Fundamentals document and the Cisco APIC Getting Started Guide.

5.2(8d) and later

N/A

With a non-english SCVMM 2012 R2 or SCVMM 2016 setup and where the virtual machine names are specified in non-english characters, if the host is removed and re-added to the host group, the GUID for all the virtual machines under that host changes. Therefore, if a user has created a micro segmentation endpoint group using "VM name" attribute specifying the GUID of respective virtual machine, then that micro segmentation endpoint group will not work if the host (hosting the virtual machines) is removed and re-added to the host group, as the GUID for all the virtual machines would have changed. This does not happen if the virtual name has name specified in all english characters.

5.2(8d) and later

N/A

A query of a configurable policy that does not have a subscription goes to the policy distributor. However, a query of a configurable policy that has a subscription goes to the policy manager. As a result, if the policy propagation from the policy distributor to the policy manager takes a prolonged amount of time, then in such cases the query with the subscription might not return the policy simply because it has not reached policy manager yet.

5.2(8d) and later

N/A

When there are silent hosts across sites, ARP glean messages might not be forwarded to remote sites if a leaf switch without -EX or a later designation in the product ID happens to be in the transit path and the VRF is deployed on that leaf switch, the switch does not forward the ARP glean packet back into the fabric to reach the remote site. This issue is specific to transit leaf switches without -EX or a later designation in the product ID and does not affect leaf switches that have -EX or a later designation in the product ID. This issue breaks the capability of discovering silent hosts.

5.2(8d) and later

N/A

Typically, faults are generally raised based on the presence of the BGP route target profile under the VRF table. However, if a BGP route target profile is configured without actual route targets (that is, the profile has empty policies), a fault will not be raised in this situation.

5.2(8d) and later

N/A

MPLS interface statistics shown in a switch's CLI get cleared after an admin or operational down event.

5.2(8d) and later

N/A

MPLS interface statistics in a switch's CLI are reported every 10 seconds. If, for example, an interface goes down 3 seconds after the collection of the statistics, the CLI reports only 3 seconds of the statistics and clears all of the other statistics.

5.2(8d) and later

Virtualization Compatibility Information

This section lists virtualization compatibility information for the Cisco APIC software.

· For a table that shows the supported virtualization products, see the ACI Virtualization Compatibility Matrix.

· For information about Cisco APIC compatibility with Cisco UCS Director, see the appropriate Cisco UCS Director Compatibility Matrix document.

· If you use Microsoft vSwitch and want to downgrade to Cisco APIC Release 2.3(1) from a later release, you first must delete any microsegment EPGs configured with the Match All filter.

· This release supports the following additional virtualization products:

Product

Supported Release

Information Location

Microsoft Hyper-V

2016 Update Rollup 1, 2, 2.1, and 3

N/A

VMM Integration and VMware Distributed Virtual Switch (DVS)

6.5, 6.7, 7.0, and 8.0

Cisco ACI Virtualization Guide, Release 5.2(x)

Hardware Compatibility Information

This release supports the following Cisco APIC servers:

Product ID

Description

APIC-L2

Cisco APIC with large CPU, hard drive, and memory configurations (more than 1000 edge ports)

APIC-L3

Cisco APIC with large CPU, hard drive, and memory configurations (more than 1200 edge ports)

APIC-M2

Cisco APIC with medium-size CPU, hard drive, and memory configurations (up to 1000 edge ports)

APIC-M3

Cisco APIC with medium-size CPU, hard drive, and memory configurations (up to 1200 edge ports)


The following list includes general hardware compatibility information:

· For the supported hardware, see the Cisco Nexus 9000 ACI-Mode Switches Release Notes, Release 15.2(8).

· Contracts using matchDscp filters are only supported on switches with "EX" on the end of the switch name. For example, N9K-93108TC-EX.

· When the fabric node switch (spine or leaf) is out-of-fabric, the environmental sensor values, such as Current Temperature, Power Draw, and Power Consumption, might be reported as "N/A." A status might be reported as "Normal" even when the Current Temperature is "N/A."

· First generation switches (switches without -EX, -FX, -GX, or a later suffix in the product ID) do not support Contract filters with match type "IPv4" or "IPv6." Only match type "IP" is supported. Because of this, a contract will match both IPv4 and IPv6 traffic when the match type of "IP" is used.

The following table provides compatibility information for specific hardware:

Product ID

Description

Cisco UCS M4-based Cisco APIC

The Cisco UCS M4-based Cisco APIC and previous versions support only the 10G interface. Connecting the Cisco APIC to the Cisco ACI fabric requires a same speed interface on the Cisco ACI leaf switch. You cannot connect the Cisco APIC directly to the Cisco N9332PQ ACI leaf switch, unless you use a 40G to 10G converter (part number CVR-QSFP-SFP10G), in which case the port on the Cisco N9332PQ switch auto-negotiates to 10G without requiring any manual configuration.

Cisco UCS M5-based Cisco APIC

The Cisco UCS M5-based Cisco APIC supports dual speed 10G and 25G interfaces. Connecting the Cisco APIC to the Cisco ACI fabric requires a same speed interface on the Cisco ACI leaf switch. You cannot connect the Cisco APIC directly to the Cisco N9332PQ ACI leaf switch, unless you use a 40G to 10G converter (part number CVR-QSFP-SFP10G), in which case the port on the Cisco N9332PQ switch auto-negotiates to 10G without requiring any manual configuration.

N2348UPQ

To connect the N2348UPQ to Cisco ACI leaf switches, the following options are available:

Directly connect the 40G FEX ports on the N2348UPQ to the 40G switch ports on the Cisco ACI leaf switches

Break out the 40G FEX ports on the N2348UPQ to 4x10G ports and connect to the 10G ports on all other Cisco ACI leaf switches.

Note: A fabric uplink port cannot be used as a FEX fabric port.

N9K-C9348GC-FXP

This switch does not read SPROM information if the PSU is in a shut state. You might see an empty string in the Cisco APIC output.

N9K-C9364C-FX

Ports 49-64 do not support 1G SFPs with QSA.

N9K-C9508-FM-E

The Cisco N9K-C9508-FM-E2 and N9K-C9508-FM-E fabric modules in the mixed mode configuration are not supported on the same spine switch.

N9K-C9508-FM-E2

The Cisco N9K-C9508-FM-E2 and N9K-C9508-FM-E fabric modules in the mixed mode configuration are not supported on the same spine switch.

The locator LED enable/disable feature is supported in the GUI and not supported in the Cisco ACI NX-OS switch CLI.

N9K-C9508-FM-E2

This fabric module must be physically removed before downgrading to releases earlier than Cisco APIC 3.0(1).

N9K-X9736C-FX

The locator LED enable/disable feature is supported in the GUI and not supported in the Cisco ACI NX-OS Switch CLI.

N9K-X9736C-FX

Ports 29 to 36 do not support 1G SFPs with QSA.

Miscellaneous Compatibility Information

This release supports the following products:

Product

Supported Release

Cisco NX-OS

15.2(8)

Cisco UCS Manager

2.2(1c) or later is required for the Cisco UCS Fabric Interconnect and other components, including the BIOS, CIMC, and the adapter.

CIMC HUU ISO

Network Insights Base, Network Insights Advisor, and Network Insights for Resources

For the release information, documentation, and download links, see the Cisco Network Insights for Data Center page.

For the supported releases, see the Cisco Data Center Networking Applications Compatibility Matrix.


· A known issue exists with the Safari browser and unsigned certificates, which applies when connecting to the Cisco APIC GUI. For more information, see the Cisco APIC Getting Started Guide, Release 5.2(x).

· For compatibility with Day-2 Operations apps, see the Cisco Data Center Networking Applications Compatibility Matrix.

· Cisco Nexus Dashboard Insights creates a user in Cisco APIC called cisco_SN_NI. This user is used when Nexus Dashboard Insights needs to make any changes or query any information from the Cisco APIC. In the Cisco APIC, navigate to the Audit Logs tab of the System > History page. The cisco_SN_NI user is displayed in the User column.

Related Content

See the Cisco Application Policy Infrastructure Controller (APIC) page for the documentation.

The documentation includes installation, upgrade, configuration, programming, and troubleshooting guides, technical references, release notes, and knowledge base (KB) articles, as well as other documentation. KB articles provide information about a specific use case or a specific topic.

By using the "Choose a topic" and "Choose a document type" fields of the APIC documentation website, you can narrow down the displayed documentation list to make it easier to find the desired document.

You can watch videos that demonstrate how to perform specific tasks in the Cisco APIC on the Cisco Data Center Networking YouTube channel.

Temporary licenses with an expiry date are available for evaluation and lab use purposes. They are strictly not allowed to be used in production. Use a permanent or subscription license that has been purchased through Cisco for production purposes. For more information, go to Cisco Data Center Networking Software Subscriptions.

The following table provides links to the release notes, verified scalability documentation, and new documentation:

Document

Description

Cisco Nexus 9000 ACI-Mode Switches Release Notes, Release 15.2(8)

The release notes for Cisco NX-OS for Cisco Nexus 9000 Series ACI-Mode Switches.

Verified Scalability Guide for Cisco APIC, Release 5.2(8) and Cisco Nexus 9000 Series ACI-Mode Switches, Release 15.2(8)

This guide contains the maximum verified scalability limits for Cisco Application Centric Infrastructure (ACI) parameters for Cisco APIC and Cisco Nexus 9000 Series ACI-Mode Switches.


Introduction

The Cisco Application Centric Infrastructure (ACI) is an architecture that allows the application to define the networking requirements in a programmatic way. This architecture simplifies, optimizes, and accelerates the entire application deployment lifecycle. Cisco Application Policy Infrastructure Controller (APIC) is the software, or operating system, that acts as the controller.

This document describes the features, issues, and limitations for the Cisco APIC software. For the features, issues, and limitations for the Cisco NX-OS software for the Cisco Nexus 9000 series switches, see the Cisco Nexus 9000 ACI-Mode Switches Release Notes, Release 15.1(4).

For more information about this product, see "Related Content."

Note: The documentation set for this product strives to use bias-free language. For the purposes of this documentation set, bias-free is defined as language that does not imply discrimination based on age, disability, gender, racial identity, ethnic identity, sexual orientation, socioeconomic status, and intersectionality. Exceptions may be present in the documentation due to language that is hardcoded in the user interfaces of the product software, language used based on RFP documentation, or language that is used by a referenced third-party product.

Date

Description

August 1, 2022

In the Miscellaneous Compatibility Information section, added:

July 1, 2022

In the Open Issues section, added bug CSCwb93239.

June 30, 2022

In the section Miscellaneous Compatibility, added information about Cisco Nexus Dashboard Insights creating the cisco_SN_NI user.

March 21, 2022

In the Miscellaneous Compatibility Information section, added:

February 23, 2022

In the Miscellaneous Compatibility Information section, added:

November 15, 2021

In the Open Issues section, added bug CSCvy17504.

November 2, 2021

In the Miscellaneous Compatibility Information section, added:

July 26, 2021

In the Miscellaneous Compatibility Information section, the CIMC 4.1(3c) release is now recommended for UCS C220/C240 M5 (APIC-L3/M3).

March 18, 2021

Release 5.1(4c) became available.

New Software Features

Feature

Description

Applying a route map to interleak redistribution from direct subnets

You can apply a route map to interleak redistribution from direct subnets (L3Out interfaces). You can also configure the deny action in the route-map for interleak redistribution for static routes and direct subnets.

For more information, see the Cisco APIC Layer 3 Networking Configuration Guide, Release 5.1(x).

Deny action in the route map for interleak redistribution for static routes and direct subnets

You can configure the deny action in the route-map for interleak redistribution for static routes and direct subnets.

For more information, see the Cisco APIC Layer 3 Networking Configuration Guide, Release 5.1(x).

New Hardware Features

For the new hardware features, see the Cisco Nexus 9000 ACI-Mode Switches Release Notes, Release 15.1(4).

Changes in Behavior

There are no changes in behavior in this release.

Open Issues

Click the bug ID to access the Bug Search tool and see additional information about the bug. The "Exists In" column of the table specifies the 5.1(4) releases in which the bug exists. A bug might also exist in releases other than the 5.1(4) releases.

Bug ID

Description

Exists in

CSCvs47602

A bridge domain route is not leaked on the service ToR switch after re-triggering the service graph.

5.1(4c) and later

CSCvs97029

All the external prefixes from VRF-A could be leaked to VRF-C even when an inter-VRF ESG leak route is configured for a specific prefix.

5.1(4c) and later

CSCvt99966

A SPAN session with the source type set to "Routed-Outside" goes down. The SPAN configuration is pushed to the anchor or non-anchor nodes, but the interfaces are not pushed due to the following fault: "Failed to configure SPAN with source SpanFL3out due to Source fvIfConn not available".

5.1(4c) and later

CSCvw12766

In a setup where there is already existing MDP confguration (spine and leaf nodes), after having deleted an MDP spine node, MDP tunnels and traffic might still be directed to that spine node. In the case of a new MDP spine node, the traffic might not get directed to the new spine node.

5.1(4c) and later

CSCvw84947

The BGP loop prevention feature for the inter-VRF shared service case does function as expected upon upgradeing to the 5.1(4) release with existing contracts for a brownfield environment. There is no impact if new contracts are used.

5.1(4c) and later

CSCvx10921

A standby APIC disappears from the GUI after cluster convergence.

5.1(4c) and later

CSCvy17504

When the OpFlexAgent moved from one vPC pair leaf switches to a new vPC pair, it may take up to 20 minutes for the OpFlexAgent detected the movement, and reconnect the OpFlex channel. Ideally, this should be completed within a few seconds.

5.1(4c) and later

CSCwb93239

The GUI displays the following error:

Failed, Local Upload Failure Msg (Request failed with status code 413).

5.1(4c) and later

Resolved Issues

Bug ID

Description

Fixed in

CSCvw62384

After upgrading the switch nodes, the policy manager crashes on all Cisco APICs in a cluster and all replicas are down for the policy manager data management engine.

5.1(4c)

CSCvx12522


When adding or deleting a static route to an L3Out, it may trigger an update to the contracts to which its VRF is related, even if the contract is not modified.

5.1(4c)

CSCvx45585

The urlToken is not returned after logging in to the Cisco APIC.

5.1(4c)

Known Issues

Click the bug ID to access the Bug Search tool and see additional information about the bug. The "Exists In" column of the table specifies the 5.1(4) releases in which the bug exists. A bug might also exist in releases other than the 5.1(4) releases.

Bug ID

Description

Exists in

CSCvj26666

The "show run leaf|spine <nodeId>" command might produce an error for scaled up configurations.

5.1(4c) and later

CSCvj90385

With a uniform distribution of EPs and traffic flows, a fabric module in slot 25 sometimes reports far less than 50% of the traffic compared to the traffic on fabric modules in non-FM25 slots.

5.1(4c) and later

CSCvm71833

Switch upgrades fail with the following error: Version not compatible.

5.1(4c) and later

CSCvq39764

When you click Restart for the Microsoft System Center Virtual Machine Manager (SCVMM) agent on a scaled-out setup, the service may stop. You can restart the agent by clicking Start.

5.1(4c) and later

CSCvq58953

One of the following symptoms occurs:

· App installation/enable/disable takes a long time and does not complete.

· Nomad leadership is lost. The output of the acidiag scheduler logs members command contains the following error:

Error querying node status: Unexpected response code: 500 (rpc error: No cluster leader)

5.1(4c) and later

CSCvr89603

The CRC and stomped CRC error values do not match when seen from the APIC CLI compared to the APIC GUI. This is expected behavior. The GUI values are from the history data, whereas the CLI values are from the current data.

5.1(4c) and later

CSCvs19322

Upgrading Cisco APIC from a 3.x release to a 4.x release causes Smart Licensing to lose its registration. Registering Smart Licensing again will clear the fault.

5.1(4c) and later

CSCvs77929

In the 4.x and later releases, if a firmware policy is created with different name than the maintenance policy, the firmware policy will be deleted and a new firmware policy gets created with the same name, which causes the upgrade process to fail.

5.1(4c) and later

N/A

Beginning in Cisco APIC release 4.1(1), the IP SLA monitor policy validates the IP SLA port value. Because of the validation, when TCP is configured as the IP SLA type, Cisco APIC no longer accepts an IP SLA port value of 0, which was allowed in previous releases. An IP SLA monitor policy from a previous release that has an IP SLA port value of 0 becomes invalid if the Cisco APIC is upgraded to release 4.1(1) or later. This results in a failure for the configuration import or snapshot rollback.

The workaround is to configure a non-zero IP SLA port value before upgrading the Cisco APIC, and use the snapshot and configuration export that was taken after the IP SLA port change.

5.1(4c) and later

N/A

If you use the REST API to upgrade an app, you must create a new firmware.OSource to be able to download a new app image.

5.1(4c) and later

N/A

In a multipod configuration, before you make any changes to a spine switch, ensure that there is at least one operationally "up" external link that is participating in the multipod topology. Failure to do so could bring down the multipod connectivity. For more information about multipod, see the Cisco Application Centric Infrastructure Fundamentals document and the Cisco APIC Getting Started Guide.

5.1(4c) and later

N/A

With a non-english SCVMM 2012 R2 or SCVMM 2016 setup and where the virtual machine names are specified in non-english characters, if the host is removed and re-added to the host group, the GUID for all the virtual machines under that host changes. Therefore, if a user has created a micro segmentation endpoint group using "VM name" attribute specifying the GUID of respective virtual machine, then that micro segmentation endpoint group will not work if the host (hosting the virtual machines) is removed and re-added to the host group, as the GUID for all the virtual machines would have changed. This does not happen if the virtual name has name specified in all english characters.

5.1(4c) and later

N/A

A query of a configurable policy that does not have a subscription goes to the policy distributor. However, a query of a configurable policy that has a subscription goes to the policy manager. As a result, if the policy propagation from the policy distributor to the policy manager takes a prolonged amount of time, then in such cases the query with the subscription might not return the policy simply because it has not reached policy manager yet.

5.1(4c) and later

N/A

When there are silent hosts across sites, ARP glean messages might not be forwarded to remote sites if a leaf switch without -EX or a later designation in the product ID happens to be in the transit path and the VRF is deployed on that leaf switch, the switch does not forward the ARP glean packet back into the fabric to reach the remote site. This issue is specific to transit leaf switches without -EX or a later designation in the product ID and does not affect leaf switches that have -EX or a later designation in the product ID. This issue breaks the capability of discovering silent hosts.

5.1(4c) and later

N/A

Typically, faults are generally raised based on the presence of the BGP route target profile under the VRF table. However, if a BGP route target profile is configured without actual route targets (that is, the profile has empty policies), a fault will not be raised in this situation.

5.1(4c) and later

N/A

MPLS interface statistics shown in a switch's CLI get cleared after an admin or operational down event.

5.1(4c) and later

N/A

MPLS interface statistics in a switch's CLI are reported every 10 seconds. If, for example, an interface goes down 3 seconds after the collection of the statistics, the CLI reports only 3 seconds of the statistics and clears all of the other statistics.

5.1(4c) and later

Virtualization Compatibility Information

This section lists virtualization compatibility information for the Cisco APIC software.

· For a table that shows the supported virtualization products, see the ACI Virtualization Compatibility Matrix.

· For information about Cisco APIC compatibility with Cisco UCS Director, see the appropriate Cisco UCS Director Compatibility Matrix document.

· If you use Microsoft vSwitch and want to downgrade to Cisco APIC Release 2.3(1) from a later release, you first must delete any microsegment EPGs configured with the Match All filter.

· This release supports the following additional virtualization products:

Product

Supported Release

Information Location

Microsoft Hyper-V

2016 Update Rollup 1, 2, 2.1, and 3

N/A

VMM Integration and VMware Distributed Virtual Switch (DVS)

6.5, 6.7, and 7.0

Cisco ACI Virtualization Guide, Release 5.1(x)

Hardware Compatibility Information

This release supports the following Cisco APIC servers:

Product ID

Description

APIC-L1

Cisco APIC with large CPU, hard drive, and memory configurations (more than 1000 edge ports)

APIC-L2

Cisco APIC with large CPU, hard drive, and memory configurations (more than 1000 edge ports)

APIC-L3

Cisco APIC with large CPU, hard drive, and memory configurations (more than 1200 edge ports)

APIC-M1

Cisco APIC with medium-size CPU, hard drive, and memory configurations (up to 1000 edge ports)

APIC-M2

Cisco APIC with medium-size CPU, hard drive, and memory configurations (up to 1000 edge ports)

APIC-M3

Cisco APIC with medium-size CPU, hard drive, and memory configurations (up to 1200 edge ports)


The following list includes general hardware compatibility information:

· For the supported hardware, see the Cisco Nexus 9000 ACI-Mode Switches Release Notes, Release 15.1(4).

· Contracts using matchDscp filters are only supported on switches with "EX" on the end of the switch name. For example, N9K-93108TC-EX.

· When the fabric node switch (spine or leaf) is out-of-fabric, the environmental sensor values, such as Current Temperature, Power Draw, and Power Consumption, might be reported as "N/A." A status might be reported as "Normal" even when the Current Temperature is "N/A."

· First generation switches (switches without -EX, -FX, -GX, or a later suffix in the product ID) do not support Contract filters with match type "IPv4" or "IPv6." Only match type "IP" is supported. Because of this, a contract will match both IPv4 and IPv6 traffic when the match type of "IP" is used.

The following table provides compatibility information for specific hardware:

Product ID

Description

Cisco UCS M4-based Cisco APIC

The Cisco UCS M4-based Cisco APIC and previous versions support only the 10G interface. Connecting the Cisco APIC to the Cisco ACI fabric requires a same speed interface on the Cisco ACI leaf switch. You cannot connect the Cisco APIC directly to the Cisco N9332PQ ACI leaf switch, unless you use a 40G to 10G converter (part number CVR-QSFP-SFP10G), in which case the port on the Cisco N9332PQ switch auto-negotiates to 10G without requiring any manual configuration.

Cisco UCS M5-based Cisco APIC

The Cisco UCS M5-based Cisco APIC supports dual speed 10G and 25G interfaces. Connecting the Cisco APIC to the Cisco ACI fabric requires a same speed interface on the Cisco ACI leaf switch. You cannot connect the Cisco APIC directly to the Cisco N9332PQ ACI leaf switch, unless you use a 40G to 10G converter (part number CVR-QSFP-SFP10G), in which case the port on the Cisco N9332PQ switch auto-negotiates to 10G without requiring any manual configuration.

N2348UPQ

To connect the N2348UPQ to Cisco ACI leaf switches, the following options are available:

· Directly connect the 40G FEX ports on the N2348UPQ to the 40G switch ports on the Cisco ACI leaf switches

· Break out the 40G FEX ports on the N2348UPQ to 4x10G ports and connect to the 10G ports on all other Cisco ACI leaf switches.

Note: A fabric uplink port cannot be used as a FEX fabric port.

N9K-C9348GC-FXP

This switch does not read SPROM information if the PSU is in a shut state. You might see an empty string in the Cisco APIC output.

N9K-C9364C-FX

Ports 49-64 do not supporFut 1G SFPs with QSA.

N9K-C9508-FM-E

The Cisco N9K-C9508-FM-E2 and N9K-C9508-FM-E fabric modules in the mixed mode configuration are not supported on the same spine switch.

N9K-C9508-FM-E2

The Cisco N9K-C9508-FM-E2 and N9K-C9508-FM-E fabric modules in the mixed mode configuration are not supported on the same spine switch.

The locator LED enable/disable feature is supported in the GUI and not supported in the Cisco ACI NX-OS switch CLI.

N9K-C9508-FM-E2

This fabric module must be physically removed before downgrading to releases earlier than Cisco APIC 3.0(1).

N9K-X9736C-FX

The locator LED enable/disable feature is supported in the GUI and not supported in the Cisco ACI NX-OS Switch CLI.

N9K-X9736C-FX

Ports 29 to 36 do not support 1G SFPs with QSA.

Adaptive Security Appliance (ASA) Compatibility Information

This section lists ASA compatibility information for the Cisco APIC software.

· This release supports Adaptive Security Appliance (ASA) device package version 1.2.5.5 or later.

· If you are running a Cisco Adaptive Security Virtual Appliance (ASA) version that is prior to version 9.3(2), you must configure SSL encryption as follows:

(config)# ssl encryption aes128-sha1

Miscellaneous Compatibility Information

This release supports the following products:

Product

Supported Release

Cisco NX-OS

15.1(4)

Cisco AVS

5.2(1)SV3(4.10)

For more information about the supported AVS releases, see the AVS software compatibility information in the Cisco Application Virtual Switch Release Notes, Release 5.2(1)SV3(4.11).

Cisco UCS Manager

2.2(1c) or later is required for the Cisco UCS Fabric Interconnect and other components, including the BIOS, CIMC, and the adapter.

CIMC HUU ISO

Network Insights Base, Network Insights Advisor, and Network Insights for Resources

For the release information, documentation, and download links, see the Cisco Network Insights for Data Center page.

For the supported releases, see the Cisco Data Center Networking Applications Compatibility Matrix.


· This release supports the partner packages specified in the L4-L7 Compatibility List Solution Overview document.

· A known issue exists with the Safari browser and unsigned certificates, which applies when connecting to the Cisco APIC GUI. For more information, see the Cisco APIC Getting Started Guide, Release 5.1(x).

· For compatibility with Day-2 Operations apps, see the Cisco Data Center Networking Applications Compatibility Matrix.

· Cisco Nexus Dashboard Insights creates a user in Cisco APIC called cisco_SN_NI. This user is used when Nexus Dashboard Insights needs to make any changes or query any information from the Cisco APIC. In the Cisco APIC, navigate to the Audit Logs tab of the System > History page. The cisco_SN_NI user is displayed in the User column.

Related Content

See the Cisco Application Policy Infrastructure Controller (APIC) page for the documentation.

The documentation includes installation, upgrade, configuration, programming, and troubleshooting guides, technical references, release notes, and knowledge base (KB) articles, as well as other documentation. KB articles provide information about a specific use case or a specific topic.

By using the "Choose a topic" and "Choose a document type" fields of the APIC documentation website, you can narrow down the displayed documentation list to make it easier to find the desired document.

You can watch videos that demonstrate how to perform specific tasks in the Cisco APIC on the Cisco Data Center Networking YouTube channel.

Temporary licenses with an expiry date are available for evaluation and lab use purposes. They are strictly not allowed to be used in production. Use a permanent or subscription license that has been purchased through Cisco for production purposes. For more information, go to Cisco Data Center Networking Software Subscriptions.

The following table provides links to the release notes, verified scalability documentation, and new documentation:

Document

Description

Cisco Nexus 9000 ACI-Mode Switches Release Notes, Release 15.1(4)

The release notes for Cisco NX-OS for Cisco Nexus 9000 Series ACI-Mode Switches.

Verified Scalability Guide for Cisco APIC, Release 5.1(3) and Cisco Nexus 9000 Series ACI-Mode Switches, Release 15.1(3)

This guide contains the maximum verified scalability limits for Cisco Application Centric Infrastructure (ACI) parameters for Cisco APIC and Cisco Nexus 9000 Series ACI-Mode Switches.

Note: The 5.1(3) release document applies to the 5.1(4) release.


Introduction

The Cisco Application Centric Infrastructure (ACI) is an architecture that allows the application to define the networking requirements in a programmatic way. This architecture simplifies, optimizes, and accelerates the entire application deployment lifecycle. Cisco Application Policy Infrastructure Controller (APIC) is the software, or operating system, that acts as the controller.

This document describes the features, issues, and limitations for the Cisco APIC software. For the features, issues, and limitations for the Cisco NX-OS software for the Cisco Nexus 9000 series switches, see the Cisco Nexus 9000 ACI-Mode Switches Release Notes, Release 15.1(3).

For more information about this product, see "Related Content."

Note: The documentation set for this product strives to use bias-free language. For the purposes of this documentation set, bias-free is defined as language that does not imply discrimination based on age, disability, gender, racial identity, ethnic identity, sexual orientation, socioeconomic status, and intersectionality. Exceptions may be present in the documentation due to language that is hardcoded in the user interfaces of the product software, language used based on RFP documentation, or language that is used by a referenced third-party product.

Date

Description

August 1, 2022

In the Miscellaneous Compatibility Information section, added:

June 30, 2022

In the section Miscellaneous Compatibility, added information about Cisco Nexus Dashboard Insights creating the cisco_SN_NI user.

March 21, 2022

In the Miscellaneous Compatibility Information section, added:

February 23, 2022

In the Miscellaneous Compatibility Information section, added:

November 15, 2021

In the Open Issues section, added bug CSCvy17504.

November 2, 2021

In the Miscellaneous Compatibility Information section, added:

July 26, 2021

In the Miscellaneous Compatibility Information section, the CIMC 4.1(3c) release is now recommended for UCS C220/C240 M5 (APIC-L3/M3).

March 11, 2021

In the Miscellaneous Compatibility Information section, for CIMC HUU ISO, added:

Changed:

To:

February 3, 2021

In the Miscellaneous Compatibility Information section, for CIMC HUU ISO, added:

February 1, 2021

Release 5.1(3e) became available.

New Software Features

Feature

Description

Support for the BGP Domain-Path feature for loop prevention

Support is now available for the BGP Domain-Path feature for loop prevention.

For more information, see the Cisco APIC Layer 3 Networking Configuration Guide, Release 5.1(x).

Support for Layer 3 multicast routing on remote leaf switches

Beginning with release 5.1(3), support is available for Layer 3 multicast routing on remote leaf switches. Prior to this release, support was available for Layer 3 multicast routing in only single-pod, multi-pod, and multi-site topologies on local leaf switches.

For more information, see the Cisco APIC Layer 3 Networking Configuration Guide, Release 5.1(x).

New IP SLA monitoring policy parameters

The IP SLA monitoring policy now has the following parameters:

· Request Data Size (bytes)

· Type of Service

· Operation Timeout (milliseconds)

· Threshold (milliseconds)

· Traffic Class Value

For more information, see the online help pages for the IP SLA monitoring policy GUI screens.

Additional auto-negotiation support

The auto-negotiation "On-Enforce" mode is now supported on the Cisco Nexus N9K-C93180YC-EX and N9K-C9364C-GX switches. All switches that support auto-negotiation now support additional speeds in the "On-Enforce" mode.

For more information, see the Cisco ACI Auto-Negotiation and Forward Error Correction document.

New Hardware Features

For the new hardware features, see the Cisco Nexus 9000 ACI-Mode Switches Release Notes, Release 15.1(3).

Changes in Behavior

· ICMP now replies with the same Class of Service (CoS) value that was sent in the request.

Open Issues

Click the bug ID to access the Bug Search tool and see additional information about the bug. The "Exists In" column of the table specifies the 5.1(3) releases in which the bug exists. A bug might also exist in releases other than the 5.1(3) releases.

Bug ID

Description

Exists in

CSCvs47602

A bridge domain route is not leaked on the service ToR switch after re-triggering the service graph.

5.1(3e) and later

CSCvs97029

All the external prefixes from VRF-A could be leaked to VRF-C even when an inter-VRF ESG leak route is configured for a specific prefix.

5.1(3e) and later

CSCvt99966

A SPAN session with the source type set to "Routed-Outside" goes down. The SPAN configuration is pushed to the anchor or non-anchor nodes, but the interfaces are not pushed due to the following fault: "Failed to configure SPAN with source SpanFL3out due to Source fvIfConn not available".

5.1(3e) and later

CSCvw12766

In a setup where there is already existing MDP confguration (spine and leaf nodes), after having deleted an MDP spine node, MDP tunnels and traffic might still be directed to that spine node. In the case of a new MDP spine node, the traffic might not get directed to the new spine node.

5.1(3e) and later

CSCvw84947

The BGP loop prevention feature for the inter-VRF shared service case does function as expected upon upgradeing to the 5.1(3) release with existing contracts for a brownfield environment. There is no impact if new contracts are used.

5.1(3e) and later

CSCvx10921

A standby APIC disappears from the GUI after cluster convergence.

5.1(3e) and later

CSCvy17504

When the OpFlexAgent moved from one vPC pair leaf switches to a new vPC pair, it may take up to 20 minutes for the OpFlexAgent detected the movement, and reconnect the OpFlex channel. Ideally, this should be completed within a few seconds.

5.1(3e) and later

Resolved Issues

Bug ID

Description

Fixed in

CSCvq54761

The application EPG or the corresponding bridge domain's public subnet may be advertised out of an L3Out in another VRF instance without a contract with the L3Out under certain conditions.

5.1(3e)

CSCvv69041

There is traffic loss on SDA after deleting an overlapping subnet.

5.1(3e)

CSCvw87460

The leaf switch is listed as unsupported and the model name is truncated.

5.1(3e)

Known Issues

Click the bug ID to access the Bug Search tool and see additional information about the bug. The "Exists In" column of the table specifies the 5.1(3) releases in which the bug exists. A bug might also exist in releases other than the 5.1(3) releases.

Bug ID

Description

Exists in

CSCvj26666

The "show run leaf|spine <nodeId>" command might produce an error for scaled up configurations.

5.1(3e) and later

CSCvj90385

With a uniform distribution of EPs and traffic flows, a fabric module in slot 25 sometimes reports far less than 50% of the traffic compared to the traffic on fabric modules in non-FM25 slots.

5.1(3e) and later

CSCvm71833

Switch upgrades fail with the following error: Version not compatible.

5.1(3e) and later

CSCvq39764

When you click Restart for the Microsoft System Center Virtual Machine Manager (SCVMM) agent on a scaled-out setup, the service may stop. You can restart the agent by clicking Start.

5.1(3e) and later

CSCvq58953

One of the following symptoms occurs:

· App installation/enable/disable takes a long time and does not complete.

· Nomad leadership is lost. The output of the acidiag scheduler logs members command contains the following error:

Error querying node status: Unexpected response code: 500 (rpc error: No cluster leader)

5.1(3e) and later

CSCvr89603

The CRC and stomped CRC error values do not match when seen from the APIC CLI compared to the APIC GUI. This is expected behavior. The GUI values are from the history data, whereas the CLI values are from the current data.

5.1(3e) and later

CSCvs19322

Upgrading Cisco APIC from a 3.x release to a 4.x release causes Smart Licensing to lose its registration. Registering Smart Licensing again will clear the fault.

5.1(3e) and later

CSCvs77929

In the 4.x and later releases, if a firmware policy is created with different name than the maintenance policy, the firmware policy will be deleted and a new firmware policy gets created with the same name, which causes the upgrade process to fail.

5.1(3e) and later

N/A

Beginning in Cisco APIC release 4.1(1), the IP SLA monitor policy validates the IP SLA port value. Because of the validation, when TCP is configured as the IP SLA type, Cisco APIC no longer accepts an IP SLA port value of 0, which was allowed in previous releases. An IP SLA monitor policy from a previous release that has an IP SLA port value of 0 becomes invalid if the Cisco APIC is upgraded to release 4.1(1) or later. This results in a failure for the configuration import or snapshot rollback.

The workaround is to configure a non-zero IP SLA port value before upgrading the Cisco APIC, and use the snapshot and configuration export that was taken after the IP SLA port change.

5.1(3e) and later

N/A

If you use the REST API to upgrade an app, you must create a new firmware.OSource to be able to download a new app image.

5.1(3e) and later

N/A

In a multipod configuration, before you make any changes to a spine switch, ensure that there is at least one operationally "up" external link that is participating in the multipod topology. Failure to do so could bring down the multipod connectivity. For more information about multipod, see the Cisco Application Centric Infrastructure Fundamentals document and the Cisco APIC Getting Started Guide.

5.1(3e) and later

N/A

With a non-english SCVMM 2012 R2 or SCVMM 2016 setup and where the virtual machine names are specified in non-english characters, if the host is removed and re-added to the host group, the GUID for all the virtual machines under that host changes. Therefore, if a user has created a micro segmentation endpoint group using "VM name" attribute specifying the GUID of respective virtual machine, then that micro segmentation endpoint group will not work if the host (hosting the virtual machines) is removed and re-added to the host group, as the GUID for all the virtual machines would have changed. This does not happen if the virtual name has name specified in all english characters.

5.1(3e) and later

N/A

A query of a configurable policy that does not have a subscription goes to the policy distributor. However, a query of a configurable policy that has a subscription goes to the policy manager. As a result, if the policy propagation from the policy distributor to the policy manager takes a prolonged amount of time, then in such cases the query with the subscription might not return the policy simply because it has not reached policy manager yet.

5.1(3e) and later

N/A

When there are silent hosts across sites, ARP glean messages might not be forwarded to remote sites if a leaf switch without -EX or a later designation in the product ID happens to be in the transit path and the VRF is deployed on that leaf switch, the switch does not forward the ARP glean packet back into the fabric to reach the remote site. This issue is specific to transit leaf switches without -EX or a later designation in the product ID and does not affect leaf switches that have -EX or a later designation in the product ID. This issue breaks the capability of discovering silent hosts.

5.1(3e) and later

N/A

Typically, faults are generally raised based on the presence of the BGP route target profile under the VRF table. However, if a BGP route target profile is configured without actual route targets (that is, the profile has empty policies), a fault will not be raised in this situation.

5.1(3e) and later

N/A

MPLS interface statistics shown in a switch's CLI get cleared after an admin or operational down event.

5.1(3e) and later

N/A

MPLS interface statistics in a switch's CLI are reported every 10 seconds. If, for example, an interface goes down 3 seconds after the collection of the statistics, the CLI reports only 3 seconds of the statistics and clears all of the other statistics.

5.1(3e) and later

Virtualization Compatibility Information

This section lists virtualization compatibility information for the Cisco APIC software.

· For a table that shows the supported virtualization products, see the ACI Virtualization Compatibility Matrix.

· For information about Cisco APIC compatibility with Cisco UCS Director, see the appropriate Cisco UCS Director Compatibility Matrix document.

· If you use Microsoft vSwitch and want to downgrade to Cisco APIC Release 2.3(1) from a later release, you first must delete any microsegment EPGs configured with the Match All filter.

· This release supports the following additional virtualization products:

Product

Supported Release

Information Location

Microsoft Hyper-V

2016 Update Rollup 1, 2, 2.1, and 3

N/A

VMM Integration and VMware Distributed Virtual Switch (DVS)

6.5, 6.7, and 7.0

Cisco ACI Virtualization Guide, Release 5.1(x)

Hardware Compatibility Information

This release supports the following Cisco APIC servers:

Product ID

Description

APIC-L1

Cisco APIC with large CPU, hard drive, and memory configurations (more than 1000 edge ports)

APIC-L2

Cisco APIC with large CPU, hard drive, and memory configurations (more than 1000 edge ports)

APIC-L3

Cisco APIC with large CPU, hard drive, and memory configurations (more than 1200 edge ports)

APIC-M1

Cisco APIC with medium-size CPU, hard drive, and memory configurations (up to 1000 edge ports)

APIC-M2

Cisco APIC with medium-size CPU, hard drive, and memory configurations (up to 1000 edge ports)

APIC-M3

Cisco APIC with medium-size CPU, hard drive, and memory configurations (up to 1200 edge ports)


The following list includes general hardware compatibility information:

· For the supported hardware, see the Cisco Nexus 9000 ACI-Mode Switches Release Notes, Release 15.1(3).

· Contracts using matchDscp filters are only supported on switches with "EX" on the end of the switch name. For example, N9K-93108TC-EX.

· When the fabric node switch (spine or leaf) is out-of-fabric, the environmental sensor values, such as Current Temperature, Power Draw, and Power Consumption, might be reported as "N/A." A status might be reported as "Normal" even when the Current Temperature is "N/A."

· First generation switches (switches without -EX, -FX, -GX, or a later suffix in the product ID) do not support Contract filters with match type "IPv4" or "IPv6." Only match type "IP" is supported. Because of this, a contract will match both IPv4 and IPv6 traffic when the match type of "IP" is used.

The following table provides compatibility information for specific hardware:

Product ID

Description

Cisco UCS M4-based Cisco APIC

The Cisco UCS M4-based Cisco APIC and previous versions support only the 10G interface. Connecting the Cisco APIC to the Cisco ACI fabric requires a same speed interface on the Cisco ACI leaf switch. You cannot connect the Cisco APIC directly to the Cisco N9332PQ ACI leaf switch, unless you use a 40G to 10G converter (part number CVR-QSFP-SFP10G), in which case the port on the Cisco N9332PQ switch auto-negotiates to 10G without requiring any manual configuration.

Cisco UCS M5-based Cisco APIC

The Cisco UCS M5-based Cisco APIC supports dual speed 10G and 25G interfaces. Connecting the Cisco APIC to the Cisco ACI fabric requires a same speed interface on the Cisco ACI leaf switch. You cannot connect the Cisco APIC directly to the Cisco N9332PQ ACI leaf switch, unless you use a 40G to 10G converter (part number CVR-QSFP-SFP10G), in which case the port on the Cisco N9332PQ switch auto-negotiates to 10G without requiring any manual configuration.

N2348UPQ

To connect the N2348UPQ to Cisco ACI leaf switches, the following options are available:

· Directly connect the 40G FEX ports on the N2348UPQ to the 40G switch ports on the Cisco ACI leaf switches

· Break out the 40G FEX ports on the N2348UPQ to 4x10G ports and connect to the 10G ports on all other Cisco ACI leaf switches.

Note: A fabric uplink port cannot be used as a FEX fabric port.

N9K-C9348GC-FXP

This switch does not read SPROM information if the PSU is in a shut state. You might see an empty string in the Cisco APIC output.

N9K-C9364C-FX

Ports 49-64 do not supporFut 1G SFPs with QSA.

N9K-C9508-FM-E

The Cisco N9K-C9508-FM-E2 and N9K-C9508-FM-E fabric modules in the mixed mode configuration are not supported on the same spine switch.

N9K-C9508-FM-E2

The Cisco N9K-C9508-FM-E2 and N9K-C9508-FM-E fabric modules in the mixed mode configuration are not supported on the same spine switch.

The locator LED enable/disable feature is supported in the GUI and not supported in the Cisco ACI NX-OS switch CLI.

N9K-C9508-FM-E2

This fabric module must be physically removed before downgrading to releases earlier than Cisco APIC 3.0(1).

N9K-X9736C-FX

The locator LED enable/disable feature is supported in the GUI and not supported in the Cisco ACI NX-OS Switch CLI.

N9K-X9736C-FX

Ports 29 to 36 do not support 1G SFPs with QSA.

Adaptive Security Appliance (ASA) Compatibility Information

This section lists ASA compatibility information for the Cisco APIC software.

· This release supports Adaptive Security Appliance (ASA) device package version 1.2.5.5 or later.

· If you are running a Cisco Adaptive Security Virtual Appliance (ASA) version that is prior to version 9.3(2), you must configure SSL encryption as follows:

(config)# ssl encryption aes128-sha1

Miscellaneous Compatibility Information

This release supports the following products:

Product

Supported Release

Cisco NX-OS

15.1(3)

Cisco AVS

5.2(1)SV3(4.10)

For more information about the supported AVS releases, see the AVS software compatibility information in the Cisco Application Virtual Switch Release Notes, Release 5.2(1)SV3(4.11).

Cisco UCS Manager

2.2(1c) or later is required for the Cisco UCS Fabric Interconnect and other components, including the BIOS, CIMC, and the adapter.

CIMC HUU ISO

Network Insights Base, Network Insights Advisor, and Network Insights for Resources

For the release information, documentation, and download links, see the Cisco Network Insights for Data Center page.

For the supported releases, see the Cisco Data Center Networking Applications Compatibility Matrix.


· This release supports the partner packages specified in the L4-L7 Compatibility List Solution Overview document.

· A known issue exists with the Safari browser and unsigned certificates, which applies when connecting to the Cisco APIC GUI. For more information, see the Cisco APIC Getting Started Guide, Release 5.1(x).

· For compatibility with Day-2 Operations apps, see the Cisco Data Center Networking Applications Compatibility Matrix.

· Cisco Nexus Dashboard Insights creates a user in Cisco APIC called cisco_SN_NI. This user is used when Nexus Dashboard Insights needs to make any changes or query any information from the Cisco APIC. In the Cisco APIC, navigate to the Audit Logs tab of the System > History page. The cisco_SN_NI user is displayed in the User column.

Related Content

See the Cisco Application Policy Infrastructure Controller (APIC) page for the documentation.

The documentation includes installation, upgrade, configuration, programming, and troubleshooting guides, technical references, release notes, and knowledge base (KB) articles, as well as other documentation. KB articles provide information about a specific use case or a specific topic.

By using the "Choose a topic" and "Choose a document type" fields of the APIC documentation website, you can narrow down the displayed documentation list to make it easier to find the desired document.

You can watch videos that demonstrate how to perform specific tasks in the Cisco APIC on the Cisco Data Center Networking YouTube channel.

Temporary licenses with an expiry date are available for evaluation and lab use purposes. They are strictly not allowed to be used in production. Use a permanent or subscription license that has been purchased through Cisco for production purposes. For more information, go to Cisco Data Center Networking Software Subscriptions.

The following table provides links to the release notes, verified scalability documentation, and new documentation:

Document

Description

Cisco Nexus 9000 ACI-Mode Switches Release Notes, Release 15.1(3)

The release notes for Cisco NX-OS for Cisco Nexus 9000 Series ACI-Mode Switches.

Verified Scalability Guide for Cisco APIC, Release 5.1(3) and Cisco Nexus 9000 Series ACI-Mode Switches, Release 15.1(3)

This guide contains the maximum verified scalability limits for Cisco Application Centric Infrastructure (ACI) parameters for Cisco APIC and Cisco Nexus 9000 Series ACI-Mode Switches.


Introduction

The Cisco Application Centric Infrastructure (ACI) is an architecture that allows the application to define the networking requirements in a programmatic way. This architecture simplifies, optimizes, and accelerates the entire application deployment lifecycle. Cisco Application Policy Infrastructure Controller (APIC) is the software, or operating system, that acts as the controller.

This document describes the features, issues, and limitations for the Cisco APIC software. For the features, issues, and limitations for the Cisco NX-OS software for the Cisco Nexus 9000 series switches, see the Cisco Nexus 9000 ACI-Mode Switches Release Notes, Release 15.1(2).

For more information about this product, see "Related Content."

Note: The documentation set for this product strives to use bias-free language. For the purposes of this documentation set, bias-free is defined as language that does not imply discrimination based on age, disability, gender, racial identity, ethnic identity, sexual orientation, socioeconomic status, and intersectionality. Exceptions may be present in the documentation due to language that is hardcoded in the user interfaces of the product software, language used based on RFP documentation, or language that is used by a referenced third-party product.

Date

Description

August 1, 2022

In the Miscellaneous Compatibility Information section, added:

June 30, 2022

In the section Miscellaneous Compatibility, added information about Cisco Nexus Dashboard Insights creating the cisco_SN_NI user.

March 21, 2022

In the Miscellaneous Compatibility Information section, added:

February 23, 2022

In the Miscellaneous Compatibility Information section, added:

November 15, 2021

In the Open Issues section, added bug CSCvy17504.

November 2, 2021

In the Miscellaneous Compatibility Information section, added:

July 26, 2021

In the Miscellaneous Compatibility Information section, the CIMC 4.1(3c) release is now recommended for UCS C220/C240 M5 (APIC-L3/M3).

March 11, 2021

In the Miscellaneous Compatibility Information section, for CIMC HUU ISO, added:

Changed:

To:

February 3, 2021

In the Miscellaneous Compatibility Information section, for CIMC HUU ISO, added:

4.1(2b) CIMC HUU ISO (recommended) for UCS C220/C240 M4 (APIC-L2/M2) and M5 (APIC-L3/M3)

November 27, 2020

Release 5.1(2e) became available.

New Software Features

There are no new software features in this release.

New Hardware Features

For the new hardware features, see the Cisco Nexus 9000 ACI-Mode Switches Release Notes, Release 15.1(2).

Changes in Behavior

There are no changes in behavior in this release.

Open Issues

Click the bug ID to access the Bug Search tool and see additional information about the bug. The "Exists In" column of the table specifies the 5.1(2) releases in which the bug exists. A bug might also exist in releases other than the 5.1(2) releases.

Bug ID

Description

Exists in

CSCvq54761

The application EPG or the corresponding bridge domain's public subnet may be advertised out of an L3Out in another VRF instance without a contract with the L3Out under certain conditions.

5.1(2e) and later

CSCvs47602

A bridge domain route is not leaked on the service ToR switch after re-triggering the service graph.

5.1(2e) and later

CSCvs97029

All the external prefixes from VRF-A could be leaked to VRF-C even when an inter-VRF ESG leak route is configured for a specific prefix.

5.1(2e) and later

CSCvt99966

A SPAN session with the source type set to "Routed-Outside" goes down. The SPAN configuration is pushed to the anchor or non-anchor nodes, but the interfaces are not pushed due to the following fault: "Failed to configure SPAN with source SpanFL3out due to Source fvIfConn not available".

5.1(2e) and later

CSCvv69041

There is traffic loss on SDA after deleting an overlapping subnet.

5.1(2e) and later

CSCvw12766

In a setup where there is already existing MDP confguration (spine and leaf nodes), after having deleted an MDP spine node, MDP tunnels and traffic might still be directed to that spine node. In the case of a new MDP spine node, the traffic might not get directed to the new spine node.

5.1(2e) and later

CSCvx10921

A standby APIC disappears from the GUI after cluster convergence.

5.1(2e) and later

CSCvy17504

When the OpFlexAgent moved from one vPC pair leaf switches to a new vPC pair, it may take up to 20 minutes for the OpFlexAgent detected the movement, and reconnect the OpFlex channel. Ideally, this should be completed within a few seconds.

5.1(2e) and later

Resolved Issues

There are no resolved issues in this release.

Known Issues

Click the bug ID to access the Bug Search tool and see additional information about the bug. The "Exists In" column of the table specifies the 5.1(2) releases in which the bug exists. A bug might also exist in releases other than the 5.1(2) releases.

Bug ID

Description

Exists in

CSCvj26666

The "show run leaf|spine <nodeId>" command might produce an error for scaled up configurations.

5.1(2e) and later

CSCvj90385

With a uniform distribution of EPs and traffic flows, a fabric module in slot 25 sometimes reports far less than 50% of the traffic compared to the traffic on fabric modules in non-FM25 slots.

5.1(2e) and later

CSCvm71833

Switch upgrades fail with the following error: Version not compatible.

5.1(2e) and later

CSCvq39764

When you click Restart for the Microsoft System Center Virtual Machine Manager (SCVMM) agent on a scaled-out setup, the service may stop. You can restart the agent by clicking Start.

5.1(2e) and later

CSCvq58953

One of the following symptoms occurs:

· App installation/enable/disable takes a long time and does not complete.

· Nomad leadership is lost. The output of the acidiag scheduler logs members command contains the following error:

Error querying node status: Unexpected response code: 500 (rpc error: No cluster leader)

5.1(2e) and later

CSCvr89603

The CRC and stomped CRC error values do not match when seen from the APIC CLI compared to the APIC GUI. This is expected behavior. The GUI values are from the history data, whereas the CLI values are from the current data.

5.1(2e) and later

CSCvs19322

Upgrading Cisco APIC from a 3.x release to a 4.x release causes Smart Licensing to lose its registration. Registering Smart Licensing again will clear the fault.

5.1(2e) and later

CSCvs77929

In the 4.x and later releases, if a firmware policy is created with different name than the maintenance policy, the firmware policy will be deleted and a new firmware policy gets created with the same name, which causes the upgrade process to fail.

5.1(2e) and later

N/A

Beginning in Cisco APIC release 4.1(1), the IP SLA monitor policy validates the IP SLA port value. Because of the validation, when TCP is configured as the IP SLA type, Cisco APIC no longer accepts an IP SLA port value of 0, which was allowed in previous releases. An IP SLA monitor policy from a previous release that has an IP SLA port value of 0 becomes invalid if the Cisco APIC is upgraded to release 4.1(1) or later. This results in a failure for the configuration import or snapshot rollback.

The workaround is to configure a non-zero IP SLA port value before upgrading the Cisco APIC, and use the snapshot and configuration export that was taken after the IP SLA port change.

5.1(2e) and later

N/A

If you use the REST API to upgrade an app, you must create a new firmware.OSource to be able to download a new app image.

5.1(2e) and later

N/A

In a multipod configuration, before you make any changes to a spine switch, ensure that there is at least one operationally "up" external link that is participating in the multipod topology. Failure to do so could bring down the multipod connectivity. For more information about multipod, see the Cisco Application Centric Infrastructure Fundamentals document and the Cisco APIC Getting Started Guide.

5.1(2e) and later

N/A

With a non-english SCVMM 2012 R2 or SCVMM 2016 setup and where the virtual machine names are specified in non-english characters, if the host is removed and re-added to the host group, the GUID for all the virtual machines under that host changes. Therefore, if a user has created a micro segmentation endpoint group using "VM name" attribute specifying the GUID of respective virtual machine, then that micro segmentation endpoint group will not work if the host (hosting the virtual machines) is removed and re-added to the host group, as the GUID for all the virtual machines would have changed. This does not happen if the virtual name has name specified in all english characters.

5.1(2e) and later

N/A

A query of a configurable policy that does not have a subscription goes to the policy distributor. However, a query of a configurable policy that has a subscription goes to the policy manager. As a result, if the policy propagation from the policy distributor to the policy manager takes a prolonged amount of time, then in such cases the query with the subscription might not return the policy simply because it has not reached policy manager yet.

5.1(2e) and later

N/A

When there are silent hosts across sites, ARP glean messages might not be forwarded to remote sites if a leaf switch without -EX or a later designation in the product ID happens to be in the transit path and the VRF is deployed on that leaf switch, the switch does not forward the ARP glean packet back into the fabric to reach the remote site. This issue is specific to transit leaf switches without -EX or a later designation in the product ID and does not affect leaf switches that have -EX or a later designation in the product ID. This issue breaks the capability of discovering silent hosts.

5.1(2e) and later

N/A

Typically, faults are generally raised based on the presence of the BGP route target profile under the VRF table. However, if a BGP route target profile is configured without actual route targets (that is, the profile has empty policies), a fault will not be raised in this situation.

5.1(2e) and later

N/A

MPLS interface statistics shown in a switch's CLI get cleared after an admin or operational down event.

5.1(2e) and later

N/A

MPLS interface statistics in a switch's CLI are reported every 10 seconds. If, for example, an interface goes down 3 seconds after the collection of the statistics, the CLI reports only 3 seconds of the statistics and clears all of the other statistics.

5.1(2e) and later

Virtualization Compatibility Information

This section lists virtualization compatibility information for the Cisco APIC software.

· For a table that shows the supported virtualization products, see the ACI Virtualization Compatibility Matrix.

· For information about Cisco APIC compatibility with Cisco UCS Director, see the appropriate Cisco UCS Director Compatibility Matrix document.

· If you use Microsoft vSwitch and want to downgrade to Cisco APIC Release 2.3(1) from a later release, you first must delete any microsegment EPGs configured with the Match All filter.

· This release supports the following additional virtualization products:

Product

Supported Release

Information Location

Microsoft Hyper-V

2016 Update Rollup 1, 2, 2.1, and 3

N/A

VMM Integration and VMware Distributed Virtual Switch (DVS)

6.5, 6.7, and 7.0

Cisco ACI Virtualization Guide, Release 5.0(x)

Hardware Compatibility Information

This release supports the following Cisco APIC servers:

Product ID

Description

APIC-L1

Cisco APIC with large CPU, hard drive, and memory configurations (more than 1000 edge ports)

APIC-L2

Cisco APIC with large CPU, hard drive, and memory configurations (more than 1000 edge ports)

APIC-L3

Cisco APIC with large CPU, hard drive, and memory configurations (more than 1200 edge ports)

APIC-M1

Cisco APIC with medium-size CPU, hard drive, and memory configurations (up to 1000 edge ports)

APIC-M2

Cisco APIC with medium-size CPU, hard drive, and memory configurations (up to 1000 edge ports)

APIC-M3

Cisco APIC with medium-size CPU, hard drive, and memory configurations (up to 1200 edge ports)


The following list includes general hardware compatibility information:

· For the supported hardware, see the Cisco Nexus 9000 ACI-Mode Switches Release Notes, Release 15.1(2).

· Contracts using matchDscp filters are only supported on switches with "EX" on the end of the switch name. For example, N9K-93108TC-EX.

· When the fabric node switch (spine or leaf) is out-of-fabric, the environmental sensor values, such as Current Temperature, Power Draw, and Power Consumption, might be reported as "N/A." A status might be reported as "Normal" even when the Current Temperature is "N/A."

· First generation switches (switches without -EX, -FX, -GX, or a later suffix in the product ID) do not support Contract filters with match type "IPv4" or "IPv6." Only match type "IP" is supported. Because of this, a contract will match both IPv4 and IPv6 traffic when the match type of "IP" is used.

The following table provides compatibility information for specific hardware:

Product ID

Description

Cisco UCS M4-based Cisco APIC

The Cisco UCS M4-based Cisco APIC and previous versions support only the 10G interface. Connecting the Cisco APIC to the Cisco ACI fabric requires a same speed interface on the Cisco ACI leaf switch. You cannot connect the Cisco APIC directly to the Cisco N9332PQ ACI leaf switch, unless you use a 40G to 10G converter (part number CVR-QSFP-SFP10G), in which case the port on the Cisco N9332PQ switch auto-negotiates to 10G without requiring any manual configuration.

Cisco UCS M5-based Cisco APIC

The Cisco UCS M5-based Cisco APIC supports dual speed 10G and 25G interfaces. Connecting the Cisco APIC to the Cisco ACI fabric requires a same speed interface on the Cisco ACI leaf switch. You cannot connect the Cisco APIC directly to the Cisco N9332PQ ACI leaf switch, unless you use a 40G to 10G converter (part number CVR-QSFP-SFP10G), in which case the port on the Cisco N9332PQ switch auto-negotiates to 10G without requiring any manual configuration.

N2348UPQ

To connect the N2348UPQ to Cisco ACI leaf switches, the following options are available:

· Directly connect the 40G FEX ports on the N2348UPQ to the 40G switch ports on the Cisco ACI leaf switches

· Break out the 40G FEX ports on the N2348UPQ to 4x10G ports and connect to the 10G ports on all other Cisco ACI leaf switches.

Note: A fabric uplink port cannot be used as a FEX fabric port.

N9K-C9348GC-FXP

This switch does not read SPROM information if the PSU is in a shut state. You might see an empty string in the Cisco APIC output.

N9K-C9364C-FX

Ports 49-64 do not supporFut 1G SFPs with QSA.

N9K-C9508-FM-E

The Cisco N9K-C9508-FM-E2 and N9K-C9508-FM-E fabric modules in the mixed mode configuration are not supported on the same spine switch.

N9K-C9508-FM-E2

The Cisco N9K-C9508-FM-E2 and N9K-C9508-FM-E fabric modules in the mixed mode configuration are not supported on the same spine switch.

The locator LED enable/disable feature is supported in the GUI and not supported in the Cisco ACI NX-OS switch CLI.

N9K-C9508-FM-E2

This fabric module must be physically removed before downgrading to releases earlier than Cisco APIC 3.0(1).

N9K-X9736C-FX

The locator LED enable/disable feature is supported in the GUI and not supported in the Cisco ACI NX-OS Switch CLI.

N9K-X9736C-FX

Ports 29 to 36 do not support 1G SFPs with QSA.

Adaptive Security Appliance (ASA) Compatibility Information

This section lists ASA compatibility information for the Cisco APIC software.

· This release supports Adaptive Security Appliance (ASA) device package version 1.2.5.5 or later.

· If you are running a Cisco Adaptive Security Virtual Appliance (ASA) version that is prior to version 9.3(2), you must configure SSL encryption as follows:

(config)# ssl encryption aes128-sha1

Miscellaneous Compatibility Information

This release supports the following products:

Product

Supported Release

Cisco NX-OS

15.1(2)

Cisco AVS

5.2(1)SV3(4.10)

For more information about the supported AVS releases, see the AVS software compatibility information in the Cisco Application Virtual Switch Release Notes, Release 5.2(1)SV3(4.11).

Cisco UCS Manager

2.2(1c) or later is required for the Cisco UCS Fabric Interconnect and other components, including the BIOS, CIMC, and the adapter.

CIMC HUU ISO

Network Insights Base, Network Insights Advisor, and Network Insights for Resources

For the release information, documentation, and download links, see the Cisco Network Insights for Data Center page.

For the supported releases, see the Cisco Data Center Networking Applications Compatibility Matrix.


· This release supports the partner packages specified in the L4-L7 Compatibility List Solution Overview document.

· A known issue exists with the Safari browser and unsigned certificates, which applies when connecting to the Cisco APIC GUI. For more information, see the Cisco APIC Getting Started Guide, Release 5.1(x).

· For compatibility with Day-2 Operations apps, see the Cisco Data Center Networking Applications Compatibility Matrix.

· Cisco Nexus Dashboard Insights creates a user in Cisco APIC called cisco_SN_NI. This user is used when Nexus Dashboard Insights needs to make any changes or query any information from the Cisco APIC. In the Cisco APIC, navigate to the Audit Logs tab of the System > History page. The cisco_SN_NI user is displayed in the User column.

Related Content

See the Cisco Application Policy Infrastructure Controller (APIC) page for the documentation.

The documentation includes installation, upgrade, configuration, programming, and troubleshooting guides, technical references, release notes, and knowledge base (KB) articles, as well as other documentation. KB articles provide information about a specific use case or a specific topic.

By using the "Choose a topic" and "Choose a document type" fields of the APIC documentation website, you can narrow down the displayed documentation list to make it easier to find the desired document.

You can watch videos that demonstrate how to perform specific tasks in the Cisco APIC on the Cisco Data Center Networking YouTube channel.

Temporary licenses with an expiry date are available for evaluation and lab use purposes. They are strictly not allowed to be used in production. Use a permanent or subscription license that has been purchased through Cisco for production purposes. For more information, go to Cisco Data Center Networking Software Subscriptions.

The following table provides links to the release notes, verified scalability documentation, and new documentation:

Document

Description

Cisco Nexus 9000 ACI-Mode Switches Release Notes, Release 15.1(2)

The release notes for Cisco NX-OS for Cisco Nexus 9000 Series ACI-Mode Switches.

Verified Scalability Guide for Cisco APIC, Release 5.1(1), Multi-Site, Release 3.1(1), and Cisco Nexus 9000 Series ACI-Mode Switches, Release 15.1(1)

This guide contains the maximum verified scalability limits for Cisco Application Centric Infrastructure (ACI) parameters for Cisco APIC, Cisco ACI Multi-Site, and Cisco Nexus 9000 Series ACI-Mode Switches.

Note: This document is not yet available, but will be soon.


Cisco ACI Multi-Site Orchestrator Release Notes, Release 3.0(3)

This document describes the features, issues, and deployment guidelines for the Cisco Application Centric Infrastructure (ACI) Multi-Site Orchestrator software.

Cisco ACI Multi-Site is an architecture that allows you to interconnect separate Cisco APIC cluster domains (fabrics), each representing a different region. This helps ensure multitenant Layer 2 and Layer 3 network connectivity across sites and extends the policy domain end-to-end across the entire system.

Cisco ACI Multi-Site Orchestrator is the intersite policy manager. It provides single-pane management that enables you to monitor the health of all the interconnected sites. It also allows you to centrally define the intersite policies that can then be pushed to the different Cisco APIC fabrics, which in term deploys them on the physical switches that make up those fabrics. This provides a high degree of control over when and where to deploy those policies.

For more information, see Related Content.

Date

Description

October 21, 2021

Additional open issue CSCvt23491.

March 24, 2021

Additional open issues CSCvx49247, CSCvx51152, CSCvx52230, CSCvx79883.

These issues are resolved in Release 3.1(1i).

December 22, 2020

Release 3.0(3m) became available.

Additional open issues for 3.0(3i) and later.

December 2, 2020

Additional open issue CSCvw61549.

October 14, 2020

Release 3.0(3l) became available.

September 4, 2020

Release 3.0(3i) became available.

Contents

■ New Software Features

■ New Hardware Features

■ Changes in Behavior

■ Open Issues

■ Resolved Issues

■ Known Issues

■ Usage Guidelines

■ Compatibility

■ Scalability

■ Related Content

■ Legal Information

New Software Features

This release adds the following new features:

Feature

Description

Support for Intersite L3Out with Policy Based Redirect (PBR)

This capability enables endpoints in site 1 to use PBR policy with site 2 L3Out for WAN connectivity (intersite L3Out). For example, if each site has its firewall cluster and L3Out, if one of the site’s L3Out goes down, the application EPG endpoints in that site can use the other site’s L3Out for WAN connectivity.

For more information, see Cisco ACI Multi-Site Configuration Guide, Release 3.0(x).

New Hardware Features

There is no new hardware supported in this release.

The complete list of supported hardware is available in the Cisco ACI Multi-Site Hardware Requirements Guide.

Changes in Behavior

If you are upgrading to this release, you will see the following changes in behavior:

■ For all new deployments, we recommend installing Multi-Site Orchestrator, Release 3.0(2j) or later in Application Services Engine.

■ If your Multi-Site Orchestrator is deployed in Application Services Engine and you are upgrading from Release 3.0(1) or earlier, you must deploy a new Application Services Engine, Release 1.1.3d cluster and migrate your existing configuration.

The procedure is described in detail in Cisco ACI Multi-Site Orchestrator Installation and Upgrade Guide.

■ Any shadow objects created by the Multi-Site Orchestrator will now be automatically hidden in each site’s APIC GUI if that site is running Cisco APIC, Release 5.0(2) or later. You can choose to disable this feature in the APIC GUI if you want the objects to be visible.

■ Simultaneous updates to Sites, Tenants, and Schemas from multiple Multi-Site Orchestrator GUI sessions will no longer cause some changes to be overwritten or lost. However, the default REST API functionality was left unchanged in order to preserve backward compatibility with existing applications. In other words, while the UI is always enabled for this protection, you must explicitly enable it for your UPDATE and DELETE API calls for MSO to keep track of configuration changes, as described in the Cisco ACI Multi-Site REST API Configuration Guide.

■ Starting with Release 2.2(3), additional External EPG subnet flags have been exposed through the Multi-Site Orchestrator GUI.

Prior to Release 2.2(3), only the following subset of external EPG subnet flags available on each site’s APIC was managed by the Multi-Site Orchestrator:

— Shared Route Control—configurable in the Orchestrator GUI

— Shared Security Import—configurable in the Orchestrator GUI

— Aggregate Shared Routes—configurable in the Orchestrator GUI

— External Subnets for External EPG—not configurable in the GUI, but always implicitly enabled

Starting with Release 2.2(3), all subnet flags available from the APIC can be configured and managed from the Orchestrator:

— Export Route Control

— Import Route Control

— Shared Route Control

— Aggregate Shared Routes

— Aggregate Export (enabled for 0.0.0.0 subnet only)

— Aggregate Import (enabled for 0.0.0.0 subnet only)

— External Subnets for External EPG

— Shared Security Import

When upgrading to this release from Release 2.2(2) or earlier, any subnet flags previously unavailable in the Orchestrator GUI will be imported from the APIC and added to the Orchestrator configuration. All imported flags will retain their state (enabled or disabled) with the exception of External Subnets for External EPG, which will remain enabled post-upgrade. If you had previously explicitly disabled the External Subnets for External EPG flag directly in the APIC (for example, in Cloud APIC use case) you will need to disable it again through the Orchestrator GUI.

When downgrading from this release to Release 2.2(2) or earlier, the subnet flags not available in those releases will be cleared and set to disabled in the sites’ APICs. You can then manually enable them directly in each site’s APIC if necessary.

For additional information on these flags, see the Cisco ACI Multi-Site Configuration Guide.

■ When upgrading from a release prior to Release 2.2(1), a GUI lockout timer for repeated failed login attempts is automatically enabled by default and is set to 5 login attempts before a lockout with the lockout duration incremented exponentially every additional failed attempt.

■ If you configure read-only user roles in Release 2.1(2) or later and then choose to downgrade your Multi-Site Orchestrator to an earlier version where the read-only roles are not supported:

— You will need to reconfigure your external authentication servers to the old attribute-value (AV) pair string format. For details, see the "Administrative Operations" chapter in the Cisco ACI Multi-Site Configuration Guide.

— The read-only roles will be removed from all users. This also means that any user that has only the read-only roles will have no roles assigned to them and a Power User or User Manager will need to re-assign them new read-write roles.

■ Starting with Release 2.1(2), the 'phone number' field is no longer mandatory when creating a new Multi-Site Orchestrator user. However, because the field was required in prior releases, any user created in Release 2.1(2) or later without a phone number provided will be unable to log into the GUI if the Orchestrator is downgraded to Release 2.1(1) or earlier. In this case, a Power User or User Manager will need to provide a phone number for the user.

■ If you are upgrading from any release prior to Release 2.1(1), the default password and the minimum password requirements for the Multi-Site Orchestrator GUI have been updated. The default password has been changed from ‘We1come!” to “We1come2msc!” and the new password requirements are:

— At least 12 characters

— At least 1 letter

— At least 1 number

— At least 1 special character apart from * and space

You will be prompted to reset your passwords when you:

— First install Release 2.1(1) or later

— Upgrade to Release 2.1(1) or later from a release prior to Release 2.1(1)

— Restore the Multi-Site Orchestrator configuration from a backup

■ Starting with Release 2.1(1), Multi-Site Orchestrator encrypts all stored passwords, such as each site’s APIC passwords and the external authentication provider passwords. As a result, if you downgrade to any release prior to Release 2.1(1), you will need to re-enter all the passwords after the Orchestrator downgrade is completed.

To update APIC passwords:

a. Log in to the Orchestrator after the downgrade.

b. From the main navigation menu, select Sites.

c. For each site, edit its properties and re-enter its APIC password.

To update external authentication passwords:

a. Log in to the Orchestrator after the downgrade.

b. From the navigation menu, select Admin à Providers.

c. For each authentication provider, edit its properties and re-enter its password.

Open Issues

This section lists the open issues. Click the bug ID to access the Bug Search Tool and see additional information about the bug. The "Exists In" column of the table specifies the 3.0(3) releases in which the bug exists. A bug might also exist in releases other than the 3.0(3) releases.

Bug ID

Description

Exists in

CSCvv90265

Migrating from vzAny to regular contract impacts traffic in APIC.

3.0(3i)

CSCvv89702

When using vzAny contracts, shadow external EPGs are created in all APIC sites even if external EPG is stretched.

3.0(3i)

CSCvw04000

Associating a contract to tenant EPGs using MSO causes external EPG or L3out to be removed from APIC.

3.0(3i)

CSCvw07287

Deployment of a template having scaled PG-enabled EPGs and BDs takes a long time.

3.0(3i)-3.0(3l)

CSCvo84218

When service graphs or devices are created on Cloud APIC by using the API and custom names are specified for AbsTermNodeProv and AbsTermNodeCons, a brownfield import to the Multi-Site Orchestrator will fail.

3.0(3i) and later

CSCvo20029

Contract is not created between shadow EPG and on-premises EPG when shared service is configured between Tenants.

3.0(3i) and later

CSCvq58349

shadow of extepg's vrf not getting updated.

3.0(3i) and later

CSCvn98355

Inter-site shared service between VRF instances across different tenants will not work, unless the tenant is stretched explicitly to the cloud site with the correct provider credentials. That is, there will be no implicit tenant stretch by Multi-Site Orchestrator.

3.0(3i) and later

CSCvr19577

If a template with empty AP (cloudApp without any cloudEpgs) is defined and it's undeployed, it deletes the cloudApp. If other templates are defined with same AP name and have cloudEpgs, then as a result of cloudApp deletion, all those cloudEpgs defined in other templates are also deleted.

3.0(3i) and later

CSCvs32126

Traffic may stop for EPGs stretched between on-premises and cloud sites.

3.0(3i) and later

CSCvs99052

Deployment window may show more policies been modified than the actual config changed by the user in the Schema.

3.0(3i) and later

CSCvt06351

Deployment window may not show all the service graph related config values that have been modified.

3.0(3i) and later

CSCvt00663

Deployment window may not show all the cloud related config values that have been modified.

3.0(3i) and later

CSCvs22418

Traffic is impacted when changing the VRF associated with BDs referred by PG enabled EPGs that have a global contract between them

3.0(3i) and later

CSCvt41911

After brownfield import, the BD subnets are present in site local and not in the common template config

3.0(3i) and later

CSCvt42771

MSO shows the L3Outs of another tenant when associating it with a BD.

3.0(3i) and later

CSCvt44081

In shared services use case, if one VRF has preferred group enabled EPGs and another VRF has vzAny contracts, traffic drop is seen.

3.0(3i) and later

CSCvt47568

Let's say APIC has EPGs with some contract relationships. If this EPG and the relationships are imported into MSO and then the relationship was removed and deployed to APIC, MSO doesn't delete the contract relationship on the APIC.

3.0(3i) and later

CSCvt47581

fvImportExtRoutes flag is created for VRF even though site1 & site3 external-epgs have provider contract.

3.0(3i) and later

CSCvt56139

When you try to upgrade MSO from 2.0(x) to 3.0(1), the upgrade script shows the following errors in the logs:

ERROR site 5e5eff4b120000892d98c2dd of templateSite (schema: 5e688b0c110000480b02b3f6 template: Template1) not found in schema!

ERROR schema not found for schemaId: 5e66c0911200004f2c6e542e

However, the upgrade completes correctly.

3.0(3i) and later

CSCvs71068

MSO-owned VRF exists on APIC when the owner Template on MSO is un-deployed.

3.0(3i) and later

CSCvt02480

The REST API call "/api/v1/execute/schema/5e43523f1100007b012b0fcd/template/Template_11?undeploy=all" can fail if the template being deployed has a large object count

3.0(3i) and later

CSCvr19577

If a template with empty AP (cloudApp without any cloudEpgs) is defined and it's undeployed, it deletes the cloudApp. If other templates are defined with same AP name and have cloudEpgs, then as a result of cloudApp deletion, all those cloudEpgs defined in other templates are also deleted.

3.0(3i) and later

CSCvt15312

Shared service traffic drops from external EPG to EPG in case of EPG provider and L3Out vzAny consumer

3.0(3i) and later

CSCvt11713

Intersite L3Out traffic is impacted because of missing import RT for VPN routes

3.0(3i) and later

CSCvt99784

Traffic between onPrem ExternalEPG (aka InstP) and the cloudEPG is disrupted due to a) the deletion of the shadow InstP created for the cloudEPG on the OnPrem Site and b) the deletion of cloudEPG's shadow VRF on the OnPrem Site.

3.0(3i) and later

CSCvu02398

When BD's subnets are changed, the cloud ExtEpg doesnt get the updated cloud extepselector.

3.0(3i) and later

CSCvt11713

Intersite L3Out traffic is impacted because of missing import RT for VPN routes

3.0(3i) and later

CSCvt08407

Schema/Template deployment fails when an already added cAPIC site with name-A and with IP address - IP-A is freshly brought up and different IP address is assigned to that cAPIC site.

3.0(3i) and later

CSCvu56069

Site deletion throws error: "Error from APIC: https://20.185.81.79, error: Invalid Configuration CT_EXTNETWORK_REQUIRES_EXTSUBNETPOOL: At present, there must be at least one cloudtemplateExtSubnetPool in cloudtemplateInfraNetwork uni/tn-infra/infranetwork-default when cloudtemplateExtNetwork is present; current count = 0 FORCE DELETE SITE"

3.0(3i) and later

CSCvu65041

Creating a contract between on-premises External EPG and stretched EPG impacts other on-premises L3Out External EPG routes.

3.0(3i) and later

CSCvu88287

User deploys multinode graph from MSO on aws cAPIC site and the configuration can fail on AWS cAPIC and the expected service chaining won't work.

Also if AWS cAPIC (POST) allows Third Party FW creation and user selects that FW device for multinode graph in MSO, the configuration can fail on AWS cAPIC and the expected service chaining won't work.

3.0(3i) and later

CSCvu71584

Routes are not programmed on CSR and the contract config is not pushed to the Cloud site.

3.0(3i) and later

CSCvv00462

Unable to add the APIC site with different site ID if it was previously removed MSO.

3.0(3i) and later

CSCvw61549

Unable to select the site local L3Out for a newly created BD from MSO.

3.0(3i) and later


CSCvu26250

Execution Engine pushing empty tenant to APIC and failing with the following error showing in MSO:

Bad Request: Error from APIC:https//<ip_address>, error: Request failed, unable to resolve destination

3.0(3i) and later

CSCvw12894

With Application Services Engine deployment, packets originating from one of the nodes have the data interface IP as the source and destination IP that is in the range of overlay addressing which should note be visible to the user network. If the destination IP of the packet is in overlay address range, they should be VXLAN encapsulation.

3.0(3i) and later

CSCvw60496

MSO fails to deploy External EPG to template with "L3Out: (l3out name) is already deployed by schema" message.

3.0(3i) and later

CSCvt47581

fvImportExtRoutes flag is created for VRF even though site1 & site3 external EPGs have provider contract.

3.0(3i) and later

CSCvw87921

L3out external EPG unexpectedly pushed to one of the sites even though the MO doesn't exists in the GUI but is pushed every time when existing parent L3Out changes are pushed to the sites.

3.0(3i) and later

CSCvx49247

Template deployment fails with exception message: "execution.exception.BadExecutionRequestException: None.get".

3.0(3i) and later

CSCvx51152

Config drift is shown for EPGs after upgrade or backup restore.

3.0(3i) and later

CSCvx52230

Template deployment fails with exception message: "BSONObj size: 16828180 (0x100C714) is invalid. Size must be between 0 and 16793600(16MB) First element: update: \"templateSites\""

3.0(3i) and later

CSCvx79883

The following error may be displayed: "L3Out was already deployed by a different template LD5".

This error is misleading as the L3Out is already deployed by a different template but the current template doesn't have this L3Out. The Execution Engine service attempts to create it causing the validation code to throws an error.

3.0(3i) and later

CSCvt23491

Enhancement to add the ability in MSO to configure multiple DHCP relay polices for a BD.

3.0(3i) and later

Resolved Issues

This section lists the resolved issues. Click the bug ID to access the Bug Search tool and see additional information about the issue. The "Fixed In" column of the table specifies whether the bug was resolved in the base release or a patch release.

Bug ID

Description

Fixed in

CSCvv22363

When user has logged into Application Services Engine to perform certain operation followed by accessing MSO application running on the same SE, MSO app API/UI gives 401/403 for some time.

3.0(3i)

CSCvv45150

When an admin desires to install a new image of the MSO firmware, this can be done via the firmware section. Images can be added either from a remote web server or from a local file system, e.g. a laptop. An upload of a local image will hang and not complete.

3.0(3i)

CSCvv90265

Migrating from vzAny to regular contract impacts traffic in APIC.

3.0(3l)

CSCvv89702

When using vzAny contracts, shadow external EPGs are created in all APIC sites even if external EPG is stretched.

3.0(3l)

CSCvw04000

Associating a contract to tenant EPGs using MSO causes external EPG or L3out to be removed from APIC.

3.0(3l)

CSCvu26250

Execution Engine pushing empty tenant to APIC and failing with the following error showing in MSO:

Bad Request: Error from APIC:https//<ip_address>, error: Request failed, unable to resolve destination

3.0(3m)

CSCvw07287

Deployment of a template having scaled PG-enabled EPGs and BDs takes a long time.

3.0(3m)

CSCvw12894

With Application Services Engine deployment, packets originating from one of the nodes have the data interface IP as the source and destination IP that is in the range of overlay addressing which should note be visible to the user network. If the destination IP of the packet is in overlay address range, they should be VXLAN encapsulation.

3.0(3m)

CSCvw60496

MSO fails to deploy External EPG to template with "L3Out: (l3out name) is already deployed by schema" message.

3.0(3m)

CSCvt47581

fvImportExtRoutes flag is created for VRF even though site1 & site3 external EPGs have provider contract.

3.0(3m)

CSCvw87921

L3out external EPG unexpectedly pushed to one of the sites even though the MO doesn't exists in the GUI but is pushed every time when existing parent L3Out changes are pushed to the sites.

3.0(3m)

Known Issues

This section lists known behaviors. Click the Bug ID to access the Bug Search Tool and see additional information about the issue.

Bug ID

Description

CSCvo82001

Unable to download Multi-Site Orchestrator report and debug logs when database and server logs are selected

CSCvo32313

Unicast traffic flow between Remote Leaf Site1 and Remote Leaf in Site2 may be enabled by default. This feature is not officially supported in this release.

CSCvn38255

After downgrading from 2.1(1), preferred group traffic continues to work. You must disable the preferred group feature before downgrading to an earlier release.

CSCvn90706

No validation is available for shared services scenarios

CSCvo59133

The upstream server may time out when enabling audit log streaming

CSCvd59276

For Cisco ACI Multi-Site, Fabric IDs Must be the Same for All Sites, or the Querier IP address Must be Higher on One Site.

The Cisco APIC fabric querier functions have a distributed architecture, where each leaf switch acts as a querier, and packets are flooded. A copy is also replicated to the fabric port. There is an Access Control List (ACL) configured on each TOR to drop this query packet coming from the fabric port. If the source MAC address is the fabric MAC address, unique per fabric, then the MAC address is derived from the fabric-id. The fabric ID is configured by users during initial bring up of a pod site.

In the Cisco ACI Multi-Site Stretched BD with Layer 2 Broadcast Extension use case, the query packets from each TOR get to the other sites and should be dropped. If the fabric-id is configured differently on the sites, it is not possible to drop them.

To avoid this, configure the fabric IDs the same on each site, or the querier IP address on one of the sites should be higher than on the other sites.

CSCvd61787

STP and "Flood in Encapsulation" Option are not Supported with Cisco ACI Multi-Site.

In Cisco ACI Multi-Site topologies, regardless of whether EPGs are stretched between sites or localized, STP packets do not reach remote sites. Similarly, the "Flood in Encapsulation" option is not supported across sites. In both cases, packets are encapsulated using an FD VNID (fab-encap) of the access VLAN on the ingress TOR. It is a known issue that there is no capability to translate these IDs on the remote sites.

CSCvi61260

If an infra L3Out that is being managed by Cisco ACI Multi-Site is modified locally in a Cisco APIC, Cisco ACI Multi-Site might delete the objects not managed by Cisco ACI Multi-Site in an L3Out.

CSCvq07769

"Phone Number" field is required in all releases prior to Release 2.2(1). Users with no phone number specified in Release 2.2(1) or later will not be able to log in to the GUI when Orchestrator is downgraded to a an earlier release.

Usage Guidelines

This section lists usage guidelines for the Cisco ACI Multi-Site software.

■ For all new deployments, we recommend installing Multi-Site Orchestrator, Release 3.0(2j) or later in Application Services Engine.

Multi-Site Orchestrator, Release 3.0(2j) requires Application Services Engine, Release 1.1.3d.

■ In Cisco ACI Multi-Site topologies, we recommend that First Hop Routing protocols such as HSRP/VRRP are not stretched across sites.

■ HTTP requests are redirected to HTTPS and there is no HTTP support globally or per user basis.

■ Up to 12 interconnected sites are supported.

■ Proxy ARP glean and unknown unicast flooding are not supported together.

Unknown Unicast Flooding and ARP Glean are not supported together in Cisco ACI Multi-Site across sites.

■ Flood in encapsulation is not supported for EPGs and Bridge Domains that are extended across ACI fabrics that are part of the same Multi-Site domain. However, flood in encapsulation is fully supported for EPGs or Bridge Domains that are locally defined in ACI fabrics, even if those fabrics may be configured for Multi-Site.

■ The leaf and spine nodes that are part of an ACI fabric do not run Spanning Tree Protocol (STP). STP frames originated from external devices can be forwarded across an ACI fabric (both single Pod and Multi-Pod), but are not forwarded across the inter-site network between sites, even if stretching a BD with BUM traffic enabled.

■ GOLF L3Outs for each tenant must be dedicated, not shared.

The inter-site L3Out functionality introduced on MSO release 2.2(1) does not apply when deploying GOLF L3Outs. This means that for a given VRF there is still the requirement of deploying at least one GOLF L3Out per site in order to enable north-south communication. An endpoint connected in a site cannot communicate with resources reachable via a GOLF L3Out connection deployed in a different site.

■ While you can create the L3Out objects in the Multi-Site Orchestrator GUI, the physical L3Out configuration (logical nodes, logical interfaces, and so on) must be done directly in each site's APIC.

■ VMM and physical domains must be configured in the Cisco APIC GUI at the site and will be imported and associated within the Cisco ACI Multi-Site.

Although domains (VMM and physical) must be configured in Cisco APIC, domain associations can be configured in the Cisco APIC or Cisco ACI Multi-Site.

■ Some VMM domain options must be configured in the Cisco APIC GUI.

The following VMM domain options must be configured in the Cisco APIC GUI at the site:

— NetFlow/EPG CoS marking in a VMM domain association

— Encapsulation mode for an AVS VMM domain

■ Some uSeg EPG attribute options must be configured in the Cisco APIC GUI.

The following uSeg EPG attribute options must be configured in the Cisco APIC GUI at the site:

— Sub-criteria under uSeg attributes

— match-all and match-any criteria under uSeg attributes

■ Site IDs must be unique.

In Cisco ACI Multi-Site, site IDs must be unique.

■ To change a Cisco APIC fabric ID, you must erase and reconfigure the fabric.

Cisco APIC fabric IDs cannot be changed. To change a Cisco APIC fabric ID, you must erase the fabric configuration and reconfigure it.

However, Cisco ACI Multi-Site supports connecting multiple fabrics with the same fabric ID.

■ Caution: When removing a spine switch port from the Cisco ACI Multi-Site infrastructure, perform the following steps:

a. Click Sites.

b. Click Configure Infra.

c. Click the site where the spine switch is located.

d. Click the spine switch.

e. Click the x on the port details.

f. Click Apply.

■ Shared services use case: order of importing tenant policies

When deploying a provider site group and a consumer site group for shared services by importing tenant policies, deploy the provider tenant policies before deploying the consumer tenant policies. This enables the relation of the consumer tenant to the provider tenant to be properly formed.

■ Caution for shared services use case when importing a tenant and stretching it to other sites

When you import the policies for a consumer tenant and deploy them to multiple sites, including the site where they originated, a new contract is deployed with the same name (different because it is modified by the inter-site relation). To avoid confusion, delete the original contract with the same name on the local site. In the Cisco APIC GUI, the original contract can be distinguished from the contract that is managed by Cisco ACI Multi-Site, because it is not marked with a cloud icon.

■ When a contract is established between EPGs in different sites, each EPG and its bridge domain (BD) are mirrored to and appear to be deployed in the other site, while only being actually deployed in its own site. These mirrored objects are known as "shadow” EPGs and BDs.

For example, if one EPG in Site 1 and another EPG in Site 2 have a contract between them, in the Cisco APIC GUI at Site 1 and Site 2, both EPGs will be present. They appear with the same names as the ones that were deployed directly to each site. This is expected behavior and the shadow objects must not be removed. In Cisco APIC releases prior to Release 5.0(2), these objects are always visible. Starting with Cisco APIC, Release 5.0(2), these shadow objects are automatically hidden from view, but are still present in the configuration; you can choose to show these objects by enabling a user setting in each site’s APIC GUI.

For more information, see the Schema Management chapter in the Cisco ACI Multi-Site Configuration Guide.

■ Inter-site traffic cannot transit sites.

Site traffic cannot transit sites on the way to another site. For example, when Site 1 routes traffic to Site 3, it cannot be forwarded through Site 2.

■ The ? icon in Cisco ACI Multi-Site opens the menu for Show Me How modules, which provide step-by-step help through specific configurations.

— If you deviate while in progress of a Show Me How module, you will no longer be able to continue.

— You must have IPv4 enabled to use the Show Me How modules.

■ User passwords must meet the following criteria:

— Minimum length is 8 characters

— Maximum length is 64 characters

— Fewer than three consecutive repeated characters

— At least three of the following character types: lowercase, uppercase, digit, symbol

— Cannot be easily guessed

— Cannot be the username or the reverse of the username

— Cannot be any variation of "cisco", "isco", or any permutation of these characters or variants obtained by changing the capitalization of letters therein

■ If you are associating a contract with the external EPG, as provider, choose contracts only from the tenant associated with the external EPG. Do not choose contracts from other tenants. If you are associating the contract to the external EPG, as consumer, you can choose any available contract.

■ Policy objects deployed from ACI Multi-Site software should not be modified or deleted from any site-APIC. If any such operation is performed, schemas have to be re-deployed from ACI Multi-Site software.

■ The Rogue Endpoint feature can be used within each site of an ACI Multi-Site deployment to help with misconfigurations of servers that cause an endpoint to move within the site. The Rogue Endpoint feaure is not designed for scenarios where the endpoint may move between sites.

Compatibility

This release supports the hardware listed in the Cisco ACI Multi-Site Hardware Requirements Guide.

Multi-Site Orchestrator releases have been decoupled from the APIC releases. The APIC clusters in each site as well as the Orchestrator itself can now be upgraded independently of each other and run in mixed operation mode. For more information, see the Interoperability Support section in the “Infrastructure Management” chapter of the Cisco ACI Multi-Site Orchestrator Installation and Upgrade Guide.

Scalability

For the verified scalability limits, see the Cisco ACI Verified Scalability Guide.

Related ContentNOTE: Available paragraph styles are listed in the Quick Styles Gallery in the Styles group on the Home tab. Alternatively, they can be accessed via the Styles window (press Alt + Ctrl + Shift + S).

See the Cisco Application Policy Infrastructure Controller (APIC) page for ACI Multi-Site documentation. On that page, you can use the "Choose a topic" and "Choose a document type" fields to narrow down the displayed documentation list and find a desired document.

The documentation includes installation, upgrade, configuration, programming, and troubleshooting guides, technical references, release notes, and knowledge base (KB) articles, and videos. KB articles provide information about a specific use cases or topics. The following tables describe the core Cisco Application Centric Infrastructure Multi-Site documentation.

Document

Description

Cisco ACI Multi-Site Release Notes

This document. Provides release information for the Cisco ACI Multi-Site Orchestrator product.

Cisco ACI Multi-Site Fundamentals Guide

Provides basic concepts and capabilities of the Cisco ACI Multi-Site.

Cisco ACI Multi-Site Hardware Requirements Guide

Provides the hardware requirements and compatibility.

Cisco ACI Multi-Site Installation and Upgrade Guide

Describes how to install Cisco ACI Multi-Site Orchestrator and perform day-0 operations.

Cisco ACI Multi-Site Configuration Guide

Describes Cisco ACI Multi-Site configuration options and procedures.

Cisco ACI Multi-Site REST API Configuration Guide

Describes how to use the Cisco ACI Multi-Site REST APIs.

Cisco ACI Multi-Site Troubleshooting Guide

Provides descriptions of common operations issues and Describes how to troubleshoot common Cisco ACI Multi-Site issues.

Cisco ACI Verified Scalability

Contains the maximum verified scalability limits for Cisco Application Centric Infrastructure (Cisco ACI), including Cisco ACI Multi-Site.

Cisco ACI YouTube channel

Contains videos that demonstrate how to perform specific tasks in the Cisco ACI Multi-Site.


Cisco ACI Multi-Site Orchestrator Release Notes, Release 3.0(2)

This document describes the features, issues, and deployment guidelines for the Cisco Application Centric Infrastructure (ACI) Multi-Site Orchestrator software.

Cisco ACI Multi-Site is an architecture that allows you to interconnect separate Cisco APIC cluster domains (fabrics), each representing a different region. This helps ensure multitenant Layer 2 and Layer 3 network connectivity across sites and extends the policy domain end-to-end across the entire system.

Cisco ACI Multi-Site Orchestrator is the intersite policy manager. It provides single-pane management that enables you to monitor the health of all the interconnected sites. It also allows you to centrally define the intersite policies that can then be pushed to the different Cisco APIC fabrics, which in term deploys them on the physical switches that make up those fabrics. This provides a high degree of control over when and where to deploy those policies.

For more information, see Related Content.

Date

Description

October 21, 2021

Additional open issue CSCvt23491.

December 2, 2020

Additional open issue CSCvw61549.

October 2, 2020

Release 3.0(2k) became available.

August 22, 2020

Additional open issue CSCvv45150.

August 17, 2020

Release 3.0(2j) became available.

July 3, 2020

Release 3.0(2d) became available.


New Software Features

This release adds the following new features:

Feature

Description

SD-WAN integration using SLA for intersite traffic engineering (path selection) between sites

Cisco ACI Multi-Site Orchestrator adds support for SD-WAN integration by allowing you to import SLA policies from a vManage controller and map the DSCP values defined in those policies to ACI QoS levels. This enables you to apply preconfigured SLA policies to specify the levels of packet loss, jitter, and latency for intersite traffic over SD-WAN.

For more information, see Cisco ACI Multi-Site Configuration Guide, Release 3.0(x).

Single Sign-On (SSO) support for MSO, APIC and Cloud APIC

When you configure remote authentication for the MSO and APIC users, you can cross-launch into individual sites' APIC GUI directly from the MSO GUI without being prompted to log in at the APIC level.

For more information, see Cisco ACI Multi-Site Configuration Guide, Release 3.0(x).

VNET peering with Infra VNET for Azure Cloud

Cisco ACI Multi-Site Orchestrator adds support for Virtual network (VNet) peering, which enables seamless connection between two Azure Virtual Networks and is Microsoft's recommended way of forwarding data between two VNets. The virtual networks appear as one for connectivity purposes. The traffic between virtual machines uses the Microsoft backbone infrastructure. Like traffic between virtual machines in the same network, traffic is routed through Microsoft's private network only.

For more information, see the Configuring VNET Peering for Cloud APIC for Azure.

User Defined Routing (UDR) for Azure Cloud

Cisco ACI Multi-Site Orchestrator adds support for user-defined routing (UDR) for Cisco Cloud APIC, similar to the policy-based redirect (PBR) feature available for Cisco APIC.

For more information, see the Cisco Cloud APIC for Azure User Guide, Release 5.0(x)

Multi-Node Service Graph with NLB for Azure Cloud

Cisco ACI Multi-Site Orchestrator adds support for multi-node Service Graphs including Network Load Balancer for Azure Cloud.

For more information, see the Cisco Cloud APIC for Azure User Guide, Release 5.0(x).

Unmanaged third-party firewall support for Azure Cloud

Cisco ACI Multi-Site Orchestrator adds support for unmanaged third-party firewalls in Service Graphs.

For more information, see the Cisco Cloud APIC for Azure User Guide, Release 5.0(x).

Ability to Hide Shadow Objects

You can choose to show or hide the shadow objects created by the Multi-Site Orchestrator in the on-premises site's APIC GUI. Shadow objects in Cloud APIC are always hidden.

For more information, see the Cisco ACI Multi-Site Configuration Guide

Multiple concurrent remote locations for log streaming

You can send the Multi-Site Orchestrator logs to multiple external log analyzer tools in real time. By streaming any events as they are generated, you can use the additional tools to quickly parse, view, and respond to important events without a delay.

For more information, see Cisco ACI Multi-Site Configuration Guide, Release 3.0(x).

Concurrent configuration updates check

The Multi-Site Orchestrator GUI will ensure that any concurrent updates on the same Site, Tenant, or Schema object cannot unintentionally overwrite each other.

For more information, see Cisco ACI Multi-Site Configuration Guide, Release 3.0(x).

New Hardware Features

There is no new hardware supported in this release.

The complete list of supported hardware is available in the Cisco ACI Multi-Site Hardware Requirements Guide.

Changes in Behavior

If you are upgrading to this release, you will see the following changes in behavior:

■ For all new deployments, we recommend installing Multi-Site Orchestrator, Release 3.0(2d) or later in Application Services Engine.

Multi-Site Orchestrator, Release 3.0(2d) requires Application Services Engine, Release 1.1.3d.

■ If your Multi-Site Orchestrator is deployed in Application Services Engine and you are upgrading to Release 3.0(2), you must deploy a new Application Services Engine, Release 1.1.3d cluster and migrate your existing configuration.

The procedure is described in detail in Cisco ACI Multi-Site Orchestrator Installation and Upgrade Guide.

■ Any shadow objects created by the Multi-Site Orchestrator will now be automatically hidden in each site’s APIC GUI. You can choose to disable this feature in the APIC GUI if you want the objects to be visible.

■ Simultaneous updates to Sites, Tenants, and Schemas will no longer cause some changes to be lost.

■ Starting with Release 2.2(3), additional External EPG subnet flags have been exposed through the Multi-Site Orchestrator GUI.

Prior to Release 2.2(3), only the following subset of external EPG subnet flags available on each site’s APIC was managed by the Multi-Site Orchestrator:

— Shared Route Control—configurable in the Orchestrator GUI

— Shared Security Import—configurable in the Orchestrator GUI

— Aggregate Shared Routes—configurable in the Orchestrator GUI

— External Subnets for External EPG—not configurable in the GUI, but always implicitly enabled

Starting with Release 2.2(3), all subnet flags available from the APIC can be configured and managed from the Orchestrator:

— Export Route Control

— Import Route Control

— Shared Route Control

— Aggregate Shared Routes

— Aggregate Export (enabled for 0.0.0.0 subnet only)

— Aggregate Import (enabled for 0.0.0.0 subnet only)

— External Subnets for External EPG

— Shared Security Import

When upgrading to this release from Release 2.2(2) or earlier, any subnet flags previously unavailable in the Orchestrator GUI will be imported from the APIC and added to the Orchestrator configuration. All imported flags will retain their state (enabled or disabled) with the exception of External Subnets for External EPG, which will remain enabled post-upgrade. If you had previously explicitly disabled the External Subnets for External EPG flag directly in the APIC (for example, in Cloud APIC use case) you will need to disable it again through the Orchestrator GUI.

When downgrading from this release to Release 2.2(2) or earlier, the subnet flags not available in those releases will be cleared and set to disabled in the sites’ APICs. You can then manually enable them directly in each site’s APIC if necessary.

For additional information on these flags, see the Cisco ACI Multi-Site Configuration Guide.

■ When upgrading from a release prior to Release 2.2(1), a GUI lockout timer for repeated failed login attempts is automatically enabled by default and is set to 5 login attempts before a lockout with the lockout duration incremented exponentially every additional failed attempt.

■ If you configure read-only user roles in Release 2.1(2) or later and then choose to downgrade your Multi-Site Orchestrator to an earlier version where the read-only roles are not supported:

— You will need to reconfigure your external authentication servers to the old attribute-value (AV) pair string format. For details, see the "Administrative Operations" chapter in the Cisco ACI Multi-Site Configuration Guide.

— The read-only roles will be removed from all users. This also means that any user that has only the read-only roles will have no roles assigned to them and a Power User or User Manager will need to re-assign them new read-write roles.

■ Starting with Release 2.1(2), the 'phone number' field is no longer mandatory when creating a new Multi-Site Orchestrator user. However, because the field was required in prior releases, any user created in Release 2.1(2) or later without a phone number provided will be unable to log into the GUI if the Orchestrator is downgraded to Release 2.1(1) or earlier. In this case, a Power User or User Manager will need to provide a phone number for the user.

■ If you are upgrading from any release prior to Release 2.1(1), the default password and the minimum password requirements for the Multi-Site Orchestrator GUI have been updated. The default password has been changed from ‘We1come!” to “We1come2msc!” and the new password requirements are:

— At least 12 characters

— At least 1 letter

— At least 1 number

— At least 1 special character apart from * and space

You will be prompted to reset your passwords when you:

— First install Release 2.1(1)

— Upgrade to Release 2.1(1) from a release prior to Release 2.1(1)

— Restore the Multi-Site Orchestrator configuration from a backup

■ Starting with Release 2.1(1), Multi-Site Orchestrator encrypts all stored passwords, such as each site’s APIC passwords and the external authentication provider passwords. As a result, if you downgrade to any release prior to Release 2.1(1), you will need to re-enter all the passwords after the Orchestrator downgrade is completed.

To update APIC passwords:

a. Log in to the Orchestrator after the downgrade.

b. From the main navigation menu, select Sites.

c. For each site, edit its properties and re-enter its APIC password.

To update external authentication passwords:

a. Log in to the Orchestrator after the downgrade.

b. From the navigation menu, select Admin à Providers.

c. For each authentication provider, edit its properties and re-enter its password.

Open Issues

This section lists the open issues. Click the bug ID to access the Bug Search Tool and see additional information about the bug. The "Exists In" column of the table specifies the 3.0(2) releases in which the bug exists. A bug might also exist in releases other than the 3.0(2) releases.

Bug ID

Description

Exists in

CSCvv35532

"External Subnets for External EPG" is removed from L3Out subnets after an MSO template deploy.

3.0(2d)

CSCvv87834

CSRs retain tunnels to deleted sites as endpoint service is running two replicas.

All endpoints and tunnels notification will be duplicated, because every instance will send the same information to the same site, which will cause an increase and unnecessary load in all cAPIC sites registered.

Notification can arrive at different times, so it can trigger some misconfigurations if create/update/delay configurations are intertwined.

Different service instances can get different information from the MSO (as happened in latest bug found), which can create configuration and services to be created where they were not required.

If one instance is restarted, can trigger a full new setup of the system with different configuration.

3.0(2d)-3.0(2j)

CSCvo84218

When service graphs or devices are created on Cloud APIC by using the API and custom names are specified for AbsTermNodeProv and AbsTermNodeCons, a brownfield import to the Multi-Site Orchestrator will fail.

3.0(2d) and later

CSCvo20029

Contract is not created between shadow EPG and on-premises EPG when shared service is configured between Tenants.

3.0(2d) and later

CSCvq58349

shadow of extepg's vrf not getting updated.

3.0(2d) and later

CSCvn98355

Inter-site shared service between VRF instances across different tenants will not work, unless the tenant is stretched explicitly to the cloud site with the correct provider credentials. That is, there will be no implicit tenant stretch by Multi-Site Orchestrator.

3.0(2d) and later

CSCvr19577

If a template with empty AP (cloudApp without any cloudEpgs) is defined and it's undeployed, it deletes the cloudApp. If other templates are defined with same AP name and have cloudEpgs, then as a result of cloudApp deletion, all those cloudEpgs defined in other templates are also deleted.

3.0(2d) and later

CSCvr99291

Unable to take backup from MSO GUI.

3.0(2d) and later

CSCvs32126

Traffic may stop for EPGs stretched between on-premises and cloud sites.

3.0(2d) and later

CSCvs99052

Deployment window may show more policies been modified than the actual config changed by the user in the Schema.

3.0(2d) and later

CSCvt06351

Deployment window may not show all the service graph related config values that have been modified.

3.0(2d) and later

CSCvt00663

Deployment window may not show all the cloud related config values that have been modified.

3.0(2d) and later

CSCvs22418

Traffic is impacted when changing the VRF associated with BDs referred by PG enabled EPGs that have a global contract between them

3.0(2d) and later

CSCvt41911

After brownfield import, the BD subnets are present in site local and not in the common template config

3.0(2d) and later

CSCvt42771

MSO shows the L3Outs of another tenant when associating it with a BD.

3.0(2d) and later

CSCvt44081

In shared services use case, if one VRF has preferred group enabled EPGs and another VRF has vzAny contracts, traffic drop is seen.

3.0(2d) and later

CSCvt47568

Let's say APIC has EPGs with some contract relationships. If this EPG and the relationships are imported into MSO and then the relationship was removed and deployed to APIC, MSO doesn't delete the contract relationship on the APIC.

3.0(2d) and later

CSCvt47581

fvImportExtRoutes flag is created for VRF even though site1 & site3 external-epgs have provider contract.

3.0(2d) and later

CSCvt56139

When you try to upgrade MSO from 2.0(x) to 3.0(1), the upgrade script shows the following errors in the logs:

ERROR site 5e5eff4b120000892d98c2dd of templateSite (schema: 5e688b0c110000480b02b3f6 template: Template1) not found in schema!

ERROR schema not found for schemaId: 5e66c0911200004f2c6e542e

However, the upgrade completes correctly.

3.0(2d) and later

CSCvs71068

MSO-owned VRF exists on APIC when the owner Template on MSO is un-deployed.

3.0(2d) and later

CSCvt02480

The REST API call "/api/v1/execute/schema/5e43523f1100007b012b0fcd/template/Template_11?undeploy=all" can fail if the template being deployed has a large object count

3.0(2d) and later

CSCvt71692

If one template contains an application profile with some EPGs or an empty application profile and another another template with same application profile name with more EPGs, if you undeploy the first template then the EPGs in the second template also get undeployed.

3.0(2d) and later

CSCvt15312

Shared service traffic drops from external EPG to EPG in case of EPG provider and L3Out vzAny consumer

3.0(2d) and later

CSCvt11713

Intersite L3Out traffic is impacted because of missing import RT for VPN routes

3.0(2d) and later

CSCvt99784

Traffic between onPrem ExternalEPG (aka InstP) and the cloudEPG is disrupted due to a) the deletion of the shadow InstP created for the cloudEPG on the OnPrem Site and b) the deletion of cloudEPG's shadow VRF on the OnPrem Site.

3.0(2d) and later

CSCvu02398

When BD's subnets are changed, the cloud ExtEpg doesnt get the updated cloud extepselector.

3.0(2d) and later

CSCvt11713

Intersite L3Out traffic is impacted because of missing import RT for VPN routes

3.0(2d) and later

CSCvt08407

Schema/Template deployment fails when an already added cAPIC site with name-A and with IP address - IP-A is freshly brought up and different IP address is assigned to that cAPIC site.

3.0(2d) and later

CSCvu56069

Site deletion throws error: "Error from APIC: https://20.185.81.79, error: Invalid Configuration CT_EXTNETWORK_REQUIRES_EXTSUBNETPOOL: At present, there must be at least one cloudtemplateExtSubnetPool in cloudtemplateInfraNetwork uni/tn-infra/infranetwork-default when cloudtemplateExtNetwork is present; current count = 0 FORCE DELETE SITE"

3.0(2d) and later

CSCvu65041

Creating a contract between on-premises External EPG and stretched EPG impacts other on-premises L3out External EPG routes.

3.0(2d) and later

CSCvu88287

User deploys multinode graph from MSO on aws cAPIC site and the configuration can fail on AWS cAPIC and the expected service chaining won't work.

Also if AWS cAPIC (POST) allows Third Party FW creation and user selects that FW device for multinode graph in MSO, the configuration can fail on AWS cAPIC and the expected service chaining won't work.

3.0(2d) and later

CSCvu71584

Routes are not programmed on CSR and the contract config is not pushed to the Cloud site.

3.0(2d) and later

CSCvv00462

Unable to add the APIC site with different site ID if it was previously removed MSO.

3.0(2d) and later

CSCvv45150

When an admin desires to install a new image of the MSO firmware, this can be done via the firmware section. Images can be added either from a remote web server or from a local file system, e.g. a laptop. An upload of a local image will hang and not complete.

3.0(2d) and later

CSCvw61549

Unable to select the site local L3Out for a newly created BD from MSO.

3.0(2d) and later

CSCvt23491

Enhancement to add the ability in MSO to configure multiple DHCP relay polices for a BD.

3.0(2d) and later

Resolved Issues

This section lists the resolved issues. Click the bug ID to access the Bug Search tool and see additional information about the issue. The "Fixed In" column of the table specifies whether the bug was resolved in the base release or a patch release.

Bug ID

Description

Fixed in

CSCvt48924

MSO sending Remote Address tty10 to ISE

3.0(2d)

CSCvt41883

DB cleanup is not happening even after deleting the Tenant.

3.0(2d)

CSCvu15073

APIC rejects the deployment from MSO on a cloud APIC site with error: "Following CtxProfiles are associated with this VRF: <ctxprofile-dn> Delete all the CtxProfiles associated with this VRF, before deleting this VRF"

3.0(2d)

CSCvu26874

HTTPs listener configuration for Load Balancer doesn't work from MSO for Azure cloud site and MSO may throw error when trying to save schema with HTTPs listener configuration.

3.0(2d)

CSCvu26941

Unable to configure BGP Remote Peer Address when BGP-Label Unicast Source IPv4 address is a /31 mask on a SR-MPLS BL/RL node.

3.0(2d)

CSCvu35959

Pushing new config to APIC via MSO can cause "Aggregate Shared Routes" flag to get overwritten.

3.0(2j)

CSCvu98786

Can't update stretched template in MSO

3.0(2j)

CSCvt93884

Un-expected shadow external EPGs are created and peer context is not correct.

3.0(2j)

CSCvu70270

Unable to deploy changes nor select an L3Out for ExtEPG after the upgrade

3.0(2j)

CSCvu63706

Updating and deploying the same schema from two different browsers shows successful completion instead of throwing an error stating that concurrent deployment was prevented.

3.0(2j)

CSCvv11608

After successfully deploying Templates that have EPGs and/or BDs, no config drift is shown. After a few seconds, config drift shows up as EPG/BD modified. On comparing the sites with MSO config, no meaningful diff is found.

3.0(2j)

CSCvv05433

Unable to un-deploy L3Out, removal of L3Out from a template is blocked with "Bad Request: External EPGs from different template/schema is associated with..." message.

3.0(2j)

CSCvv35532

"External Subnets for External EPG" is removed from L3Out subnets after an MSO template deploy.

3.0(2j)

CSCvv87834

CSRs retain tunnels to deleted sites as endpoint service is running two replicas.

All endpoints and tunnels notification will be duplicated, because every instance will send the same information to the same site, which will cause an increase and unnecessary load in all cAPIC sites registered.

Notification can arrive at different times, so it can trigger some misconfigurations if create/update/delay configurations are intertwined.

Different service instances can get different information from the MSO (as happened in latest bug found), which can create configuration and services to be created where they were not required.

If one instance is restarted, can trigger a full new setup of the system with different configuration.

3.0(2k)

Known Issues

This section lists known behaviors. Click the Bug ID to access the Bug Search Tool and see additional information about the issue.

Bug ID

Description

CSCvo82001

Unable to download Multi-Site Orchestrator report and debug logs when database and server logs are selected

CSCvo32313

Unicast traffic flow between Remote Leaf Site1 and Remote Leaf in Site2 may be enabled by default. This feature is not officially supported in this release.

CSCvn38255

After downgrading from 2.1(1), preferred group traffic continues to work. You must disable the preferred group feature before downgrading to an earlier release.

CSCvn90706

No validation is available for shared services scenarios

CSCvo59133

The upstream server may time out when enabling audit log streaming

CSCvd59276

For Cisco ACI Multi-Site, Fabric IDs Must be the Same for All Sites, or the Querier IP address Must be Higher on One Site.

The Cisco APIC fabric querier functions have a distributed architecture, where each leaf switch acts as a querier, and packets are flooded. A copy is also replicated to the fabric port. There is an Access Control List (ACL) configured on each TOR to drop this query packet coming from the fabric port. If the source MAC address is the fabric MAC address, unique per fabric, then the MAC address is derived from the fabric-id. The fabric ID is configured by users during initial bring up of a pod site.

In the Cisco ACI Multi-Site Stretched BD with Layer 2 Broadcast Extension use case, the query packets from each TOR get to the other sites and should be dropped. If the fabric-id is configured differently on the sites, it is not possible to drop them.

To avoid this, configure the fabric IDs the same on each site, or the querier IP address on one of the sites should be higher than on the other sites.

CSCvd61787

STP and "Flood in Encapsulation" Option are not Supported with Cisco ACI Multi-Site.

In Cisco ACI Multi-Site topologies, regardless of whether EPGs are stretched between sites or localized, STP packets do not reach remote sites. Similarly, the "Flood in Encapsulation" option is not supported across sites. In both cases, packets are encapsulated using an FD VNID (fab-encap) of the access VLAN on the ingress TOR. It is a known issue that there is no capability to translate these IDs on the remote sites.

CSCvi61260

If an infra L3Out that is being managed by Cisco ACI Multi-Site is modified locally in a Cisco APIC, Cisco ACI Multi-Site might delete the objects not managed by Cisco ACI Multi-Site in an L3Out.

CSCvq07769

"Phone Number" field is required in all releases prior to Release 2.2(1). Users with no phone number specified in Release 2.2(1) or later will not be able to log in to the GUI when Orchestrator is downgraded to a an earlier release.

Usage Guidelines

This section lists usage guidelines for the Cisco ACI Multi-Site software.

■ For all new deployments, we recommend installing Multi-Site Orchestrator, Release 3.0(2d) or later in Application Services Engine.

Multi-Site Orchestrator, Release 3.0(2d) requires Application Services Engine, Release 1.1.3d.

■ If your Multi-Site Orchestrator is deployed in Application Services Engine and you are upgrading to Release 3.0(2), you must deploy a new Application Services Engine, Release 1.1.3d cluster and migrate your existing configuration.

The procedure is described in detail in Cisco ACI Multi-Site Orchestrator Installation and Upgrade Guide.

■ In Cisco ACI Multi-Site topologies, we recommend that First Hop Routing protocols such as HSRP/VRRP are not stretched across sites.

■ HTTP requests are redirected to HTTPS and there is no HTTP support globally or per user basis.

■ Up to 12 interconnected sites are supported.

■ Proxy ARP glean and unknown unicast flooding are not supported together.

Unknown Unicast Flooding and ARP Glean are not supported together in Cisco ACI Multi-Site across sites.

■ Flood in encapsulation is not supported for EPGs and Bridge Domains that are extended across ACI fabrics that are part of the same Multi-Site domain. However, flood in encapsulation is fully supported for EPGs or Bridge Domains that are locally defined in ACI fabrics, even if those fabrics may be configured for Multi-Site.

■ The leaf and spine nodes that are part of an ACI fabric do not run Spanning Tree Protocol (STP). STP frames originated from external devices can be forwarded across an ACI fabric (both single Pod and Multi-Pod), but are not forwarded across the inter-site network between sites, even if stretching a BD with BUM traffic enabled.

■ GOLF L3Outs for each tenant must be dedicated, not shared.

The inter-site L3Out functionality introduced on MSO release 2.2(1) does not apply when deploying GOLF L3Outs. This means that for a given VRF there is still the requirement of deploying at least one GOLF L3Out per site in order to enable north-south communication. An endpoint connected in a site cannot communicate with resources reachable via a GOLF L3Out connection deployed in a different site.

■ While you can create the L3Out objects in the Multi-Site Orchestrator GUI, the physical L3Out configuration (logical nodes, logical interfaces, and so on) must be done directly in each site's APIC.

■ VMM and physical domains must be configured in the Cisco APIC GUI at the site and will be imported and associated within the Cisco ACI Multi-Site.

Although domains (VMM and physical) must be configured in Cisco APIC, domain associations can be configured in the Cisco APIC or Cisco ACI Multi-Site.

■ Some VMM domain options must be configured in the Cisco APIC GUI.

The following VMM domain options must be configured in the Cisco APIC GUI at the site:

— NetFlow/EPG CoS marking in a VMM domain association

— Encapsulation mode for an AVS VMM domain

■ Some uSeg EPG attribute options must be configured in the Cisco APIC GUI.

The following uSeg EPG attribute options must be configured in the Cisco APIC GUI at the site:

— Sub-criteria under uSeg attributes

— match-all and match-any criteria under uSeg attributes

■ Site IDs must be unique.

In Cisco ACI Multi-Site, site IDs must be unique.

■ To change a Cisco APIC fabric ID, you must erase and reconfigure the fabric.

Cisco APIC fabric IDs cannot be changed. To change a Cisco APIC fabric ID, you must erase the fabric configuration and reconfigure it.

However, Cisco ACI Multi-Site supports connecting multiple fabrics with the same fabric ID.

■ Caution: When removing a spine switch port from the Cisco ACI Multi-Site infrastructure, perform the following steps:

a. Click Sites.

b. Click Configure Infra.

c. Click the site where the spine switch is located.

d. Click the spine switch.

e. Click the x on the port details.

f. Click Apply.

■ Shared services use case: order of importing tenant policies

When deploying a provider site group and a consumer site group for shared services by importing tenant policies, deploy the provider tenant policies before deploying the consumer tenant policies. This enables the relation of the consumer tenant to the provider tenant to be properly formed.

■ Caution for shared services use case when importing a tenant and stretching it to other sites

When you import the policies for a consumer tenant and deploy them to multiple sites, including the site where they originated, a new contract is deployed with the same name (different because it is modified by the inter-site relation). To avoid confusion, delete the original contract with the same name on the local site. In the Cisco APIC GUI, the original contract can be distinguished from the contract that is managed by Cisco ACI Multi-Site, because it is not marked with a cloud icon.

■ When a contract is established between EPGs in different sites, each EPG and its bridge domain (BD) are mirrored to and appear to be deployed in the other site, while only being actually deployed in its own site. These mirrored objects are known as "shadow” EPGs and BDs.

For example, if one EPG in Site 1 and another EPG in Site 2 have a contract between them, in the Cisco APIC GUI at Site 1 and Site 2, you will see both EPGs. They appear with the same names as the ones that were deployed directly to each site. This is expected behavior and the shadow objects must not be removed.

For more information, see the Schema Management chapter in the Cisco ACI Multi-Site Configuration Guide.

■ Inter-site traffic cannot transit sites.

Site traffic cannot transit sites on the way to another site. For example, when Site 1 routes traffic to Site 3, it cannot be forwarded through Site 2.

■ The ? icon in Cisco ACI Multi-Site opens the menu for Show Me How modules, which provide step-by-step help through specific configurations.

— If you deviate while in progress of a Show Me How module, you will no longer be able to continue.

— You must have IPv4 enabled to use the Show Me How modules.

■ User passwords must meet the following criteria:

— Minimum length is 8 characters

— Maximum length is 64 characters

— Fewer than three consecutive repeated characters

— At least three of the following character types: lowercase, uppercase, digit, symbol

— Cannot be easily guessed

— Cannot be the username or the reverse of the username

— Cannot be any variation of "cisco", "isco", or any permutation of these characters or variants obtained by changing the capitalization of letters therein

■ If you are associating a contract with the external EPG, as provider, choose contracts only from the tenant associated with the external EPG. Do not choose contracts from other tenants. If you are associating the contract to the external EPG, as consumer, you can choose any available contract.

■ Policy objects deployed from ACI Multi-Site software should not be modified or deleted from any site-APIC. If any such operation is performed, schemas have to be re-deployed from ACI Multi-Site software.

■ The Rogue Endpoint feature can be used within each site of an ACI Multi-Site deployment to help with misconfigurations of servers that cause an endpoint to move within the site. The Rogue Endpoint feaure is not designed for scenarios where the endpoint may move between sites.

Compatibility

This release supports the hardware listed in the Cisco ACI Multi-Site Hardware Requirements Guide.

Multi-Site Orchestrator releases have been decoupled from the APIC releases. The APIC clusters in each site as well as the Orchestrator itself can now be upgraded independently of each other and run in mixed operation mode. For more information, see the Interoperability Support section in the “Infrastructure Management” chapter of the Cisco ACI Multi-Site Orchestrator Installation and Upgrade Guide.

Scalability

For the verified scalability limits, see the Cisco ACI Verified Scalability Guide.

Related ContentNOTE: Available paragraph styles are listed in the Quick Styles Gallery in the Styles group on the Home tab. Alternatively, they can be accessed via the Styles window (press Alt + Ctrl + Shift + S).

See the Cisco Application Policy Infrastructure Controller (APIC) page for ACI Multi-Site documentation. On that page, you can use the "Choose a topic" and "Choose a document type" fields to narrow down the displayed documentation list and find a desired document.

The documentation includes installation, upgrade, configuration, programming, and troubleshooting guides, technical references, release notes, and knowledge base (KB) articles, and videos. KB articles provide information about a specific use cases or topics. The following tables describe the core Cisco Application Centric Infrastructure Multi-Site documentation.

Document

Description

Cisco ACI Multi-Site Release Notes

This document. Provides release information for the Cisco ACI Multi-Site Orchestrator product.

Cisco ACI Multi-Site Fundamentals Guide

Provides basic concepts and capabilities of the Cisco ACI Multi-Site.

Cisco ACI Multi-Site Hardware Requirements Guide

Provides the hardware requirements and compatibility.

Cisco ACI Multi-Site Installation and Upgrade Guide

Describes how to install Cisco ACI Multi-Site Orchestrator and perform day-0 operations.

Cisco ACI Multi-Site Configuration Guide

Describes Cisco ACI Multi-Site configuration options and procedures.

Cisco ACI Multi-Site REST API Configuration Guide

Describes how to use the Cisco ACI Multi-Site REST APIs.

Cisco ACI Multi-Site Troubleshooting Guide

Provides descriptions of common operations issues and Describes how to troubleshoot common Cisco ACI Multi-Site issues.

Cisco ACI Verified Scalability

Contains the maximum verified scalability limits for Cisco Application Centric Infrastructure (Cisco ACI), including Cisco ACI Multi-Site.

Cisco ACI YouTube channel

Contains videos that demonstrate how to perform specific tasks in the Cisco ACI Multi-Site.


Cisco ACI Multi-Site Orchestrator Release Notes, Release 3.0(1)

This document describes the features, issues, and deployment guidelines for the Cisco Application Centric Infrastructure (ACI) Multi-Site Orchestrator software.

Cisco ACI Multi-Site is an architecture that allows you to interconnect separate Cisco APIC cluster domains (fabrics), each representing a different region. This helps ensure multitenant Layer 2 and Layer 3 network connectivity across sites and extends the policy domain end-to-end across the entire system.

Cisco ACI Multi-Site Orchestrator is the intersite policy manager. It provides single-pane management that enables you to monitor the health of all the interconnected sites. It also allows you to centrally define the intersite policies that can then be pushed to the different Cisco APIC fabrics, which in term deploys them on the physical switches that make up those fabrics. This provides a high degree of control over when and where to deploy those policies.

For more information, see Related Content.

Date

Description

October 21, 2021

Additional open issue CSCvt23491.

December 2, 2020

Additional open issue CSCvw61549.

August 17, 2020

Additional open issue CSCvv35532.

May 14, 2020

Release 3.0(1i) became available.

New Software Features

This release adds the following new features:

Feature

Description

Support for SR-MPLS handoff

Prior to Cisco ACI Multi-Site Orchestrator release 3.0(1), Multi-Site architecture supported only sites connected via IP handoff, otherwise known as VRF-Lite. Beginning with Cisco ACI Multi-Site release 3.0(1), you can use the Multi-Site Orchestrator to also manage ACI fabrics that are connected via segment routing (SR) Multiprotocol Label Switching (MPLS) handoff.

For details about SR-MPLS handoff design, policy model, and implementation, see Cisco APIC Layer 3 Networking Configuration Guide, Release 5.0(x). For MSO SR-MPLS configuration, see Cisco ACI Multi-Site Configuration Guide, Release 3.0(x).

Support for SR-MPLS custom QoS policies

Beginning with Cisco ACI Multi-Site release 3.0(1), you can create custom QoS translation policies for traffic coming into or leaving the ACI fabric via an SR-MPLS interface.

For more information, see Cisco ACI Multi-Site Configuration Guide, Release 3.0(x).

Support for multicast filtering

Beginning with Cisco ACI Multi-Site release 3.0(1), you can create custom multicast route map policies to enable source or destination filtering for multicast traffic for all EPGs within a bridge domain.

For more information, see Cisco ACI Multi-Site Configuration Guide, Release 3.0(x).

Support for Rendezvous Points (RPs)

Beginning with Cisco ACI Multi-Site release 3.0(1), you can use the Multi-Site Orchestrator GUI to add multicast rendezvous points (RPs) for multicast-enabled VRFs. The RPs can be inside or outside the ACI fabric and you can create custom route map policies if you want to limit the RPs to specific multicast groups only.

For more information, see Cisco ACI Multi-Site Configuration Guide, Release 3.0(x).

Support for Transit Gateway (TWG) for Cloud APIC sites in AWS

Beginning with Cisco ACI Multi-Site release 3.0(1), you can use Amazon Web Services (AWS) Transit Gateway with Cisco Cloud APIC to automate connectivity between virtual private clouds (VPCs).

For more information, see Increasing Bandwidth Between VPCs by Using AWS Transit Gateway.

MSO GUI enhancements

The Multi-Site Orchestrator Graphical User Interface (GUI) has been redesigned for consistency across Cisco ACI products, improved user experience, and simplified configuration workflows.

New Hardware Features

There is no new hardware supported in this release.

The complete list of supported hardware is available in the Cisco ACI Multi-Site Hardware Requirements Guide.

Changes in Behavior

If you are upgrading to this release, you will see the following changes in behavior:

■ Starting with Release 2.2(3), additional External EPG subnet flags have been exposed through the Multi-Site Orchestrator GUI.

Prior to Release 2.2(3), only the following subset of external EPG subnet flags available on each site’s APIC was managed by the Multi-Site Orchestrator:

— Shared Route Control—configurable in the Orchestrator GUI

— Shared Security Import—configurable in the Orchestrator GUI

— Aggregate Shared Routes—configurable in the Orchestrator GUI

— External Subnets for External EPG—not configurable in the GUI, but always implicitly enabled

Starting with Release 2.2(3), all subnet flags available from the APIC can be configured and managed from the Orchestrator:

— Export Route Control

— Import Route Control

— Shared Route Control

— Aggregate Shared Routes

— Aggregate Export (enabled for 0.0.0.0 subnet only)

— Aggregate Import (enabled for 0.0.0.0 subnet only)

— External Subnets for External EPG

— Shared Security Import

When upgrading to this release from Release 2.2(2) or earlier, any subnet flags previously unavailable in the Orchestrator GUI will be imported from the APIC and added to the Orchestrator configuration. All imported flags will retain their state (enabled or disabled) with the exception of External Subnets for External EPG, which will remain enabled post-upgrade. If you had previously explicitly disabled the External Subnets for External EPG flag directly in the APIC (for example, in Cloud APIC use case) you will need to disable it again through the Orchestrator GUI.

When downgrading from this release to Release 2.2(2) or earlier, the subnet flags not available in those releases will be cleared and set to disabled in the sites’ APICs. You can then manually enable them directly in each site’s APIC if necessary.

For additional information on these flags, see Cisco ACI Multi-Site Configuration Guide.

■ When upgrading from a release prior to Release 2.2(1), a GUI lockout timer for repeated failed login attempts is automatically enabled by default and is set to 5 login attempts before a lockout with the lockout duration incremented exponentially every additional failed attempt.

■ If you configure read-only user roles in Release 2.1(2) or later and then choose to downgrade your Multi-Site Orchestrator to an earlier version where the read-only roles are not supported:

— You will need to reconfigure your external authentication servers to the old attribute-value (AV) pair string format. For details, see the "Administrative Operations" chapter in the Cisco ACI Multi-Site Configuration Guide.

— The read-only roles will be removed from all users. This also means that any user that has only the read-only roles will have no roles assigned to them and a Power User or User Manager will need to re-assign them new read-write roles.

■ Starting with Release 2.1(2), the 'phone number' field is no longer mandatory when creating a new Multi-Site Orchestrator user. However, because the field was required in prior releases, any user created in Release 2.1(2) or later without a phone number provided will be unable to log into the GUI if the Orchestrator is downgraded to Release 2.1(1) or earlier. In this case, a Power User or User Manager will need to provide a phone number for the user.

■ If you are upgrading from any release prior to Release 2.1(1), the default password and the minimum password requirements for the Multi-Site Orchestrator GUI have been updated. The default password has been changed from ‘We1come!” to “We1come2msc!” and the new password requirements are:

— At least 12 characters

— At least 1 letter

— At least 1 number

— At least 1 special character apart from * and space

You will be prompted to reset your passwords when you:

— First install Release 2.1(x)

— Upgrade to Release 2.1(x) from a release prior to Release 2.1(1)

— Restore the Multi-Site Orchestrator configuration from a backup

■ Starting with Release 2.1(1), Multi-Site Orchestrator encrypts all stored passwords, such as each site’s APIC passwords and the external authentication provider passwords. As a result, if you downgrade to any release prior to Release 2.1(1), you will need to re-enter all the passwords after the Orchestrator downgrade is completed.

To update APIC passwords:

a. Log in to the Orchestrator after the downgrade.

b. From the main navigation menu, select Sites.

c. For each site, edit its properties and re-enter its APIC password.

To update external authentication passwords:

a. Log in to the Orchestrator after the downgrade.

b. From the navigation menu, select Admin à Providers.

c. For each authentication provider, edit its properties and re-enter its password.

Open Issues

This section lists the open issues. Click the bug ID to access the Bug Search Tool and see additional information about the bug. The "Exists In" column of the table specifies the 3.0(1) releases in which the bug exists. A bug might also exist in releases other than the 3.0(1) releases.

Bug ID

Description

Exists in

CSCvt48924

MSO sending Remote Address tty10 to ISE

3.0(1i) and later

CSCvo84218

When service graphs or devices are created on Cloud APIC by using the API and custom names are specified for AbsTermNodeProv and AbsTermNodeCons, a brownfield import to the Multi-Site Orchestrator will fail.

3.0(1i) and later

CSCvo20029

Contract is not created between shadow EPG and on-premises EPG when shared service is configured between Tenants.

3.0(1i) and later

CSCvq58349

shadow of extepg's vrf not getting updated.

3.0(1i) and later

CSCvn98355

Inter-site shared service between VRF instances across different tenants will not work, unless the tenant is stretched explicitly to the cloud site with the correct provider credentials. That is, there will be no implicit tenant stretch by Multi-Site Orchestrator.

3.0(1i) and later

CSCvr19577

If a template with empty AP (cloudApp without any cloudEpgs) is defined and it's undeployed, it deletes the cloudApp. If other templates are defined with same AP name and have cloudEpgs, then as a result of cloudApp deletion, all those cloudEpgs defined in other templates are also deleted.

3.0(1i) and later

CSCvr99291

Unable to take backup from MSO GUI.

3.0(1i) and later

CSCvs32126

Traffic may stop for EPGs stretched between on-premises and cloud sites.

3.0(1i) and later

CSCvs99052

Deployment window may show more policies been modified than the actual config changed by the user in the Schema.

3.0(1i) and later

CSCvt06351

Deployment window may not show all the service graph related config values that have been modified.

3.0(1i) and later

CSCvt00663

Deployment window may not show all the cloud related config values that have been modified.

3.0(1i) and later

CSCvs22418

Traffic is impacted when changing the VRF associated with BDs referred by PG enabled EPGs that have a global contract between them

3.0(1i) and later

CSCvt41883

DB cleanup is not happening even after deleting the Tenant.

3.0(1i) and later

CSCvt41911

After brownfield import, the BD subnets are present in site local and not in the common template config

3.0(1i) and later

CSCvt42771

MSO shows the L3Outs of another tenant when associating it with a BD.

3.0(1i) and later

CSCvt44081

In shared services use case, if one VRF has preferred group enabled EPGs and another VRF has vzAny contracts, traffic drop is seen.

3.0(1i) and later

CSCvt47568

Let's say APIC has EPGs with some contract relationships. If this EPG and the relationships are imported into MSO and then the relationship was removed and deployed to APIC, MSO doesn't delete the contract relationship on the APIC.

3.0(1i) and later

CSCvt47581

fvImportExtRoutes flag is created for VRF even though site1 & site3 external-epgs have provider contract.

3.0(1i) and later

CSCvt56139

When you try to upgrade MSO from 2.0(x) to 3.0(1), the upgrade script shows the following errors in the logs:

ERROR site 5e5eff4b120000892d98c2dd of templateSite (schema: 5e688b0c110000480b02b3f6 template: Template1) not found in schema!

ERROR schema not found for schemaId: 5e66c0911200004f2c6e542e

However, the upgrade completes correctly.

3.0(1i) and later

CSCvs71068

MSO-owned VRF exists on APIC when the owner Template on MSO is un-deployed.

3.0(1i) and later

CSCvt02480

The REST API call "/api/v1/execute/schema/5e43523f1100007b012b0fcd/template/Template_11?undeploy=all" can fail if the template being deployed has a large object count

3.0(1i) and later

CSCvt71692

If one template contains an application profile with some EPGs or an empty application profile and another another template with same application profile name with more EPGs, if you undeploy the first template then the EPGs in the second template also get undeployed.

3.0(1i) and later

CSCvt15312

Shared service traffic drops from external EPG to EPG in case of EPG provider and L3Out vzAny consumer

3.0(1i) and later

CSCvt11713

Intersite L3Out traffic is impacted because of missing import RT for VPN routes

3.0(1i) and later

CSCvt99784

Traffic between onPrem ExternalEPG (aka InstP) and the cloudEPG is disrupted due to a) the deletion of the shadow InstP created for the cloudEPG on the OnPrem Site and b) the deletion of cloudEPG's shadow VRF on the OnPrem Site.

3.0(1i) and later

CSCvu02398

When BD's subnets are changed, the cloud ExtEpg doesnt get the updated cloud extepselector.

3.0(1i) and later

CSCvt11713

Intersite L3Out traffic is impacted because of missing import RT for VPN routes

3.0(1i) and later

CSCvu15073

APIC rejects the deployment from MSO on a cloud APIC site with error: "Following CtxProfiles are associated with this VRF: <ctxprofile-dn> Delete all the CtxProfiles associated with this VRF, before deleting this VRF"

3.0(1i) and later

CSCvu26874

HTTPs listener configuration for Load Balancer doesn't work from MSO for Azure cloud site and MSO may throw error when trying to save schema with HTTPs listener configuration.

3.0(1i) and later

CSCvu26941

Unable to configure BGP Remote Peer Address when BGP-Label Unicast Source IPv4 address is a /31 mask on a SR-MPLS BL/RL node.

3.0(1i) and later

CSCvv35532

"External Subnets for External EPG" is removed from L3Out subnets after an MSO template deploy.

3.0(1i) and later

CSCvw61549

Unable to select the site local L3Out for a newly created BD from MSO.

3.0(1i) and later

CSCvt23491

Enhancement to add the ability in MSO to configure multiple DHCP relay polices for a BD.

3.0(1i) and later

Resolved Issues

This section lists the resolved issues. Click the bug ID to access the Bug Search tool and see additional information about the issue. The "Fixed In" column of the table specifies whether the bug was resolved in the base release or a patch release.

Bug ID

Description

Fixed in

CSCvt39879

Preferred group gets disabled on the APIC VRF when deployed from MSO.

3.0(1i) and later

CSCvq79052

Updating TEP pool may cause a validation error.

3.0(1i) and later

CSCvs31527

Object import from Cloud APIC doesn't show the forward rule info and unable to save new rules.

3.0(1i) and later

CSCvt89710

If a DHCP policy is associated to two bridge domains in different templates but in the same schema, then if you make a change in the DHCP policy and come to schema, only 1 template becomes deployable, i.e. the deploy button gets enabled.

3.0(1i) and later

CSCvt91895

Schema save fails intermittently when L3Out is in one template and external EPG is in another template.

3.0(1i) and later

Known Issues

This section lists known behaviors. Click the Bug ID to access the Bug Search Tool and see additional information about the issue.

Bug ID

Description

CSCvo82001

Unable to download Multi-Site Orchestrator report and debug logs when database and server logs are selected

CSCvo32313

Unicast traffic flow between Remote Leaf Site1 and Remote Leaf in Site2 may be enabled by default. This feature is not officially supported in this release.

CSCvn38255

After downgrading from 2.1(1), preferred group traffic continues to work. You must disable the preferred group feature before downgrading to an earlier release.

CSCvn90706

No validation is available for shared services scenarios

CSCvo59133

The upstream server may time out when enabling audit log streaming

CSCvd59276

For Cisco ACI Multi-Site, Fabric IDs Must be the Same for All Sites, or the Querier IP address Must be Higher on One Site.

The Cisco APIC fabric querier functions have a distributed architecture, where each leaf switch acts as a querier, and packets are flooded. A copy is also replicated to the fabric port. There is an Access Control List (ACL) configured on each TOR to drop this query packet coming from the fabric port. If the source MAC address is the fabric MAC address, unique per fabric, then the MAC address is derived from the fabric-id. The fabric ID is configured by users during initial bring up of a pod site.

In the Cisco ACI Multi-Site Stretched BD with Layer 2 Broadcast Extension use case, the query packets from each TOR get to the other sites and should be dropped. If the fabric-id is configured differently on the sites, it is not possible to drop them.

To avoid this, configure the fabric IDs the same on each site, or the querier IP address on one of the sites should be higher than on the other sites.

CSCvd61787

STP and "Flood in Encapsulation" Option are not Supported with Cisco ACI Multi-Site.

In Cisco ACI Multi-Site topologies, regardless of whether EPGs are stretched between sites or localized, STP packets do not reach remote sites. Similarly, the "Flood in Encapsulation" option is not supported across sites. In both cases, packets are encapsulated using an FD VNID (fab-encap) of the access VLAN on the ingress TOR. It is a known issue that there is no capability to translate these IDs on the remote sites.

CSCvi61260

If an infra L3Out that is being managed by Cisco ACI Multi-Site is modified locally in a Cisco APIC, Cisco ACI Multi-Site might delete the objects not managed by Cisco ACI Multi-Site in an L3Out.

CSCvq07769

"Phone Number" field is required in all releases prior to Release 2.2(1). Users with no phone number specified in Release 2.2(1) or later will not be able to log in to the GUI when Orchestrator is downgraded to a an earlier release.

Usage Guidelines

This section lists usage guidelines for the Cisco ACI Multi-Site software.

■ In Cisco ACI Multi-Site topologies, we recommend that First Hop Routing protocols such as HSRP/VRRP are not stretched across sites.

■ HTTP requests are redirected to HTTPS and there is no HTTP support globally or per user basis.

■ Up to 12 interconnected sites are supported.

■ Proxy ARP glean and unknown unicast flooding are not supported together.

Unknown Unicast Flooding and ARP Glean are not supported together in Cisco ACI Multi-Site across sites.

■ Flood in encapsulation is not supported for EPGs and Bridge Domains that are extended across ACI fabrics that are part of the same Multi-Site domain. However, flood in encapsulation is fully supported for EPGs or Bridge Domains that are locally defined in ACI fabrics, even if those fabrics may be configured for Multi-Site.

■ The leaf and spine nodes that are part of an ACI fabric do not run Spanning Tree Protocol (STP). STP frames originated from external devices can be forwarded across an ACI fabric (both single Pod and Multi-Pod), but are not forwarded across the inter-site network between sites, even if stretching a BD with BUM traffic enabled.

■ GOLF L3Outs for each tenant must be dedicated, not shared.

The inter-site L3Out functionality introduced on MSO release 2.2(1) does not apply when deploying GOLF L3Outs. This means that for a given VRF there is still the requirement of deploying at least one GOLF L3Out per site in order to enable north-south communication. An endpoint connected in a site cannot communicate with resources reachable via a GOLF L3Out connection deployed in a different site.

■ While you can create the L3Out objects in the Multi-Site Orchestrator GUI, the physical L3Out configuration (logical nodes, logical interfaces, and so on) must be done directly in each site's APIC.

■ VMM and physical domains must be configured in the Cisco APIC GUI at the site and will be imported and associated within the Cisco ACI Multi-Site.

Although domains (VMM and physical) must be configured in Cisco APIC, domain associations can be configured in the Cisco APIC or Cisco ACI Multi-Site.

■ Some VMM domain options must be configured in the Cisco APIC GUI.

The following VMM domain options must be configured in the Cisco APIC GUI at the site:

— NetFlow/EPG CoS marking in a VMM domain association

— Encapsulation mode for an AVS VMM domain

■ Some uSeg EPG attribute options must be configured in the Cisco APIC GUI.

The following uSeg EPG attribute options must be configured in the Cisco APIC GUI at the site:

— Sub-criteria under uSeg attributes

— match-all and match-any criteria under uSeg attributes

■ Site IDs must be unique.

In Cisco ACI Multi-Site, site IDs must be unique.

■ To change a Cisco APIC fabric ID, you must erase and reconfigure the fabric.

Cisco APIC fabric IDs cannot be changed. To change a Cisco APIC fabric ID, you must erase the fabric configuration and reconfigure it.

However, Cisco ACI Multi-Site supports connecting multiple fabrics with the same fabric ID.

■ Caution: When removing a spine switch port from the Cisco ACI Multi-Site infrastructure, perform the following steps:

a. Click Sites.

b. Click Configure Infra.

c. Click the site where the spine switch is located.

d. Click the spine switch.

e. Click the x on the port details.

f. Click Apply.

■ Shared services use case: order of importing tenant policies

When deploying a provider site group and a consumer site group for shared services by importing tenant policies, deploy the provider tenant policies before deploying the consumer tenant policies. This enables the relation of the consumer tenant to the provider tenant to be properly formed.

■ Caution for shared services use case when importing a tenant and stretching it to other sites

When you import the policies for a consumer tenant and deploy them to multiple sites, including the site where they originated, a new contract is deployed with the same name (different because it is modified by the inter-site relation). To avoid confusion, delete the original contract with the same name on the local site. In the Cisco APIC GUI, the original contract can be distinguished from the contract that is managed by Cisco ACI Multi-Site, because it is not marked with a cloud icon.

■ When a contract is established between EPGs in different sites, each EPG and its bridge domain (BD) are mirrored to and appear to be deployed in the other site, while only being actually deployed in its own site. These mirrored objects are known as "shadow” EPGs and BDs.

For example, if one EPG in Site 1 and another EPG in Site 2 have a contract between them, in the Cisco APIC GUI at Site 1 and Site 2, you will see both EPGs. They appear with the same names as the ones that were deployed directly to each site. This is expected behavior and the shadow objects must not be removed.

For more information, see the Schema Management chapter in the Cisco ACI Multi-Site Configuration Guide.

■ Inter-site traffic cannot transit sites.

Site traffic cannot transit sites on the way to another site. For example, when Site 1 routes traffic to Site 3, it cannot be forwarded through Site 2.

■ The ? icon in Cisco ACI Multi-Site opens the menu for Show Me How modules, which provide step-by-step help through specific configurations.

— If you deviate while in progress of a Show Me How module, you will no longer be able to continue.

— You must have IPv4 enabled to use the Show Me How modules.

■ User passwords must meet the following criteria:

— Minimum length is 8 characters

— Maximum length is 64 characters

— Fewer than three consecutive repeated characters

— At least three of the following character types: lowercase, uppercase, digit, symbol

— Cannot be easily guessed

— Cannot be the username or the reverse of the username

— Cannot be any variation of "cisco", "isco", or any permutation of these characters or variants obtained by changing the capitalization of letters therein

■ If you are associating a contract with the external EPG, as provider, choose contracts only from the tenant associated with the external EPG. Do not choose contracts from other tenants. If you are associating the contract to the external EPG, as consumer, you can choose any available contract.

■ Policy objects deployed from ACI Multi-Site software should not be modified or deleted from any site-APIC. If any such operation is performed, schemas have to be re-deployed from ACI Multi-Site software.

■ The Rogue Endpoint feature can be used within each site of an ACI Multi-Site deployment to help with misconfigurations of servers that cause an endpoint to move within the site. The Rogue Endpoint feaure is not designed for scenarios where the endpoint may move between sites.

Compatibility

This release supports the hardware listed in the Cisco ACI Multi-Site Hardware Requirements Guide.

Multi-Site Orchestrator releases have been decoupled from the APIC releases. The APIC clusters in each site as well as the Orchestrator itself can now be upgraded independently of each other and run in mixed operation mode. For more information, see the Interoperability Support section in the “Infrastructure Management” chapter of the Cisco ACI Multi-Site Orchestrator Installation and Upgrade Guide.

Scalability

For the verified scalability limits, see the Cisco ACI Verified Scalability Guide.

Related ContentNOTE: Available paragraph styles are listed in the Quick Styles Gallery in the Styles group on the Home tab. Alternatively, they can be accessed via the Styles window (press Alt + Ctrl + Shift + S).

See the Cisco Application Policy Infrastructure Controller (APIC) page for ACI Multi-Site documentation. On that page, you can use the "Choose a topic" and "Choose a document type" fields to narrow down the displayed documentation list and find a desired document.

The documentation includes installation, upgrade, configuration, programming, and troubleshooting guides, technical references, release notes, and knowledge base (KB) articles, and videos. KB articles provide information about a specific use cases or topics. The following tables describe the core Cisco Application Centric Infrastructure Multi-Site documentation.

Document

Description

Cisco ACI Multi-Site Release Notes

This document. Provides release information for the Cisco ACI Multi-Site Orchestrator product.

Cisco ACI Multi-Site Fundamentals Guide

Provides basic concepts and capabilities of the Cisco ACI Multi-Site.

Cisco ACI Multi-Site Hardware Requirements Guide

Provides the hardware requirements and compatibility.

Cisco ACI Multi-Site Installation and Upgrade Guide

Describes how to install Cisco ACI Multi-Site Orchestrator and perform day-0 operations.

Cisco ACI Multi-Site Configuration Guide

Describes Cisco ACI Multi-Site configuration options and procedures.

Cisco ACI Multi-Site REST API Configuration Guide

Describes how to use the Cisco ACI Multi-Site REST APIs.

Cisco ACI Multi-Site Troubleshooting Guide

Provides descriptions of common operations issues and Describes how to troubleshoot common Cisco ACI Multi-Site issues.

Cisco ACI Verified Scalability

Contains the maximum verified scalability limits for Cisco Application Centric Infrastructure (Cisco ACI), including Cisco ACI Multi-Site.

Cisco ACI YouTube channel

Contains videos that demonstrate how to perform specific tasks in the Cisco ACI Multi-Site.


Introduction

If you have a private cloud, you might run part of your workload on a public cloud. However, migrating workload to the public cloud requires working with a different cloud provider interface and learning different ways to set up connectivity and define security policies. Meeting these challenges can result in increased operational cost and loss of consistency. Cisco Cloud Application Policy Infrastructure Controller (APIC) can be used to solve the these problems by extending a Cisco Multi-Site fabric to Amazon Web Services (AWS), Microsoft Azure, or Google public clouds. You can also mix AWS, Azure, and Google Cloud in your deployment.

This document describes the features, issues, and limitations for the Cisco Cloud APIC software. For the features, issues, and limitations for the Cisco APIC, see the appropriate Cisco Application Policy Infrastructure Controller Release Notes. For the features, issues, and limitations for the Cisco Multi-Site Orchestrator/Nexus Dashboard Orchestrator, see the appropriate Cisco Nexus Dashboard Orchestrator Release Notes.

For more information about this product, see "Related Content."

Note: The documentation set for this product strives to use bias-free language. For the purposes of this documentation set, bias-free is defined as language that does not imply discrimination based on age, disability, gender, racial identity, ethnic identity, sexual orientation, socioeconomic status, and intersectionality. Exceptions may be present in the documentation due to language that is hardcoded in the user interfaces of the product software, language used based on RFP documentation, or language that is used by a referenced third-party product.

Date

Description

July 9, 2022

Release 25.0(4k) became available.

New Software Features

Feature

Description

Google Cloud Statistics

Beginning with the Cisco Cloud APIC Release 25.0(4k), you can view statistics that are derived by processing Google Cloud flow logs. Available statistics include ingress and egress bytes and packets for VPCs, regions, and endpoints.

For more information, see Cisco Cloud APIC for Google Cloud User Guide, Release 25.0(1) - 25.0(4).

Managed Brownfield VPC/VNETs using different access policies

This release provides updates for Managed Brownfield VPC/VNETs using the access policies in Cisco Cloud APIC with AWS and Azure, where new access policies are available at different levels.

For more information, see:

· Importing Existing Brownfield AWS Cloud VPCs into Cisco Cloud APIC

· Importing Existing Brownfield Azure Cloud VNets into Cisco Cloud APIC

Route table copying

This release allows for route table copying when importing AWS VPCs or Azure VNets into Cisco Cloud APIC.

For more information, see:

· Importing Existing Brownfield AWS Cloud VPCs into Cisco Cloud APIC

· Importing Existing Brownfield Azure Cloud VNets into Cisco Cloud APIC

Device Connector to enable telemetry to Cisco Intersight

Support is added for telemetry from Cisco Cloud APIC to Cisco Intersight using Device Connector.

For more information, see Cisco Cloud APIC and Intersight Device Connector.

Support for PAYG Licensing Model for Cisco Catalyst 8000V in Cisco Cloud APIC

Beginning with the 25.0(4k) release, Cisco Cloud APIC supports Pay-As-You-Go (PAYG) Licensing Model on Cisco Catalyst 8000V which allows users to deploy a Catalyst 8000V instance in the cloud based on the VM size and purchase the usage on an hourly basis. For this release, this feature is supported for AWS and Azure clouds.

For more information, see:

· Cisco Cloud APIC for AWS User Guide, Release 25.0(1) - 25.0(4)

· Cisco Cloud APIC for Azure User Guide, Release 25.0(1) - 25.0(4)

· Cisco Cloud APIC for AWS Installation Guide, Release 25.0(1) - 25.0(4)

· Cisco Cloud APIC for Azure Installation Guide, Release 25.0(1) - 25.0(4)


Fast Convergence using Bidirectional Forwarding Detection Protocol for Cisco Hybrid Cloud deployments

This release provides a solution for faster convergence/ fast failover using Bidirectional Forwarding Detection protocol in Cisco Cloud ACI hybrid cloud deployments for AWS and Azure Cloud Direct Connect and Express Route respectively.

For more information, see Improving Convergence Between On Premises ACI sites and Cloud Sites.

Configuration Drift detection

Detects and alerts when Cisco Cloud APIC-managed configurations have been modified in cloud outside of Cisco Cloud APIC.

For more information, see:

· Cisco Cloud APIC for AWS User Guide, Release 25.0(1) - 25.0(4)

· Cisco Cloud APIC for Azure User Guide, Release 25.0(1) - 25.0(4)


Supported Upgrade Paths

Cisco Cloud APIC supports policy-based upgrades for the following upgrade paths for AWS and Azure:

● Release 5.2(1) to 25.0(4)

● Release 25.0(1) to 25.0(4)

● Release 25.0(2) to 25.0(4)

● Release 25.0(3) to 24.0(4)

Changes in Behavior

There are no changes in behavior in this release.

Open Issues

Click the bug ID to access the Bug Search tool and see additional information about the bug. The "Exists In" column of the table specifies the 25.0(4) releases in which the bug exists. A bug might also exist in releases other than the 25.0(4) releases.

Bug ID

Description

Exists in

CSCvo30542

TACACS monitoring of the destination group is not supported through the GUI.

25.0(1c) and later

CSCvu64277

Stats seen on Cisco Cloud APIC are sometimes not in sync with Azure stats.

25.0(1c) and later

CSCvu66521

In the "Cloud Resources" section of the GUI, the names displayed in the "Name" column are not the same as the name of resources on the cloud. These are showing the Cloud APIC object names.

25.0(1c) and later

CSCvu72354

Adding an EPG endpoint selector fails with an error message saying the selector is already attached.

25.0(1c) and later

CSCvu78074

Route nextHop is not set to the redirect service node specified in the service graph.

25.0(1c) and later

CSCvv32664

When the CSR bandwidth needs to be increased, the user needs to undeploy all the CSRs in all the regions and redeploy with the desired bandwidth, which can cause traffic loss.

25.0(1c) and later

CSCvx16601

When the "AllowAll" flag is enabled on a service device such as a native load balancer or on the logical interface of a third party device, it is possible that to see some specific rules apart form a rule that allows all traffic from any source to any destination.

25.0(1c) and later

CSCvy06610

The eventmgr crashes when handling a fault triggered by a new cloud account.

25.0(1c) and later

CSCvy45517

The Cisco Cloud APIC GUI shows the total allowed count for CtxProfile, VRF (fvCtx), EPGs, and contracts. These numbers have been validated only for Azure-based deployments. For AWS deployments, the numbers supported are much lower.

25.0(1c) and later

CSCvy89617

Cloud routers may not get created if external network objects are not configured. External network configuration is required for configuring cloud routers.

25.0(1c) and later

CSCvy97972

Cisco Cloud APIC in this release limits the number of regions where we can deploy the hubnetwork in order to establish external connectivity. When you attempt to deploy/configure hubnetwork in more than four regions, the configuration will be rejected with the following error:

Invalid Configuration CT_INTNETWORK_REGION_MAXIMUM: At present, there can be at most 4 cloudRegionName in cloudtemplateIntNetwork uni/tn-infra/infranetwork-default/intnetwork-default; current count = <total-hubnetwork-regions-attempted>

25.0(1c) and later

CSCvz17160

Customers are restricted to shorter key value pairs than they need to be.

25.0(1c) and later

CSCvz21771

VPN tunnels may not come up when the Cisco Cloud APIC configuration is posted via XML interface. This problem won't be seen/encountered when we use the Cisco Cloud APIC UI.

25.0(1c) and later

CSCvz38067

Incorrect DNS server is configured on Cisco Cloud APIC with Google Cloud. Though this is not directly used when deploying Cisco Cloud APIC with Google Cloud, an incorrect IP address is configured.

25.0(1c) and later

CSCvz47166

When delete followed by add operations of tenant or other resources (such as VPCs and contracts) are done within a short span of time, it is possible that the resource deployment in Google Cloud may get out of sync with the configuration on Cisco Cloud APIC. The likelihood of this happening is directly proportionate to the scale of configuration and how quickly the operations are done.

We may see resources either not created or not deleted on Google Cloud to match the user configuration on Cisco Cloud APIC.

25.0(1c) and later

CSCvz52773

When performing a Cisco Cloud APIC upgrade (but not also performing a CSR upgrade), before the upgrade is finished and when the Cisco Cloud APIC is reconciling the CSR configurations, if you delete certain configurations and add the same configurations back (for example, if you delete a VRF and add the VRF back), a traffic drop may happen. Eventually it should recover.

25.0(1c) and later

CSCvz62225

When you scale up the number of CSRs or routers per region, some of the configurations may be missing on the newly created CSR. This issue happens randomly on the newly created CSRs, in this case tunnels or BGP sessions on the new CSRs may be down due to missing configuration.

25.0(1c) and later

CSCwa03277

When the brownfield VPC is imported into Cisco Cloud APIC, you need to take care of the creation and management of the route table and route table entries, security group rules, and transit gateway VPC attachment.

After creating the transit gateway VPC attachment with the infra transit gateway for the brownfield VPC in the AWS console, the corresponding cloudCtxPeerOper for the brownfield cloud context profile will move from Failed state to Configured state.

After that, if the created transit gateway attachment for the brownfield VPC is deleted in the AWS console, cloudCtxPeerOper is not moving back to Failed state.

25.0(2e) and later

CSCwa07078

When the brownfield VPC is imported into Cisco Cloud APIC via REST API POST, a new cloud context profile is created. Under this cloud context profile, cloudRsCtxToAccessPolicy is created, which is in relation to read only access policy. One or more cloudCIDRs and cloudBrownfield with cloudIDMapping, which holds the VPC ID, is posted to Cloud APIC.

cloudIDMapping, which contains the VPC ID, points to the VPC present in the cloud. If the VPC ID is non-existent or if there is any difference between the cloudCIDRs posted vs the CIDRs present in the VPC, cloudCtxOper and cloudCidrOper moves to a Failed state.

But because of the delegate's distinguished name, the imported unmanaged VPC shows healthy.

25.0(2e) and later

CSCwa08564

UI dashboard shows the wrong status for inter-region connectivity.

25.0(2e) and later

CSCwa10752

When CSRs are deployed in non-home regions, and no CSR is deployed in the home region (where Cloud APIC is deployed) in Azure, faults are seen in the Cloud APIC where the ssh connectivity to the CSR is down.

25.0(2e) and later

CSCwa26716

An external endpoint group of type non internet cannot leak all routes(or public internet IPs) to a cloud endpoint group. This results in creating a static route in the Vnet route table to point to CSR NLB for that destination CIDR. If it's leak-all, we configure 0.0.0.0/0 to point to CSR NLB.

In case of Azure Cloud APIC we do not create a static route to internet, Azure does this implicitly by default. Cloud APIC only programs the rules. When a user creates a contract or route leak, Cloud APIC programs a static route to CSR NLB. This overrides the Azure implicit default route to internet if the CIDR overlaps with the pubic internet IPs, since the leak-all creates a 0.0.0.0/0 route to point to CSR NLB.

This is an invalid configuration unless the user intended it, i,e the user intended to login to the VM through it's private IP through CSR. If not the user has to leak specific routes to the cloud endpoint group.

25.0(2e) and later

CSCwa28888

This issue is hit in some cases where Cloud APIC is unable to deploy infra configuration, such as creating cloud routers in overlay-1 VPC. This is sometimes seen in new deployments, but not in the case of upgrade scenario. Cloud routers and other configurations do not get deployed in Google Cloud.

25.0(2e) and later

CSCwa29007

In the TGW l3out configuration, modifying the IPsec tunnel pre-shared key is not supported. UI does not allow the modification of the IPsec tunnel pre-shared key, as this field is grayed out.

API accepts this modification of the IPsec tunnel pre-shared key, but it's not updated on the AWS.

25.0(2e) and later

CSCwa36940

Transit gateway external connectivity is not getting deployed in regions where cloud context profiles are deployed.

25.0(2e) and later

CSCwa40705

When an IKEv1 tunnel is configured to a destination while another tunnel already exists to the same destination but with a different source interface, this tunnel will remain with the protocol shown as down.

25.0(2e) and later

CSCwa40843

Stale IPsec configuration remains when an IKEv1 IPsec tunnel is deleted.

25.0(2e) and later

CSCwa43845

This issue occurs if there is a misconfiguration done where the local subnet was provided under routes leaked from external VRF to internal VRF.

This is an Azure Cloud APIC only issue, since AWS does not allow programming routes that overlap with VPC's CIDR.

25.0(2e) and later

CSCwa44822

When the tunnels are created from source interface Gig 2 or 4, we put allow all for security group. In future the allow all rule needs to be removed and provide explicit IP needs to be allowed.

25.0(2e) and later

CSCwa45047

This is not a functional issue. There will be no fault shown in the Cloud APIC UI if the border gateway protocol sessions of the transit gateway external connectivity are down.

25.0(2e) and later

CSCwa48929

If Cloud APIC is rebooted, in a rare case an expected rule may not be programmed on the cloud due to a timing mismatch in the reconcile and programming workflow.

25.0(2e) and later

CSCwa49263

In VPC route table, the route table entry for a destination CIDR pointing to transit gateway is sometimes missing when a quick delete and add of tenant or contract is done or when we move from transit gateway connect to legacy transit gateway solution. This happens only with legacy transit gateway solution in either of the cases.

This is a timing issue with the legacy transit gateway solution, where we create two transit gateways per hub network in a region. This can happen either if we move to legacy transit gateway solution or if legacy transit gateways are coming up for the first time.

These conditions result in deleting the route table entry for a given destination CIDR and adding back the same entry at the same time. Due to an issue with the AWS API which returns a deleted route table entry as a non deleted entry, Cloud APIC deletes the wrong entry.

25.0(2e) and later

CSCwa97027

Sometimes the Cloud APIC CSR/CCR "Instance Status" in "Cloud Resources ->Instances -> is shown as "Not-applicable".

25.0(3k) and later

CSCwa99935

Disabling "Hub peering" triggers a CSR deployment, which restricts the CSR count and increases the bandwidth together in a single action.

25.0(3k) and later

CSCwa18353

The route between the internal and external VRF is not programmed on the CSR. It is expected to be configured on CSR as a part of the leakTo subnet configuration.

25.0(4k) and later

CSCwa92033

Cisco Catalyst 8000V License is not getting deregistered from Cisco Smart License Server as the Cisco Catalyst 8000V lose connection to the internet before getting deleted.

25.0(4k) and later

CSCwa97199

No functional issue, stale licenses are configured on the Cisco Catalyst 8000V. On programming a T2 license on the Cisco Catalyst 8000V, a stale license entry is seen on both the Cisco Catalyst 8000V and the Cisco Smart account.


25.0(4k) and later

CSCwa99599

On Azure, it takes a long time for Cisco Catalyst 8000Vs to become ready when deployed in a new region. As a result, the inter-site tunnels take time to come up, delaying the traffic.

25.0(4k) and later

CSCwb01378

The following fault appears on the Cisco Cloud APIC indicating that the license is not configured on the CCR :

Oper State of HcplatformLicense is down with administrative-down.

25.0(4k) and later

CSCwc01909

When staying on the VRFs table, the VRF internal/external statuses sometimes do not update for a long time.

25.0(4k) and later

CSCwc11244

In this case the HcloudCtxOper of infra VNet, which is used to deploy NLB/ALB, is down. Since one of the network interface associated with the Vnet is in a failed state, Cisco Cloud APIC is unable to add a new Cidr to the VNet.

25.0(4k) and later

CSCwc15976

When we have a contract whose resource name does not match regex '(?:[a-z](?:[-a-z0-9]{0,61}[a-z0-9])?)', it raises a fault with the following message, "Must be a match of regex '(?:[a-z](?:[-a-z0-9]{0,61}[a-z0-9])?)', invalid", as received from the specific cloud.

The details on the exact mismatch are missing from the fault.

25.0(4k) and later

CSCwc28787

When BFD is enabled between sites, the BFD sessions go down and come back up quickly with no user intervention/trigger.

25.0(4k) and later

CSCwc30301

Cloning of route-table entries to Cloud APIC route-table does not port route-table associations with other entities, such as managed-prefix-lists in AWS, SecurityGroup in Azure, and other such entities. This issue appears when the user imports a VPC/VNet into Cloud APIC in Routing Only mode or Routing & Security mode and clones the route-table(s) into an already-imported VPC/VNet.

25.0(4k) and later

CSCwc39392

In a few specific cases, on importing a brownfield VNet with a Routing & Security access policy, an NSG might not get created and/or VNets might not peer.

25.0(4k) and later

CSCwc43147

Azure vWAN feature is available on the Cloud APIC Setup Region Management screen.

25.0(4k) and later

Resolved Issues

Click the bug ID to access the Bug Search tool and see additional information about the bug. The "Fixed In" column of the table specifies whether the bug was resolved in the base release or a patch release.

Bug ID

Description

Fixed in

CSCvz87367

Even though the preferred DNS server is taking into effect in Cloud APIC, the DNS server which was configured first continues to be the preferred DNS server.

25.0(4k)

CSCwa54001

When a quick delete and add of inter-site connectivity happens in such a way that the tunnels on the cloud are unchanged, the status on the UI may show tunnels as down even when the tunnels are up on the GCP cloud and the Azure CSR.

This issue might also happen during an upgrade and/or connector restart or a crash.

25.0(4k)

Known Issues

Click the bug ID to access the Bug Search tool and see additional information about the bug. The "Exists In" column of the table specifies the 25.0(4) releases in which the bug exists. A bug might also exist in releases other than the 25.0(4) releases.

Bug ID

Description

Exists in

CSCvo06626

When a cloudExtEpg matches on a 0/0 network and has a bi-directional contract with two cloud EPGs, such as cloudEpg1 and CloudEpg2, this can result in inadvertent communication between endpoints in cloudEpg1 and cloudEpg2 without a contract between the two EPGs themselves.

25.0(1c) and later

CSCvo55112

Logs are lost upon stopping the Cloud APIC instance.

25.0(1c) and later

CSCvo95998

There is traffic loss after a Cloud APIC upgrade. Traffic will eventually converge, but this could take a few minutes.

25.0(1c) and later

CSCvq11780

Creating VPN connections fail with the "invalidCidr" error in AWS or the "More than one connection having the same BGP setting is not allowed" error in Azure.

25.0(1c) and later

CSCvq76039

When a fault is raised in the Cloud APIC, the fault message will be truncated and will not include the entire cloud message description.

25.0(1c) and later

CSCvr01341

REST API access to the Cloud APIC becomes delayed after deleting a tenant with scaled EPGs and endpoints. The client needs to retry after receiving the error.

25.0(1c) and later

CSCvu05329

The Ctx Oper managed object is not deleted after the attachment is deleted.

25.0(1c) and later

CSCvu81355

Traffic gets dropped after downgrading to the 5.0(1) release. Cloud Services Router has incompatible configurations due to an issue with reading configurations using SSH.

25.0(1c) and later

CSCvu88006

On the Dashboard, fewer VNet peerings are shown than expected.

25.0(1c) and later

CSCvv81647

When an invalid Cloud Services Router license token is configured after initially configuring a valid token, the Cloud Services Router fails the license registration and keeps using the old valid token. This failure can only be found from the CSR event log.

25.0(1c) and later

CSCvw05821

Redirection and UDR does not take effect when traffic coming through an express route and destined to a service end point is redirected to a native load balancer or firewall.

25.0(1c) and later

CSCvw07392

Inter-site VxLAN traffic drops for a given VRF table when it is deleted and re-added. Packet capture on the CSR shows "Incomplete Adjacency" as follows:

Punt 1 Count Code Cause 1 10 Incomplete adjacency <<<<<<<

Drop 1 Count Code Cause 1 94 Ipv4NoAdj

25.0(1c) and later

CSCvw07781

There is complete traffic loss for 180 seconds.

25.0(1c) and later

CSCvw24376

Inter region traffic is black-holed after the delete trigger for contracts/filter. It was observed that the TGW entry pointing to the remote region TGW is missing for the destination routes. On further debugging it was found that post delete trigger as part of re-add flow, when a describe call is sent to AWS got a reply with the state of this entry as "active" because of which a new create request is not being sent.

25.0(1c) and later

CSCvw39814

Infra VPC subnet route table entry for 0.0.0.0/0 route with TGW attachment as nh, is left as a stale entry upon being undeployed. There is no functional impact. Upon being redeployed, this entry is updated with the correct TGW attachment ID as nh.

25.0(1c) and later

CSCvw40737

SSH to a virtual machine's public IP address fails, despite the NSG allowing the traffic inbound. SSH to the private IP address of the virtual machine from within the VNet works.

25.0(1c) and later

CSCvw40818

After upgrading Cloud APIC, the Cloud Services Routers will be upgraded in two batches. The even set of CSRs are triggered for upgrade first. After their upgrade is complete and all of the even CSRs are datapathReady, only then the odd set of CSRs will be triggered for upgrade. When even one of the upgrade of the even CSRs fail and they don't become datapathReady, the odd set of CSRs will not be triggered for upgrade. This is the behavior followed to avoid any traffic loss.

25.0(1c) and later

CSCvw48190

When Cloud APIC is restart, the VPN connection from a tenant's VNets will get deleted and re-created, one by one. This can be seen in the Azure activity logs. It should not impact traffic, as all connections are not deleted at the same time.

25.0(1c) and later

CSCvw49898

When the downgrading from the 5.2(1) release to the 5.0(2) release, traffic loss is expected until all of the CSRs are downgraded back to the 17.1 release. The traffic loss occurs because when the CSRs are getting downgraded to the 17.1 release, the CSR NIC1s will be in the backendPools and traffic from the spokes will still be forwarded to the native load balancer. The traffic gets blackholed until the CSRs get fully programmed with all the configurations in the 17.1 release.

25.0(1c) and later

CSCvw50918

Upon downgrading Cloud APIC, VPN connections between Cloud APIC and the cloud (AWS/Azure VPN gateway) will be deleted and re-created, causing traffic loss. Traffic loss is based on how quickly the VPN connections are deleted and re-created in AWS due to AWS throttling.

25.0(1c) and later

CSCvw51544

A user who is assigned a large number of security domains may not be able to create other Cisco ACI policies.

25.0(1c) and later

CSCvw55088

A user who is assigned a large number of security domains may not be able to create other Cisco ACI policies.

25.0(1c) and later

CSCvx91010

When TGW Connect is disabled, traffic loss is observed for about 8 minutes.

25.0(1c) and later

CSCvx98260

When delete followed by add operations of tenant or other resources (such as VPCs and contracts) are done within a short span of time, it is possible that the resource deployment in Google Cloud may get out of sync with the configuration on Cisco Cloud APIC. The likelihood of this happening is directly proportionate to the scale of configuration and how quickly the operations are done.

We may see resources either not created or not deleted on Google Cloud to match the user configuration on Cisco Cloud APIC.

25.0(1c) and later

CSCvy10936

Downgrading Cisco Cloud APIC from release 5.2(1) to 5.1(2) may cause CSRs to not be downgraded. The CSR release for 5.2(1) is 17.3.2, and the CSR version for release 5.1(2) is 17.3.1. After the Cisco Cloud APIC downgrade, the CSR version should be downgraded to 17.3.1, but it will not happen due to this bug.

25.0(1c) and later

CSCvy12722

Loss of traffic between a cloud and Cisco ACI On-Premises deployment.

25.0(1c) and later

CSCvy13369

After upgrading AWS, infra vPC peering does not get deleted.

25.0(1c) and later

CSCvy19286

There is traffic loss after downgrading from 5.2(1) to 5.1(2).

25.0(1c) and later

CSCvy28890

There is a loss in SSH connectivity to the Cisco Cloud APIC across reboots. But, after a few minutes, the connection should come back and users will be able to SSH in to the Cisco Cloud APIC again.

25.0(1c) and later

CSCvy28896

There is an increase in the connector's memory utilization. All of the CSR workflows rerunning might happen even after the setup is in the steady state.

25.0(1c) and later

CSCvy30314

After upgrading the Cisco Cloud APIC, on the TGW route tables, the default route (0.0.0.0/0) does not point to infra VPC attachment or is missing. In this case, traffic intended to get forwarded to the CSR will be dropped or forwarded to an invalid next-hop.

25.0(1c) and later

CSCvy33435

There is intersite traffic loss when TGW Connect is enabled.

25.0(1c) and later

CSCvy34180

Cloud Intersite traffic is dropped due to the CSR in the cloud site not advertising the EVPN routes.

25.0(1c) and later

CSCvy77233

Routes for subnets that are not yet configured in Google Cloud may become visible on an external device. When you configure routes to be advertised to an external device, but don't actually configure subnets in the cloud that you intend to advertise the routes for, those routes are still advertised.

Remote router may see routes that are advertised even when the subnets are not yet configured.

The traffic will get dropped because the subnets are not actually configured.

25.0(1c) and later

CSCvz11574

The cloud VRF egress route table is missing the route for 0.0.0.0/0 via the Internet Gateway (IGW), which leads to issues with ssh for VMs in the cloud VRF.

25.0(1c) and later

CSCvz20282

An upgrade to or downgrade from the Cloud APIC 5.2(1g) release to any release while using "Ignore Compatibility Check: no" will fail. The following fault is raised: "The upgrade has an upgrade status of Failed Due to Incompatible Desired Version."

25.0(1c) and later

CSCvz49747

When delete followed by add operations of tenant or other resources (such as VPCs and contracts) are done within a short span of time, it is possible that the resource deployment in Google Cloud may get out of sync with the configuration on Cisco Cloud APIC. The likelihood of this happening is directly proportionate to the scale of configuration and how quickly the operations are done.

We may see resources either not created or not deleted on Google Cloud to match the user configuration on Cisco Cloud APIC.

25.0(1c) and later

CSCwa92698

If the Cloud APIC infra CIDR has a collision with the reserved CIDR 172.17.0.0/16, connectivity to the Cat8kv VMs from Cloud APIC VM might fail. If the connectivity fails, the configuration push to Cat8kv will fail and Cat8kv will remain unreachable from Cloud APIC. A fault will be raised in the Cloud APIC.

Currently 172.17.0.0/16 is reserved by Cloud APIC and it cannot be used as Infra CIDR. 172.17.0.0/16 is used by the docker network running on Cloud APIC.

25.0(3k) and later

Compatibility Information

This section lists the compatibility information for the Cisco Cloud APIC software. In addition to the information in this section, see the appropriate Cisco Application Policy Infrastructure Controller Release Notes and Cisco Nexus Dashboard Orchestrator Release Notes for compatibility information for those products.

· Cloud APIC release 25.0(4k) is compatible with Cisco Nexus Dashboard Orchestrator, release 3.7(1).

· Cloud APIC supports the following AWS regions:

o Asia Pacific (Hong Kong)

o Asia Paciﬁc (Mumbai)

o Asia Paciﬁc (Osaka-Local)

o Asia Paciﬁc (Seoul)

o Asia Paciﬁc (Singapore)

o Asia Paciﬁc (Sydney)

o Asia Paciﬁc (Tokyo)

o AWS GovCloud (US-Gov-West)

o Canada (Central)

o EU (Frankfurt)

o EU (Ireland)

o EU (London)

o EU (Milan)

o EU (Stockholm)

o South America (São Paulo)

o US East (N. Virginia)

o US East (Ohio)

o US West (N. California)

o US West (Oregon)

· Cloud APIC supports the following Azure regions:

o Australiacentral

o Australiacentral2

o Australiaeast

o Australiasoutheast

o Brazilsouth

o Canadacentral

o Canadaeast

o Centralindia

o Centralus

o Eastasia

o Eastus

o Eastus2

o Francecentral

o Germanywestcentral

o Japaneast

o Japanwest

o Koreacentral

o Koreasouth

o Northcentralus

o Northeurope

o Norwayeast

o Southafricanorth

o Southcentralus

o Southeastasia

o Southindia

o Switzerlandnorth

o Uaenorth

o Uksouth

o Ukwest

o Westcentralus

o Westeurope

o Westindia

o Westus

o Westus2

· Cloud APIC supports the following Azure Government cloud regions:

o US DoD Central

o US DoD East

o US Gov Arizona

o US Gov Texas

o US Gov Virginia

Note: The US Gov Iowa region is not supported in Cisco Cloud APIC because Azure has deprecated support for this region.

· Cloud APIC supports all Google Cloud regions.

Related Content

See the Cisco Cloud Application Policy Infrastructure Controller page for the documentation.

See the Cisco Application Policy Infrastructure Controller (APIC) page for the verified scability, Cisco Application Policy Infrastructure Controller (APIC), and Cisco Multi-Site Orchestrator (MSO) documentation.

The documentation includes installation, upgrade, configuration, programming, and troubleshooting guides, technical references, release notes, and knowledge base (KB) articles, as well as other documentation. KB articles provide information about a specific use case or a specific topic.

By using the "Choose a topic" and "Choose a document type" fields of the APIC documentation website, you can narrow down the displayed documentation list to make it easier to find the desired document.


Introduction

If you have a private cloud, you might run part of your workload on a public cloud. However, migrating workload to the public cloud requires working with a different cloud provider interface and learning different ways to set up connectivity and define security policies. Meeting these challenges can result in increased operational cost and loss of consistency. Cisco Cloud Application Policy Infrastructure Controller (APIC) can be used to solve the these problems by extending a Cisco Multi-Site fabric to Amazon Web Services (AWS), Microsoft Azure, or Google public clouds. You can also mix AWS, Azure, and Google Cloud in your deployment.

This document describes the features, issues, and limitations for the Cisco Cloud APIC software. For the features, issues, and limitations for the Cisco APIC, see the appropriate Cisco Application Policy Infrastructure Controller Release Notes. For the features, issues, and limitations for the Cisco Multi-Site Orchestrator/Nexus Dashboard Orchestrator, see the appropriate Cisco Multi-Site Orchestrator Release Notes.

For more information about this product, see "Related Content."

Note: The documentation set for this product strives to use bias-free language. For the purposes of this documentation set, bias-free is defined as language that does not imply discrimination based on age, disability, gender, racial identity, ethnic identity, sexual orientation, socioeconomic status, and intersectionality. Exceptions may be present in the documentation due to language that is hardcoded in the user interfaces of the product software, language used based on RFP documentation, or language that is used by a referenced third-party product.

Date

Description

March 11, 2022

Release 25.0(3k) became available.

New Software Features

Feature

Description

Move from the Cisco Cloud Services Router 1000v to the Cisco Catalyst 8000V

Cisco Cloud APIC moves from the Cisco Cloud Services Router 1000v to the Cisco Catalyst 8000V beginning with release 25.0(3).

For more information, see:

· Cisco Cloud APIC for AWS User Guide, Release 25.0(x)

· Cisco Cloud APIC for Azure User Guide, Release 25.0(x)

· Cisco Cloud APIC for AWS Installation Guide, Release 25.0(x)

· Cisco Cloud APIC for Azure Installation Guide, Release 25.0(x)

Support for multiple frontend IP addresses for Azure network load balancer

This release provides support for multiple frontend IP addresses for the Azure network load balancer in Cisco Cloud APIC.

For more information, see Cisco Cloud APIC for Azure User Guide, Release 25.0(x).


Supported Upgrade Paths

Cisco Cloud APIC supports policy-based upgrades for the following upgrade paths:

● Release 5.2(1) to 25.0(3)

● Release 25.0(1) to 25.0(3)

● Release 25.0(2) to 25.0(3)

Changes in Behavior

There are no changes in behavior in this release.

Open Issues

Click the bug ID to access the Bug Search tool and see additional information about the bug. The "Exists In" column of the table specifies the 25.0(3) releases in which the bug exists. A bug might also exist in releases other than the 25.0(3) releases.

Bug ID

Description

Exists in

CSCvz62225

When you scale up the number of CSRs or routers per region, some of the configurations may be missing on the newly created CSR. This issue happens randomly on the newly created CSRs, in this case tunnels or BGP sessions on the new CSRs may be down due to missing configuration.

25.0(1c) and later

CSCvz47166

When delete followed by add operations of tenant or other resources (such as VPCs and contracts) are done within a short span of time, it is possible that the resource deployment in Google Cloud may get out of sync with the configuration on Cisco Cloud APIC. The likelihood of this happening is directly proportionate to the scale of configuration and how quickly the operations are done.

We may see resources either not created or not deleted on Google Cloud to match the user configuration on Cisco Cloud APIC.

25.0(1c) and later

CSCvy97972

Cisco Cloud APIC in this release limits the number of regions where we can deploy the hubnetwork in order to establish external connectivity. When you attempt to deploy/configure hubnetwork in more than four regions, the configuration will be rejected with the following error:

Invalid Configuration CT_INTNETWORK_REGION_MAXIMUM: At present, there can be at most 4 cloudRegionName in cloudtemplateIntNetwork uni/tn-infra/infranetwork-default/intnetwork-default; current count = <total-hubnetwork-regions-attempted>

25.0(1c) and later

CSCvz17160

Customers are restricted to shorter key value pairs than they need to be.

25.0(1c) and later

CSCvz21771

VPN tunnels may not come up when the Cisco Cloud APIC configuration is posted via XML interface. This problem won't be seen/encountered when we use the Cisco Cloud APIC UI.

25.0(1c) and later

CSCvz47232

When we delete any configuration, it takes time to reach eventual consistency and clean up all the resources on the cloud. This may not happen when Cisco Cloud APIC is rebooted while the delete/cleanup operation is underway. A few resources may remain on the cloud if Cisco Cloud APIC is rebooted while the cleanup is in progress.

25.0(1c) and later

CSCvy89617

Cloud routers may not get created if external network objects are not configured. External network configuration is required for configuring cloud routers.

25.0(1c) and later

CSCvz38067

Incorrect DNS server is configured on Cisco Cloud APIC with Google Cloud. Though this is not directly used when deploying Cisco Cloud APIC with Google Cloud, an incorrect IP address is configured.

25.0(1c) and later

CSCvz52773

When performing a Cisco Cloud APIC upgrade (but not also performing a CSR upgrade), before the upgrade is finished and when the Cisco Cloud APIC is reconciling the CSR configurations, if you delete certain configurations and add the same configurations back (for example, if you delete a VRF and add the VRF back), a traffic drop may happen. Eventually it should recover.

25.0(1c) and later

CSCvo30542

TACACS monitoring of the destination group is not supported through the GUI.

25.0(1c) and later

CSCvu64277

Stats seen on Cisco Cloud APIC are sometimes not in sync with Azure stats.

25.0(1c) and later

CSCvu66521

In the "Cloud Resources" section of the GUI, the names displayed in the "Name" column are not the same as the name of resources on the cloud. These are showing the Cloud APIC object names.

25.0(1c) and later

CSCvu72354

Adding an EPG endpoint selector fails with an error message saying the selector is already attached.

25.0(1c) and later

CSCvu78074

Route nextHop is not set to the redirect service node specified in the service graph.

25.0(1c) and later

CSCvv32664

When the CSR bandwidth needs to be increased, the user needs to undeploy all the CSRs in all the regions and redeploy with the desired bandwidth, which can cause traffic loss.

25.0(1c) and later

CSCvx16601

When the "AllowAll" flag is enabled on a service device such as a native load balancer or on the logical interface of a third party device, it is possible that to see some specific rules apart form a rule that allows all traffic from any source to any destination.

25.0(1c) and later

CSCvy06610

The eventmgr crashes when handling a fault triggered by a new cloud account.

25.0(1c) and later

CSCwa03277

When the brownfield VPC is imported into Cisco Cloud APIC, you need to take care of the creation and management of the route table and route table entries, security group rules, and transit gateway VPC attachment.

After creating the transit gateway VPC attachment with the infra transit gateway for the brownfield VPC in the AWS console, the corresponding cloudCtxPeerOper for the brownfield cloud context profile will move from Failed state to Configured state.

After that, if the created transit gateway attachment for the brownfield VPC is deleted in the AWS console, cloudCtxPeerOper is not moving back to Failed state.

25.0(2e) and later

CSCwa40843

Stale IPsec configuration remains when an IKEv1 IPsec tunnel is deleted.

25.0(2e) and later

CSCwa07078

When the brownfield VPC is imported into Cisco Cloud APIC via REST API POST, a new cloud context profile is created. Under this cloud context profile, cloudRsCtxToAccessPolicy is created, which is in relation to read only access policy. One or more cloudCIDRs and cloudBrownfield with cloudIDMapping, which holds the VPC ID, is posted to Cloud APIC.

cloudIDMapping, which contains the VPC ID, points to the VPC present in the cloud. If the VPC ID is non-existent or if there is any difference between the cloudCIDRs posted vs the CIDRs present in the VPC, cloudCtxOper and cloudCidrOper moves to a Failed state.

But because of the delegate's distinguished name, the imported unmanaged VPC shows healthy.

25.0(2e) and later

CSCwa10752

When CSRs are deployed in non-home regions, and no CSR is deployed in the home region (where Cloud APIC is deployed) in Azure, faults are seen in the Cloud APIC where the ssh connectivity to the CSR is down.

25.0(2e) and later

CSCwa29007

In the TGW l3out configuration, modifying the IPsec tunnel pre-shared key is not supported. UI does not allow the modification of the IPsec tunnel pre-shared key, as this field is grayed out.

API accepts this modification of the IPsec tunnel pre-shared key, but it's not updated on the AWS.

25.0(2e) and later

CSCwa08564

UI dashboard shows the wrong status for inter-region connectivity.

25.0(2e) and later

CSCwa40705

When an IKEv1 tunnel is configured to a destination while another tunnel already exists to the same destination but with a different source interface, this tunnel will remain with the protocol shown as down.

25.0(2e) and later

CSCwa28888

This issue is hit in some cases where Cloud APIC is unable to deploy infra configuration, such as creating cloud routers in overlay-1 VPC. This is sometimes seen in new deployments, but not in the case of upgrade scenario. Cloud routers and other configurations do not get deployed in Google Cloud.

25.0(2e) and later

CSCwa44822

When the tunnels are created from source interface Gig 2 or 4, we put allow all for security group. In future the allow all rule needs to be removed and provide explicit IP needs to be allowed.

25.0(2e) and later

CSCwa36940

Transit gateway external connectivity is not getting deployed in regions where cloud context profiles are deployed.

25.0(2e) and later

CSCwa54001

When a quick delete and add of inter-site connectivity happens in such a way that the tunnels on the cloud are unchanged, the status on the UI may show tunnels as down even when the tunnels are up on the GCP cloud and the Azure CSR.


This issue might also happen during an upgrade and/or connector restart or a crash.

25.0(2e) and later

CSCwa43845

This issue occurs if there is a misconfiguration done where the local subnet was provided under routes leaked from external VRF to internal VRF.

This is an Azure Cloud APIC only issue, since AWS does not allow programming routes that overlap with VPC's CIDR.

25.0(2e) and later

CSCwa48929

If Cloud APIC is rebooted, in a rare case an expected rule may not be programmed on the cloud due to a timing mismatch in the reconcile and programming workflow.

25.0(2e) and later

CSCwa49263

In VPC route table, the route table entry for a destination CIDR pointing to transit gateway is sometimes missing when a quick delete and add of tenant or contract is done or when we move from transit gateway connect to legacy transit gateway solution. This happens only with legacy transit gateway solution in either of the cases.

This is a timing issue with the legacy transit gateway solution, where we create two transit gateways per hub network in a region. This can happen either if we move to legacy transit gateway solution or if legacy transit gateways are coming up for the first time.

These conditions result in deleting the route table entry for a given destination CIDR and adding back the same entry at the same time. Due to an issue with the AWS API which returns a deleted route table entry as a non deleted entry, Cloud APIC deletes the wrong entry.

25.0(2e) and later

CSCwa49534

Consider 3 regions (A, B, C), where both A and B have CSR in their region.

Transit gateway cross-region comes in picture when there are no local CSRs in a region(region 'A'). The traffic of region "A" will hit the local transit gateway. On this transit gateway's user VPC Route Table, there will be a 0/0 entry which will have next-hop pointing to the nearest regions (region "B") transit gateway peering attachment.

When the CSRs of region "B" go down, the 0/0 route in the region "A" transit gateway route table should now point to take the next hop to region "C" transit gateway peering.

But in this bug, this flip does not happen at all times, which leads to traffic loss.

25.0(2e) and later

CSCwa26716

An external endpoint group of type non internet cannot leak all routes(or public internet IPs) to a cloud endpoint group. This results in creating a static route in the Vnet route table to point to CSR NLB for that destination CIDR. If it's leak-all, we configure 0.0.0.0/0 to point to CSR NLB.

In case of Azure Cloud APIC we do not create a static route to internet, Azure does this implicitly by default. Cloud APIC only programs the rules. When a user creates a contract or route leak, Cloud APIC programs a static route to CSR NLB. This overrides the Azure implicit default route to internet if the CIDR overlaps with the pubic internet IPs, since the leak-all creates a 0.0.0.0/0 route to point to CSR NLB.

This is an invalid configuration unless the user intended it, i,e the user intended to login to the VM through it's private IP through CSR. If not the user has to leak specific routes to the cloud endpoint group.

25.0(2e) and later

CSCvz87367

Even though the preferred DNS server is taking into effect in Cloud APIC, the DNS server which was configured first continues to be the preferred DNS server.

25.0(2e) and later

CSCwa45047

This is not a functional issue. There will be no fault shown in the Cloud APIC UI if the border gateway protocol sessions of the transit gateway external connectivity are down.

25.0(2e) and later

CSCwa92698

If the Cloud APIC infra CIDR has a collision with the reserved CIDR 172.17.0.0/16, connectivity to the Cat8kv VMs from Cloud APIC VM might fail. If the connectivity fails, the configuration push to Cat8kv will fail and Cat8kv will remain unreachable from Cloud APIC. A fault will be raised in the Cloud APIC.

Currently 172.17.0.0/16 is reserved by Cloud APIC and it cannot be used as Infra CIDR. 172.17.0.0/16 is used by the docker network running on Cloud APIC.

25.0(3k) and later

CSCwa97027

Sometimes the Cloud APIC CSR/CCR "Instance Status" in "Cloud Resources ->Instances -> is shown as "Not-applicable".

25.0(3k) and later

CSCwa99935

Disabling "Hub peering" triggers a CSR deployment, which restricts the CSR count and increases the bandwidth together in a single action.

25.0(3k) and later

CSCwb15333

You might see transient traffic drop upon upgrading the Cloud APIC from release 25.0(2f) to 25.0(3). This upgrade also requires an upgrade of the CSR to the Cisco Catalyst 8000V.

25.0(3k) and later

Resolved Issues

Click the bug ID to access the Bug Search tool and see additional information about the bug. The "Fixed In" column of the table specifies whether the bug was resolved in the base release or a patch release.

Bug ID

Description

Fixed in

CSCvy07759

CSR upgrade banner is not updated after the upgrade is complete.

25.0(2f)

CSCvy42684

Importing a configuration into Cloud APIC 5.2 displays the following error: maximum buffer length exceeded.

25.0(2f)

CSCvy94328

overlay-1 VRF in tenant "infra" shows up as one of the VRFs with which an external network is associated. However, Cisco Cloud APIC does not allow overlay-1 VRF to be associated with any external network. In this release, overlay-1 VRF shows up alongside other VRFs to be associated with an external network. It has no functional impact, but it gives an incorrect impression that we have a external network associated with overlay-1 VRF. This external network name is set to default and there are no other objects/MOs for this external network configured.

25.0(2f)

CSCvz26752

The Cisco Cloud APIC UI may display empty entries when routes are leaked to or from non-existing VRFs. For example, this is seen when a VRF is deleted and another VRF leaked one or more routes to that deleted VRF. The UI may indicate an empty value under the VRF to which the routes are leaked.

25.0(2f)

CSCvz31331

APIC REST APIs allows to create a cloudEPg that refers to an external VRF in the infra tenant.

This is disabled when configuring through the GUI and should be blocked in the backend as well.

25.0(2f)

CSCvz39389

After a clean reboot of Cisco Cloud APIC and an import of the configuration, a CSR might take around 45 minutes to re-establish the datapath readiness.

25.0(2f)

CSCvz40326

Routes to and from overlay-2 VRF may not be configured in the cloud deployment.

25.0(2f)

CSCvz41009

When delete followed by add operations of tenant or other resources (such as VPCs and contracts) are done within a short span of time, it is possible that the resource deployment in Google Cloud may get out of sync with the configuration on Cisco Cloud APIC. The likelihood of this happening is directly proportionate to the scale of configuration and how quickly the operations are done.

We may see resources either not created or not deleted on Google Cloud to match the user configuration on Cisco Cloud APIC.

25.0(2f)

CSCvz43324

The Cloud APIC REST APIs allow you to create a cloud EPG that refers to an External VRF in the infra tenant.

This is disabled on the UI and should be disabled through the REST API as well.

25.0(2f)

CSCvz46464

There may be some traffic loss encountered when Cisco Cloud APIC is rebooted and a new configuration is imported. The new configuration takes time to deploy as we achieve eventual consistency.

25.0(2f)

CSCvz66172

Unable to get public IP addresses assigned to non Gig1 Interfaces of CSR. Gig1 Interface gets a public IP addresses.

25.0(2f)

CSCwa50116

This bug has been filed to evaluate the Cisco Cloud APIC against the vulnerability in the Apache Log4j Java library disclosed on December 9th, 2021.

Cisco has reviewed this product and concluded that it contains a vulnerable version of Apache Log4j and is affected by the following vulnerability:

CVE-2021-44228 - Apache Log4j2 JNDI features do not protect against attacker controlled LDAP and other JNDI related endpoints

This advisory is available at the following link:

https://tools.cisco.com/security/center/content/CiscoSecurityAdvisory/cisco-sa-apache-log4j-qRuKNEbd

25.0(2f)

Known Issues

Click the bug ID to access the Bug Search tool and see additional information about the bug. The "Exists In" column of the table specifies the 25.0(3) releases in which the bug exists. A bug might also exist in releases other than the 25.0(3) releases.

Bug ID

Description

Exists in

CSCvz49747

When delete followed by add operations of tenant or other resources (such as VPCs and contracts) are done within a short span of time, it is possible that the resource deployment in Google Cloud may get out of sync with the configuration on Cisco Cloud APIC. The likelihood of this happening is directly proportionate to the scale of configuration and how quickly the operations are done.

We may see resources either not created or not deleted on Google Cloud to match the user configuration on Cisco Cloud APIC.

25.0(1c) and later

CSCvy77233

Routes for subnets that are not yet configured in Google Cloud may become visible on an external device. When you configure routes to be advertised to an external device, but don't actually configure subnets in the cloud that you intend to advertise the routes for, those routes are still advertised.

Remote router may see routes that are advertised even when the subnets are not yet configured.

The traffic will get dropped because the subnets are not actually configured.

25.0(1c) and later

CSCvx98260

When delete followed by add operations of tenant or other resources (such as VPCs and contracts) are done within a short span of time, it is possible that the resource deployment in Google Cloud may get out of sync with the configuration on Cisco Cloud APIC. The likelihood of this happening is directly proportionate to the scale of configuration and how quickly the operations are done.

We may see resources either not created or not deleted on Google Cloud to match the user configuration on Cisco Cloud APIC.

25.0(1c) and later

CSCvo06626

When a cloudExtEpg matches on a 0/0 network and has a bi-directional contract with two cloud EPGs, such as cloudEpg1 and CloudEpg2, this can result in inadvertent communication between endpoints in cloudEpg1 and cloudEpg2 without a contract between the two EPGs themselves.

25.0(1c) and later

CSCvo55112

Logs are lost upon stopping the Cloud APIC instance.

25.0(1c) and later

CSCvo95998

There is traffic loss after a Cloud APIC upgrade. Traffic will eventually converge, but this could take a few minutes.

25.0(1c) and later

CSCvq11780

Creating VPN connections fail with the "invalidCidr" error in AWS or the "More than one connection having the same BGP setting is not allowed" error in Azure.

25.0(1c) and later

CSCvq76039

When a fault is raised in the Cloud APIC, the fault message will be truncated and will not include the entire cloud message description.

25.0(1c) and later

CSCvr01341

REST API access to the Cloud APIC becomes delayed after deleting a tenant with scaled EPGs and endpoints. The client needs to retry after receiving the error.

25.0(1c) and later

CSCvu05329

The Ctx Oper managed object is not deleted after the attachment is deleted.

25.0(1c) and later

CSCvu81355

Traffic gets dropped after downgrading to the 5.0(1) release. Cloud Services Router has incompatible configurations due to an issue with reading configurations using SSH.

25.0(1c) and later

CSCvu88006

On the Dashboard, fewer VNet peerings are shown than expected.

25.0(1c) and later

CSCvv81647

When an invalid Cloud Services Router license token is configured after initially configuring a valid token, the Cloud Services Router fails the license registration and keeps using the old valid token. This failure can only be found from the CSR event log.

25.0(1c) and later

CSCvw05821

Redirection and UDR does not take effect when traffic coming through an express route and destined to a service end point is redirected to a native load balancer or firewall.

25.0(1c) and later

CSCvw07392

Inter-site VxLAN traffic drops for a given VRF table when it is deleted and re-added. Packet capture on the CSR shows "Incomplete Adjacency" as follows:

Punt 1 Count Code Cause 1 10 Incomplete adjacency <<<<<<<

Drop 1 Count Code Cause 1 94 Ipv4NoAdj

25.0(1c) and later

CSCvw07781

There is complete traffic loss for 180 seconds.

25.0(1c) and later

CSCvw24376

Inter region traffic is black-holed after the delete trigger for contracts/filter. It was observed that the TGW entry pointing to the remote region TGW is missing for the destination routes. On further debugging it was found that post delete trigger as part of re-add flow, when a describe call is sent to AWS got a reply with the state of this entry as "active" because of which a new create request is not being sent.

25.0(1c) and later

CSCvw39814

Infra VPC subnet route table entry for 0.0.0.0/0 route with TGW attachment as nh, is left as a stale entry upon being undeployed. There is no functional impact. Upon being redeployed, this entry is updated with the correct TGW attachment ID as nh.

25.0(1c) and later

CSCvw40737

SSH to a virtual machine's public IP address fails, despite the NSG allowing the traffic inbound. SSH to the private IP address of the virtual machine from within the VNet works.

25.0(1c) and later

CSCvw40818

After upgrading Cloud APIC, the Cloud Services Routers will be upgraded in two batches. The even set of CSRs are triggered for upgrade first. After their upgrade is complete and all of the even CSRs are datapathReady, only then the odd set of CSRs will be triggered for upgrade. When even one of the upgrade of the even CSRs fail and they don't become datapathReady, the odd set of CSRs will not be triggered for upgrade. This is the behavior followed to avoid any traffic loss.

25.0(1c) and later

CSCvw48190

When Cloud APIC is restart, the VPN connection from a tenant's VNets will get deleted and re-created, one by one. This can be seen in the Azure activity logs. It should not impact traffic, as all connections are not deleted at the same time.

25.0(1c) and later

CSCvw49898

When the downgrading from the 5.2(1) release to the 5.0(2) release, traffic loss is expected until all of the CSRs are downgraded back to the 17.1 release. The traffic loss occurs because when the CSRs are getting downgraded to the 17.1 release, the CSR NIC1s will be in the backendPools and traffic from the spokes will still be forwarded to the native load balancer. The traffic gets blackholed until the CSRs get fully programmed with all the configurations in the 17.1 release.

25.0(1c) and later

CSCvw50918

Upon downgrading Cloud APIC, VPN connections between Cloud APIC and the cloud (AWS/Azure VPN gateway) will be deleted and re-created, causing traffic loss. Traffic loss is based on how quickly the VPN connections are deleted and re-created in AWS due to AWS throttling.

25.0(1c) and later

CSCvw51544

A user who is assigned a large number of security domains may not be able to create other Cisco ACI policies.

25.0(1c) and later

CSCvw55088

A user who is assigned a large number of security domains may not be able to create other Cisco ACI policies.

25.0(1c) and later

CSCvx91010

When TGW Connect is disabled, traffic loss is observed for about 8 minutes.

25.0(1c) and later

CSCvy10936

Downgrading Cisco Cloud APIC from release 5.2(1) to 5.1(2) may cause CSRs to not be downgraded. The CSR release for 5.2(1) is 17.3.2, and the CSR version for release 5.1(2) is 17.3.1. After the Cisco Cloud APIC downgrade, the CSR version should be downgraded to 17.3.1, but it will not happen due to this bug.

25.0(1c) and later

CSCvy12722

Loss of traffic between a cloud and Cisco ACI On-Premises deployment.

25.0(1c) and later

CSCvy13369

After upgrading AWS, infra vPC peering does not get deleted.

25.0(1c) and later

CSCvy19286

There is traffic loss after downgrading from 5.2(1) to 5.1(2).

25.0(1c) and later

CSCvy28890

There is a loss in SSH connectivity to the Cisco Cloud APIC across reboots. But, after a few minutes, the connection should come back and users will be able to SSH in to the Cisco Cloud APIC again.

25.0(1c) and later

CSCvy28896

There is an increase in the connector's memory utilization. All of the CSR workflows rerunning might happen even after the setup is in the steady state.

25.0(1c) and later

CSCvy30314

After upgrading the Cisco Cloud APIC, on the TGW route tables, the default route (0.0.0.0/0) does not point to infra VPC attachment or is missing. In this case, traffic intended to get forwarded to the CSR will be dropped or forwarded to an invalid next-hop.

25.0(1c) and later

CSCvy33435

There is intersite traffic loss when TGW Connect is enabled.

25.0(1c) and later

CSCvy34180

Cloud Intersite traffic is dropped due to the CSR in the cloud site not advertising the EVPN routes.

25.0(1c) and later

CSCvy45517

The Cisco Cloud APIC GUI shows the total allowed count for CtxProfile, VRF (fvCtx), EPGs, and contracts. These numbers have been validated only for Azure-based deployments. For AWS deployments, the numbers supported are much lower.

25.0(1c) and later

CSCvz11574

The cloud VRF egress route table is missing the route for 0.0.0.0/0 via the Internet Gateway (IGW), which leads to issues with ssh for VMs in the cloud VRF.

25.0(1c) and later

CSCvz20282

An upgrade to or downgrade from the Cloud APIC 5.2(1g) release to any release while using "Ignore Compatibility Check: no" will fail. The following fault is raised: "The upgrade has an upgrade status of Failed Due to Incompatible Desired Version."

25.0(1c) and later

Compatibility Information

This section lists the compatibility information for the Cisco Cloud APIC software. In addition to the information in this section, see the appropriate Cisco Application Policy Infrastructure Controller Release Notes and Cisco Multi-Site Orchestrator Release Notes for compatibility information for those products.

· Cloud APIC release 25.0(3) is compatible with Cisco Nexus Dashboard Orchestrator, release 3.7(1).

· Cloud APIC supports the following AWS regions:

o Asia Pacific (Hong Kong)

o Asia Paciﬁc (Mumbai)

o Asia Paciﬁc (Osaka-Local)

o Asia Paciﬁc (Seoul)

o Asia Paciﬁc (Singapore)

o Asia Paciﬁc (Sydney)

o Asia Paciﬁc (Tokyo)

o AWS GovCloud (US-Gov-West)

o Canada (Central)

o EU (Frankfurt)

o EU (Ireland)

o EU (London)

o EU (Milan)

o EU (Stockholm)

o South America (São Paulo)

o US East (N. Virginia)

o US East (Ohio)

o US West (N. California)

o US West (Oregon)

· Cloud APIC supports the following Azure regions:

o Australiacentral

o Australiacentral2

o Australiaeast

o Australiasoutheast

o Brazilsouth

o Canadacentral

o Canadaeast

o Centralindia

o Centralus

o Eastasia

o Eastus

o Eastus2

o Francecentral

o Germanywestcentral

o Japaneast

o Japanwest

o Koreacentral

o Koreasouth

o Northcentralus

o Northeurope

o Norwayeast

o Southafricanorth

o Southcentralus

o Southeastasia

o Southindia

o Switzerlandnorth

o Uaenorth

o Uksouth

o Ukwest

o Westcentralus

o Westeurope

o Westindia

o Westus

o Westus2

· Cloud APIC supports the following Azure Government cloud regions:

o US DoD Central

o US DoD East

o US Gov Arizona

o US Gov Texas

o US Gov Virginia

· Cloud APIC supports all Google Cloud regions.

Related Content

See the Cisco Cloud Application Policy Infrastructure Controller page for the documentation.

See the Cisco Application Policy Infrastructure Controller (APIC) page for the verified scability, Cisco Application Policy Infrastructure Controller (APIC), and Cisco Multi-Site Orchestrator (MSO) documentation.

The documentation includes installation, upgrade, configuration, programming, and troubleshooting guides, technical references, release notes, and knowledge base (KB) articles, as well as other documentation. KB articles provide information about a specific use case or a specific topic.

By using the "Choose a topic" and "Choose a document type" fields of the APIC documentation website, you can narrow down the displayed documentation list to make it easier to find the desired document.


Introduction

If you have a private cloud, you might run part of your workload on a public cloud. However, migrating workload to the public cloud requires working with a different cloud provider interface and learning different ways to set up connectivity and define security policies. Meeting these challenges can result in increased operational cost and loss of consistency. Cisco Cloud Application Policy Infrastructure Controller (APIC) can be used to solve the these problems by extending a Cisco Multi-Site fabric to Amazon Web Services (AWS) or Microsoft Azure public clouds. You can also mix AWS and Azure in your deployment.

This document describes the features, issues, and limitations for the Cisco Cloud APIC software. For the features, issues, and limitations for the Cisco APIC, see the appropriate Cisco Application Policy Infrastructure Controller Release Notes. For the features, issues, and limitations for the Cisco Multi-Site Orchestrator, see the appropriate Cisco Multi-Site Orchestrator Release Notes.

For more information about this product, see "Related Content."

Note: The documentation set for this product strives to use bias-free language. For the purposes of this documentation set, bias-free is defined as language that does not imply discrimination based on age, disability, gender, racial identity, ethnic identity, sexual orientation, socioeconomic status, and intersectionality. Exceptions may be present in the documentation due to language that is hardcoded in the user interfaces of the product software, language used based on RFP documentation, or language that is used by a referenced third-party product.

Date

Description

January 18, 2022

Release 25.0(2f) became available. In the Resolved Issues section, added bug CSCwa50116.

December 17, 2021

Release 25.0(2e) became available.

New Software Features

Feature

Description

Support for importing existing AWS cloud VPCs into Cisco Cloud APIC in unmanaged mode

This release provides support for importing existing brownfield AWS cloud VPCs (VPCs that were not configured through Cisco Cloud APIC) into Cisco Cloud APIC.

For more information, see Importing Existing Brownfield AWS Cloud VPCs Into Cisco Cloud APIC.

Support for external networking using AWS transit gateways

Beginning with release 25.0(2), several external networking enhancements are available for the AWS transit gateway feature:

Cloud APIC on AWS supports network level, account level, and custom routing tables

Support for external branch connectivity over AWS transit gateway with IPsec

For more information, see Increasing Bandwidth Between VPCs by Using AWS Transit Gateway or AWS Transit Gateway Connect.

Support for site-external EPGs using Azure VPN gateways

Support is available for providing connectivity between a Cloud APIC-managed cloud site and a non-ACI remote site using VPN gateway.

For more information, see Cisco Cloud APIC for Azure User Guide, Release 25.0(x).

Support for all availability zones in an AWS region for user tenants

Support is now provided for multiple (greater than two) availability zones in AWS for Cisco Cloud APIC.

For more information, see Cisco Cloud APIC for AWS User Guide, Release 25.0(x).

Support for configuring routing and security policies independently in Azure and AWS

Beginning with release 25.0(2), the following updates are available for the routing policies:

Support for route maps-based route leaking between a pair of internal VRFs

Support for a global internal VRF route leak policy, which allows you to choose whether you want to use contract-based routing or route map-based routing between a pair of internal VRFs.

For more information, see:

· Cisco Cloud APIC for AWS User Guide, Release 25.0(x)

· Cisco Cloud APIC for Azure User Guide, Release 25.0(x)

CSR IPsec tunnels can now use any of the three available data interfaces for external branch connectivity

Prior to release 25.0(2), all the tunnels to external networks are originated from one specific interface on the CSR router (the GigabitEthernet3 interface, or cloudHostIfp-2). Beginning with release 25.0(2), support is now extended where tunnels to the same destination can be formed from the GigabitEthernet2, GigabitEthernet3, and GigabitEthernet4 interfaces. This is supported for tunnels with IKEv2 configurations only.

For more information, see:

· Cisco Cloud APIC for AWS User Guide, Release 25.0(x)

· Cisco Cloud APIC for Azure User Guide, Release 25.0(x)

Support for VM scale sets for Azure NLB backend pools

Beginning with release 25.0(2), support is added for Azure virtual machine scale sets as backend targets for load balancers.

For more information, see Cisco Cloud APIC for Azure User Guide, Release 25.0(x).

Support for increased number of cloud regions for workload deployment

Prior to release 25.0(2), you can have a maximum of four regions per site. Beginning with release 25.0(2), you can have a maximum of sixteen regions per site.

For more information, see:

· Cisco Cloud APIC for AWS User Guide, Release 25.0(x)

· Cisco Cloud APIC for Azure User Guide, Release 25.0(x)

Supported Upgrade Paths

Cisco Cloud APIC supports policy-based upgrades for the following upgrade paths:

● Release 5.2(1) to 25.0(2)

● Release 25.0(1) to 25.0(2)

Changes in Behavior

There are no changes in behavior in this release.

Open Issues

Click the bug ID to access the Bug Search tool and see additional information about the bug. The "Exists In" column of the table specifies the 25.0(2) releases in which the bug exists. A bug might also exist in releases other than the 25.0(2) releases.

Bug ID

Description

Exists in

CSCvy07759

CSR upgrade banner is not updated after the upgrade is complete.

25.0(1c) and later

CSCvz31331

APIC REST APIs allows to create a cloudEPg that refers to an external VRF in the infra tenant.

This is disabled when configuring through the GUI and should be blocked in the backend as well.

25.0(1c) and later

CSCvz62225

When you scale up the number of CSRs or routers per region, some of the configurations may be missing on the newly created CSR. This issue happens randomly on the newly created CSRs, in this case tunnels or BGP sessions on the new CSRs may be down due to missing configuration.

25.0(1c) and later

CSCvz66172

Unable to get public IP addresses assigned to non Gig1 Interfaces of CSR. Gig1 Interface gets a public IP addresses.

25.0(1c) and later

CSCvz47166

When delete followed by add operations of tenant or other resources (such as VPCs and contracts) are done within a short span of time, it is possible that the resource deployment in Google Cloud may get out of sync with the configuration on Cisco Cloud APIC. The likelihood of this happening is directly proportionate to the scale of configuration and how quickly the operations are done.

We may see resources either not created or not deleted on Google Cloud to match the user configuration on Cisco Cloud APIC.

25.0(1c) and later

CSCvy97972

Cisco Cloud APIC in this release limits the number of regions where we can deploy the hubnetwork in order to establish external connectivity. When you attempt to deploy/configure hubnetwork in more than four regions, the configuration will be rejected with the following error:

Invalid Configuration CT_INTNETWORK_REGION_MAXIMUM: At present, there can be at most 4 cloudRegionName in cloudtemplateIntNetwork uni/tn-infra/infranetwork-default/intnetwork-default; current count = <total-hubnetwork-regions-attempted>

25.0(1c) and later

CSCvz17160

Customers are restricted to shorter key value pairs than they need to be.

25.0(1c) and later

CSCvz21771

VPN tunnels may not come up when the Cisco Cloud APIC configuration is posted via XML interface. This problem won't be seen/encountered when we use the Cisco Cloud APIC UI.

25.0(1c) and later

CSCvz26752

The Cisco Cloud APIC UI may display empty entries when routes are leaked to or from non-existing VRFs. For example, this is seen when a VRF is deleted and another VRF leaked one or more routes to that deleted VRF. The UI may indicate an empty value under the VRF to which the routes are leaked.

25.0(1c) and later

CSCvz41009

When delete followed by add operations of tenant or other resources (such as VPCs and contracts) are done within a short span of time, it is possible that the resource deployment in Google Cloud may get out of sync with the configuration on Cisco Cloud APIC. The likelihood of this happening is directly proportionate to the scale of configuration and how quickly the operations are done.

We may see resources either not created or not deleted on Google Cloud to match the user configuration on Cisco Cloud APIC.

25.0(1c) and later

CSCvz43324

The Cloud APIC REST APIs allow you to create a cloud EPG that refers to an External VRF in the infra tenant.

This is disabled on the UI and should be disabled through the REST API as well.

25.0(1c) and later

CSCvz46464

There may be some traffic loss encountered when Cisco Cloud APIC is rebooted and a new configuration is imported. The new configuration takes time to deploy as we achieve eventual consistency.

25.0(1c) and later

CSCvz47232

When we delete any configuration, it takes time to reach eventual consistency and clean up all the resources on the cloud. This may not happen when Cisco Cloud APIC is rebooted while the delete/cleanup operation is underway. A few resources may remain on the cloud if Cisco Cloud APIC is rebooted while the cleanup is in progress.

25.0(1c) and later

CSCvy89617

Cloud routers may not get created if external network objects are not configured. External network configuration is required for configuring cloud routers.

25.0(1c) and later

CSCvy94328

verlay-1 VRF in tenant "infra" shows up as one of the VRFs with which an external network is associated. However, Cisco Cloud APIC does not allow overlay-1 VRF to be associated with any external network. In this release, overlay-1 VRF shows up alongside other VRFs to be associated with an external network. It has no functional impact, but it gives an incorrect impression that we have a external network associated with overlay-1 VRF. This external network name is set to default and there are no other objects/MOs for this external network configured.

25.0(1c) and later

CSCvz38067

Incorrect DNS server is configured on Cisco Cloud APIC with Google Cloud. Though this is not directly used when deploying Cisco Cloud APIC with Google Cloud, an incorrect IP address is configured.

25.0(1c) and later

CSCvz11574

The cloud VRF egress route table is missing the route for 0.0.0.0/0 via the Internet Gateway (IGW), which leads to issues with ssh for VMs in the cloud VRF.

25.0(1c) and later

CSCvz52773

When performing a Cisco Cloud APIC upgrade (but not also performing a CSR upgrade), before the upgrade is finished and when the Cisco Cloud APIC is reconciling the CSR configurations, if you delete certain configurations and add the same configurations back (for example, if you delete a VRF and add the VRF back), a traffic drop may happen. Eventually it should recover.

25.0(1c) and later

CSCvz39389

After a clean reboot of Cisco Cloud APIC and an import of the configuration, a CSR might take around 45 minutes to re-establish the datapath readiness.

25.0(1c) and later

CSCvz40326

Routes to and from overlay-2 VRF may not be configured in the cloud deployment.

25.0(1c) and later

CSCvo30542

TACACS monitoring of the destination group is not supported through the GUI.

25.0(1c) and later

CSCvu64277

Stats seen on Cisco Cloud APIC are sometimes not in sync with Azure stats.

25.0(1c) and later

CSCvu66521

In the "Cloud Resources" section of the GUI, the names displayed in the "Name" column are not the same as the name of resources on the cloud. These are showing the Cloud APIC object names.

25.0(1c) and later

CSCvu72354

Adding an EPG endpoint selector fails with an error message saying the selector is already attached.

25.0(1c) and later

CSCvu78074

Route nextHop is not set to the redirect service node specified in the service graph.

25.0(1c) and later

CSCvv32664

When the CSR bandwidth needs to be increased, the user needs to undeploy all the CSRs in all the regions and redeploy with the desired bandwidth, which can cause traffic loss.

25.0(1c) and later

CSCvx16601

When the "AllowAll" flag is enabled on a service device such as a native load balancer or on the logical interface of a third party device, it is possible that to see some specific rules apart form a rule that allows all traffic from any source to any destination.

25.0(1c) and later

CSCvy06610

The eventmgr crashes when handling a fault triggered by a new cloud account.

25.0(1c) and later

CSCwa03277

When the brownfield VPC is imported into Cisco Cloud APIC, you need to take care of the creation and management of the route table and route table entries, security group rules, and transit gateway VPC attachment.

After creating the transit gateway VPC attachment with the infra transit gateway for the brownfield VPC in the AWS console, the corresponding cloudCtxPeerOper for the brownfield cloud context profile will move from Failed state to Configured state.

After that, if the created transit gateway attachment for the brownfield VPC is deleted in the AWS console, cloudCtxPeerOper is not moving back to Failed state.

25.0(2e) and later

CSCwa40843

Stale IPsec configuration remains when an IKEv1 IPsec tunnel is deleted.

25.0(2e) and later

CSCwa07078

When the brownfield VPC is imported into Cisco Cloud APIC via REST API POST, a new cloud context profile is created. Under this cloud context profile, cloudRsCtxToAccessPolicy is created, which is in relation to read only access policy. One or more cloudCIDRs and cloudBrownfield with cloudIDMapping, which holds the VPC ID, is posted to Cloud APIC.

cloudIDMapping, which contains the VPC ID, points to the VPC present in the cloud. If the VPC ID is non-existent or if there is any difference between the cloudCIDRs posted vs the CIDRs present in the VPC, cloudCtxOper and cloudCidrOper moves to a Failed state.

But because of the delegate's distinguished name, the imported unmanaged VPC shows healthy.

25.0(2e) and later

CSCwa10752

When CSRs are deployed in non-home regions, and no CSR is deployed in the home region (where Cloud APIC is deployed) in Azure, faults are seen in the Cloud APIC where the ssh connectivity to the CSR is down.

25.0(2e) and later

CSCwa29007

In the TGW l3out configuration, modifying the IPsec tunnel pre-shared key is not supported. UI does not allow the modification of the IPsec tunnel pre-shared key, as this field is grayed out.

API accepts this modification of the IPsec tunnel pre-shared key, but it's not updated on the AWS.

25.0(2e) and later

CSCwa08564

UI dashboard shows the wrong status for inter-region connectivity.

25.0(2e) and later

CSCwa40705

When an IKEv1 tunnel is configured to a destination while another tunnel already exists to the same destination but with a different source interface, this tunnel will remain with the protocol shown as down.

25.0(2e) and later

CSCwa28888

This issue is hit in some cases where Cloud APIC is unable to deploy infra configuration, such as creating cloud routers in overlay-1 VPC. This is sometimes seen in new deployments, but not in the case of upgrade scenario. Cloud routers and other configurations do not get deployed in Google Cloud.

25.0(2e) and later

CSCwa44822

When the tunnels are created from source interface Gig 2 or 4, we put allow all for security group. In future the allow all rule needs to be removed and provide explicit IP needs to be allowed.

25.0(2e) and later

CSCwa36940

Transit gateway external connectivity is not getting deployed in regions where cloud context profiles are deployed.

25.0(2e) and later

CSCwa54001

When a quick delete and add of inter-site connectivity happens in such a way that the tunnels on the cloud are unchanged, the status on the UI may show tunnels as down even when the tunnels are up on the GCP cloud and the Azure CSR.


This issue might also happen during an upgrade and/or connector restart or a crash.

25.0(2e) and later

CSCwa43845

This issue occurs if there is a misconfiguration done where the local subnet was provided under routes leaked from external VRF to internal VRF.

This is an Azure Cloud APIC only issue, since AWS does not allow programming routes that overlap with VPC's CIDR.

25.0(2e) and later

CSCwa48929

If Cloud APIC is rebooted, in a rare case an expected rule may not be programmed on the cloud due to a timing mismatch in the reconcile and programming workflow.

25.0(2e) and later

CSCwa49263

In VPC route table, the route table entry for a destination CIDR pointing to transit gateway is sometimes missing when a quick delete and add of tenant or contract is done or when we move from transit gateway connect to legacy transit gateway solution. This happens only with legacy transit gateway solution in either of the cases.

This is a timing issue with the legacy transit gateway solution, where we create two transit gateways per hub network in a region. This can happen either if we move to legacy transit gateway solution or if legacy transit gateways are coming up for the first time.

These conditions result in deleting the route table entry for a given destination CIDR and adding back the same entry at the same time. Due to an issue with the AWS API which returns a deleted route table entry as a non deleted entry, Cloud APIC deletes the wrong entry.

25.0(2e) and later

CSCwa49534

Consider 3 regions (A, B, C), where both A and B have CSR in their region.

Transit gateway cross-region comes in picture when there are no local CSRs in a region(region 'A'). The traffic of region "A" will hit the local transit gateway. On this transit gateway's user VPC Route Table, there will be a 0/0 entry which will have next-hop pointing to the nearest regions (region "B") transit gateway peering attachment.

When the CSRs of region "B" go down, the 0/0 route in the region "A" transit gateway route table should now point to take the next hop to region "C" transit gateway peering.

But in this bug, this flip does not happen at all times, which leads to traffic loss.

25.0(2e) and later

CSCwa26716

An external endpoint group of type non internet cannot leak all routes(or public internet IPs) to a cloud endpoint group. This results in creating a static route in the Vnet route table to point to CSR NLB for that destination CIDR. If it's leak-all, we configure 0.0.0.0/0 to point to CSR NLB.

In case of Azure Cloud APIC we do not create a static route to internet, Azure does this implicitly by default. Cloud APIC only programs the rules. When a user creates a contract or route leak, Cloud APIC programs a static route to CSR NLB. This overrides the Azure implicit default route to internet if the CIDR overlaps with the pubic internet IPs, since the leak-all creates a 0.0.0.0/0 route to point to CSR NLB.

This is an invalid configuration unless the user intended it, i,e the user intended to login to the VM through it's private IP through CSR. If not the user has to leak specific routes to the cloud endpoint group.

25.0(2e) and later

CSCvz87367

Even though the preferred DNS server is taking into effect in Cloud APIC, the DNS server which was configured first continues to be the preferred DNS server.

25.0(2e) and later

CSCwa45047

This is not a functional issue. There will be no fault shown in the Cloud APIC UI if the border gateway protocol sessions of the transit gateway external connectivity are down.

25.0(2e) and later

CSCwa50116

This bug has been filed to evaluate the Cisco Cloud APIC against the vulnerability in the Apache Log4j Java library disclosed on December 9th, 2021.

Cisco has reviewed this product and concluded that it contains a vulnerable version of Apache Log4j and is affected by the following vulnerability:

CVE-2021-44228 - Apache Log4j2 JNDI features do not protect against attacker controlled LDAP and other JNDI related endpoints

This advisory is available at the following link:

https://tools.cisco.com/security/center/content/CiscoSecurityAdvisory/cisco-sa-apache-log4j-qRuKNEbd

25.0(1c) and 25.0(2e)

Resolved Issues

Click the bug ID to access the Bug Search tool and see additional information about the bug. The "Fixed In" column of the table specifies whether the bug was resolved in the base release or a patch release.

Bug ID

Description

Fixed in

CSCwa18353

The route between the internal and external VRF is not programmed on the CSR. It is expected to be configured on CSR as a part of the leakTo subnet configuration.

25.0(2f)

CSCwa50116

This bug has been filed to evaluate the Cisco Cloud APIC against the vulnerability in the Apache Log4j Java library disclosed on December 9th, 2021.

Cisco has reviewed this product and concluded that it contains a vulnerable version of Apache Log4j and is affected by the following vulnerability:

CVE-2021-44228 - Apache Log4j2 JNDI features do not protect against attacker controlled LDAP and other JNDI related endpoints

This advisory is available at the following link:

https://tools.cisco.com/security/center/content/CiscoSecurityAdvisory/cisco-sa-apache-log4j-qRuKNEbd

25.0(2f)

CSCvy42684

Importing a configuration into Cloud APIC 5.2 displays the following error: maximum buffer length exceeded.

25.0(2e)

Known Issues

Click the bug ID to access the Bug Search tool and see additional information about the bug. The "Exists In" column of the table specifies the 25.0(2) releases in which the bug exists. A bug might also exist in releases other than the 25.0(2) releases.

Bug ID

Description

Exists in

CSCvz49747

When delete followed by add operations of tenant or other resources (such as VPCs and contracts) are done within a short span of time, it is possible that the resource deployment in Google Cloud may get out of sync with the configuration on Cisco Cloud APIC. The likelihood of this happening is directly proportionate to the scale of configuration and how quickly the operations are done.

We may see resources either not created or not deleted on Google Cloud to match the user configuration on Cisco Cloud APIC.

25.0(1c) and later

CSCvy77233

Routes for subnets that are not yet configured in Google Cloud may become visible on an external device. When you configure routes to be advertised to an external device, but don't actually configure subnets in the cloud that you intend to advertise the routes for, those routes are still advertised.

Remote router may see routes that are advertised even when the subnets are not yet configured.

The traffic will get dropped because the subnets are not actually configured.

25.0(1c) and later

CSCvx98260

When delete followed by add operations of tenant or other resources (such as VPCs and contracts) are done within a short span of time, it is possible that the resource deployment in Google Cloud may get out of sync with the configuration on Cisco Cloud APIC. The likelihood of this happening is directly proportionate to the scale of configuration and how quickly the operations are done.

We may see resources either not created or not deleted on Google Cloud to match the user configuration on Cisco Cloud APIC.

25.0(1c) and later

CSCvo06626

When a cloudExtEpg matches on a 0/0 network and has a bi-directional contract with two cloud EPGs, such as cloudEpg1 and CloudEpg2, this can result in inadvertent communication between endpoints in cloudEpg1 and cloudEpg2 without a contract between the two EPGs themselves.

25.0(1c) and later

CSCvo55112

Logs are lost upon stopping the Cloud APIC instance.

25.0(1c) and later

CSCvo95998

There is traffic loss after a Cloud APIC upgrade. Traffic will eventually converge, but this could take a few minutes.

25.0(1c) and later

CSCvq11780

Creating VPN connections fail with the "invalidCidr" error in AWS or the "More than one connection having the same BGP setting is not allowed" error in Azure.

25.0(1c) and later

CSCvq76039

When a fault is raised in the Cloud APIC, the fault message will be truncated and will not include the entire cloud message description.

25.0(1c) and later

CSCvr01341

REST API access to the Cloud APIC becomes delayed after deleting a tenant with scaled EPGs and endpoints. The client needs to retry after receiving the error.

25.0(1c) and later

CSCvu05329

The Ctx Oper managed object is not deleted after the attachment is deleted.

25.0(1c) and later

CSCvu81355

Traffic gets dropped after downgrading to the 5.0(1) release. Cloud Services Router has incompatible configurations due to an issue with reading configurations using SSH.

25.0(1c) and later

CSCvu88006

On the Dashboard, fewer VNet peerings are shown than expected.

25.0(1c) and later

CSCvv81647

When an invalid Cloud Services Router license token is configured after initially configuring a valid token, the Cloud Services Router fails the license registration and keeps using the old valid token. This failure can only be found from the CSR event log.

25.0(1c) and later

CSCvw05821

Redirection and UDR does not take effect when traffic coming through an express route and destined to a service end point is redirected to a native load balancer or firewall.

25.0(1c) and later

CSCvw07392

Inter-site VxLAN traffic drops for a given VRF table when it is deleted and re-added. Packet capture on the CSR shows "Incomplete Adjacency" as follows:

Punt 1 Count Code Cause 1 10 Incomplete adjacency <<<<<<<

Drop 1 Count Code Cause 1 94 Ipv4NoAdj

25.0(1c) and later

CSCvw07781

There is complete traffic loss for 180 seconds.

25.0(1c) and later

CSCvw24376

Inter region traffic is black-holed after the delete trigger for contracts/filter. It was observed that the TGW entry pointing to the remote region TGW is missing for the destination routes. On further debugging it was found that post delete trigger as part of re-add flow, when a describe call is sent to AWS got a reply with the state of this entry as "active" because of which a new create request is not being sent.

25.0(1c) and later

CSCvw39814

Infra VPC subnet route table entry for 0.0.0.0/0 route with TGW attachment as nh, is left as a stale entry upon being undeployed. There is no functional impact. Upon being redeployed, this entry is updated with the correct TGW attachment ID as nh.

25.0(1c) and later

CSCvw40737

SSH to a virtual machine's public IP address fails, despite the NSG allowing the traffic inbound. SSH to the private IP address of the virtual machine from within the VNet works.

25.0(1c) and later

CSCvw40818

After upgrading Cloud APIC, the Cloud Services Routers will be upgraded in two batches. The even set of CSRs are triggered for upgrade first. After their upgrade is complete and all of the even CSRs are datapathReady, only then the odd set of CSRs will be triggered for upgrade. When even one of the upgrade of the even CSRs fail and they don't become datapathReady, the odd set of CSRs will not be triggered for upgrade. This is the behavior followed to avoid any traffic loss.

25.0(1c) and later

CSCvw48190

When Cloud APIC is restart, the VPN connection from a tenant's VNets will get deleted and re-created, one by one. This can be seen in the Azure activity logs. It should not impact traffic, as all connections are not deleted at the same time.

25.0(1c) and later

CSCvw49898

When the downgrading from the 5.2(1) release to the 5.0(2) release, traffic loss is expected until all of the CSRs are downgraded back to the 17.1 release. The traffic loss occurs because when the CSRs are getting downgraded to the 17.1 release, the CSR NIC1s will be in the backendPools and traffic from the spokes will still be forwarded to the native load balancer. The traffic gets blackholed until the CSRs get fully programmed with all the configurations in the 17.1 release.

25.0(1c) and later

CSCvw50918

Upon downgrading Cloud APIC, VPN connections between Cloud APIC and the cloud (AWS/Azure VPN gateway) will be deleted and re-created, causing traffic loss. Traffic loss is based on how quickly the VPN connections are deleted and re-created in AWS due to AWS throttling.

25.0(1c) and later

CSCvw51544

A user who is assigned a large number of security domains may not be able to create other Cisco ACI policies.

25.0(1c) and later

CSCvw55088

A user who is assigned a large number of security domains may not be able to create other Cisco ACI policies.

25.0(1c) and later

CSCvx91010

When TGW Connect is disabled, traffic loss is observed for about 8 minutes.

25.0(1c) and later

CSCvy10936

Downgrading Cisco Cloud APIC from release 5.2(1) to 5.1(2) may cause CSRs to not be downgraded. The CSR release for 5.2(1) is 17.3.2, and the CSR version for release 5.1(2) is 17.3.1. After the Cisco Cloud APIC downgrade, the CSR version should be downgraded to 17.3.1, but it will not happen due to this bug.

25.0(1c) and later

CSCvy12722

Loss of traffic between a cloud and Cisco ACI On-Premises deployment.

25.0(1c) and later

CSCvy13369

After upgrading AWS, infra vPC peering does not get deleted.

25.0(1c) and later

CSCvy19286

There is traffic loss after downgrading from 5.2(1) to 5.1(2).

25.0(1c) and later

CSCvy28890

There is a loss in SSH connectivity to the Cisco Cloud APIC across reboots. But, after a few minutes, the connection should come back and users will be able to SSH in to the Cisco Cloud APIC again.

25.0(1c) and later

CSCvy28896

There is an increase in the connector's memory utilization. All of the CSR workflows rerunning might happen even after the setup is in the steady state.

25.0(1c) and later

CSCvy30314

After upgrading the Cisco Cloud APIC, on the TGW route tables, the default route (0.0.0.0/0) does not point to infra VPC attachment or is missing. In this case, traffic intended to get forwarded to the CSR will be dropped or forwarded to an invalid next-hop.

25.0(1c) and later

CSCvy33435

There is intersite traffic loss when TGW Connect is enabled.

25.0(1c) and later

CSCvy34180

Cloud Intersite traffic is dropped due to the CSR in the cloud site not advertising the EVPN routes.

25.0(1c) and later

CSCvy45517

The Cisco Cloud APIC GUI shows the total allowed count for CtxProfile, VRF (fvCtx), EPGs, and contracts. These numbers have been validated only for Azure-based deployments. For AWS deployments, the numbers supported are much lower.

25.0(1c) and later

CSCvz20282

An upgrade to or downgrade from the Cloud APIC 5.2(1g) release to any release while using "Ignore Compatibility Check: no" will fail. The following fault is raised: "The upgrade has an upgrade status of Failed Due to Incompatible Desired Version."

25.0(1c) and later

Compatibility Information

This section lists the compatibility information for the Cisco Cloud APIC software. In addition to the information in this section, see the appropriate Cisco Application Policy Infrastructure Controller Release Notes and Cisco Multi-Site Orchestrator Release Notes for compatibility information for those products.

· Cloud APIC release 25.0(2) is compatible with Cisco Nexus Dashboard Orchestrator, release 3.6(1).

· Cloud APIC supports the following AWS regions:

o Asia Pacific (Hong Kong)

o Asia Paciﬁc (Mumbai)

o Asia Paciﬁc (Osaka-Local)

o Asia Paciﬁc (Seoul)

o Asia Paciﬁc (Singapore)

o Asia Paciﬁc (Sydney)

o Asia Paciﬁc (Tokyo)

o AWS GovCloud (US-Gov-West)

o Canada (Central)

o EU (Frankfurt)

o EU (Ireland)

o EU (London)

o EU (Milan)

o EU (Stockholm)

o South America (São Paulo)

o US East (N. Virginia)

o US East (Ohio)

o US West (N. California)

o US West (Oregon)

· Cloud APIC supports the following Azure regions:

o Australiacentral

o Australiacentral2

o Australiaeast

o Australiasoutheast

o Brazilsouth

o Canadacentral

o Canadaeast

o Centralindia

o Centralus

o Eastasia

o Eastus

o Eastus2

o Francecentral

o Germanywestcentral

o Japaneast

o Japanwest

o Koreacentral

o Koreasouth

o Northcentralus

o Northeurope

o Norwayeast

o Southafricanorth

o Southcentralus

o Southeastasia

o Southindia

o Switzerlandnorth

o Uaenorth

o Uksouth

o Ukwest

o Westcentralus

o Westeurope

o Westindia

o Westus

o Westus2

· Cloud APIC supports the following Azure Government cloud regions:

o US DoD Central

o US DoD East

o US Gov Arizona

o US Gov Texas

o US Gov Virginia

· Cloud APIC supports all Google Cloud regions.

Related Content

See the Cisco Cloud Application Policy Infrastructure Controller page for the documentation.

See the Cisco Application Policy Infrastructure Controller (APIC) page for the verified scability, Cisco Application Policy Infrastructure Controller (APIC), and Cisco Multi-Site Orchestrator (MSO) documentation.

The documentation includes installation, upgrade, configuration, programming, and troubleshooting guides, technical references, release notes, and knowledge base (KB) articles, as well as other documentation. KB articles provide information about a specific use case or a specific topic.

By using the "Choose a topic" and "Choose a document type" fields of the APIC documentation website, you can narrow down the displayed documentation list to make it easier to find the desired document.


Introduction

If you have a private cloud, you might run part of your workload on a public cloud. However, migrating workload to the public cloud requires working with a different cloud provider interface and learning different ways to set up connectivity and define security policies. Meeting these challenges can result in increased operational cost and loss of consistency. Cisco Cloud Application Policy Infrastructure Controller (APIC) can be used to solve the these problems by extending a Cisco Multi-Site fabric to Amazon Web Services (AWS) or Microsoft Azure public clouds. You can also mix AWS and Azure in your deployment.

This document describes the features, issues, and limitations for the Cisco Cloud APIC software. For the features, issues, and limitations for the Cisco APIC, see the appropriate Cisco Application Policy Infrastructure Controller Release Notes. For the features, issues, and limitations for the Cisco Multi-Site Orchestrator, see the appropriate Cisco Multi-Site Orchestrator Release Notes.

For more information about this product, see "Related Content."

Note: The documentation set for this product strives to use bias-free language. For the purposes of this documentation set, bias-free is defined as language that does not imply discrimination based on age, disability, gender, racial identity, ethnic identity, sexual orientation, socioeconomic status, and intersectionality. Exceptions may be present in the documentation due to language that is hardcoded in the user interfaces of the product software, language used based on RFP documentation, or language that is used by a referenced third-party product.

Date

Description

October 8, 2021

Added compatibility information with Nexus Dashboard Orchestrator (NDO).

September 21, 2021

Release 25.0(1c) became available.

Beginning with release 25.0(1c), the release numbering has changed for Cisco Cloud APIC. The sequential order of releases for Cisco Cloud APIC is as follows:

· 4.1(x) (support for AWS only)

· 4.2(x)

· 5.0(x)

· 5.1(x)

· 5.2(x)

· 25.0(x)

New Software Features

Feature

Description

Support for Google Cloud with Cisco Cloud APIC

Support is available for Google Cloud with Cisco Cloud APIC. As part of the support for Google Cloud with Cisco Cloud APIC, the following are supported:

· Support for external connectivity from Google Cloud to other external sites

· Support for configuring routing and security policies separately

For more information, see:

· Cisco Cloud APIC for Google Cloud User Guide, Release 25.0(x)

· Cisco Cloud APIC for Google Cloud Installation Guide, Release 25.0(x)


Support for external connectivity from AWS or Azure to other external sites

Support is available for external connectivity from AWS or Azure to other external sites.

For more information, see:

· Cisco Cloud APIC for AWS Installation Guide, Release 25.0(x)

· Cisco Cloud APIC for Azure Installation Guide, Release 25.0(x)

· Cisco Cloud APIC for AWS User Guide, Release 25.0(x)

· Cisco Cloud APIC for Azure User Guide, Release 25.0(x)

Support for configuring routing policies separately for AWS or Azure when configuring for external connectivity

Support is available for configuring routing policies separately for AWS or Azure, independent of security policies, between internal and external VRFs when configuring for external connectivity.

For more information, see:

· Cisco Cloud APIC for AWS Installation Guide, Release 25.0(x)

· Cisco Cloud APIC for Azure Installation Guide, Release 25.0(x)

· Cisco Cloud APIC for AWS User Guide, Release 25.0(x)

· Cisco Cloud APIC for Azure User Guide, Release 25.0(x)

Support for Prometheus Node Exporter on Cisco Cloud APIC

The Prometheus Node Exporter is supported on Cisco Cloud APIC.

For more information, see:

· Cisco Cloud APIC for AWS User Guide, Release 25.0(x)

· Cisco Cloud APIC for Azure User Guide, Release 25.0(x)

· Cisco Cloud APIC for Google Cloud User Guide, Release 25.0(x)

Changes in Behavior

There are no changes in behavior in this release.

Open Issues

Click the bug ID to access the Bug Search tool and see additional information about the bug. The "Exists In" column of the table specifies the 25.0(1) releases in which the bug exists. A bug might also exist in releases other than the 25.0(1) releases.

Bug ID

Description

Exists in

CSCvy07759

CSR upgrade banner is not updated after the upgrade is complete.

25.0(1c) and later

CSCvz31331

APIC REST APIs allows to create a cloudEPg that refers to an external VRF in the infra tenant.

This is disabled when configuring through the GUI and should be blocked in the backend as well.

25.0(1c) and later

CSCvz62225

When you scale up the number of CSRs or routers per region, some of the configurations may be missing on the newly created CSR. This issue happens randomly on the newly created CSRs, in this case tunnels or BGP sessions on the new CSRs may be down due to missing configuration.

25.0(1c) and later

CSCvz66172

Unable to get public IP addresses assigned to non Gig1 Interfaces of CSR. Gig1 Interface gets a public IP addresses.

25.0(1c) and later

CSCvz47166

When delete followed by add operations of tenant or other resources (such as VPCs and contracts) are done within a short span of time, it is possible that the resource deployment in Google Cloud may get out of sync with the configuration on Cisco Cloud APIC. The likelihood of this happening is directly proportionate to the scale of configuration and how quickly the operations are done.

We may see resources either not created or not deleted on Google Cloud to match the user configuration on Cisco Cloud APIC.

25.0(1c) and later

CSCvy97972

Cisco Cloud APIC in this release limits the number of regions where we can deploy the hubnetwork in order to establish external connectivity. When you attempt to deploy/configure hubnetwork in more than four regions, the configuration will be rejected with the following error:

Invalid Configuration CT_INTNETWORK_REGION_MAXIMUM: At present, there can be at most 4 cloudRegionName in cloudtemplateIntNetwork uni/tn-infra/infranetwork-default/intnetwork-default; current count = <total-hubnetwork-regions-attempted>

25.0(1c) and later

CSCvz17160

Customers are restricted to shorter key value pairs than they need to be.

25.0(1c) and later

CSCvz21771

VPN tunnels may not come up when the Cisco Cloud APIC configuration is posted via XML interface. This problem won't be seen/encountered when we use the Cisco Cloud APIC UI.

25.0(1c) and later

CSCvz26752

The Cisco Cloud APIC UI may display empty entries when routes are leaked to or from non-existing VRFs. For example, this is seen when a VRF is deleted and another VRF leaked one or more routes to that deleted VRF. The UI may indicate an empty value under the VRF to which the routes are leaked.

25.0(1c) and later

CSCvz41009

When delete followed by add operations of tenant or other resources (such as VPCs and contracts) are done within a short span of time, it is possible that the resource deployment in Google Cloud may get out of sync with the configuration on Cisco Cloud APIC. The likelihood of this happening is directly proportionate to the scale of configuration and how quickly the operations are done.

We may see resources either not created or not deleted on Google Cloud to match the user configuration on Cisco Cloud APIC.

25.0(1c) and later

CSCvz43324

The Cloud APIC REST APIs allow you to create a cloud EPG that refers to an External VRF in the infra tenant.

This is disabled on the UI and should be disabled through the REST API as well.

25.0(1c) and later

CSCvz46464

There may be some traffic loss encountered when Cisco Cloud APIC is rebooted and a new configuration is imported. The new configuration takes time to deploy as we achieve eventual consistency.

25.0(1c) and later

CSCvz47232

When we delete any configuration, it takes time to reach eventual consistency and clean up all the resources on the cloud. This may not happen when Cisco Cloud APIC is rebooted while the delete/cleanup operation is underway. A few resources may remain on the cloud if Cisco Cloud APIC is rebooted while the cleanup is in progress.

25.0(1c) and later

CSCvy89617

Cloud routers may not get created if external network objects are not configured. External network configuration is required for configuring cloud routers.

25.0(1c) and later

CSCvy94328

overlay-1 VRF in tenant "infra" shows up as one of the VRFs with which an external network is associated. However, Cisco Cloud APIC does not allow overlay-1 VRF to be associated with any external network. In this release, overlay-1 VRF shows up alongside other VRFs to be associated with an external network. It has no functional impact, but it gives an incorrect impression that we have a external network associated with overlay-1 VRF. This external network name is set to default and there are no other objects/MOs for this external network configured.

25.0(1c) and later

CSCvz38067

Incorrect DNS server is configured on Cisco Cloud APIC with Google Cloud. Though this is not directly used when deploying Cisco Cloud APIC with Google Cloud, an incorrect IP address is configured.

25.0(1c) and later

CSCvz11574

The cloud VRF egress route table is missing the route for 0.0.0.0/0 via the Internet Gateway (IGW), which leads to issues with ssh for VMs in the cloud VRF.

25.0(1c) and later

CSCvz52773

When performing a Cisco Cloud APIC upgrade (but not also performing a CSR upgrade), before the upgrade is finished and when the Cisco Cloud APIC is reconciling the CSR configurations, if you delete certain configurations and add the same configurations back (for example, if you delete a VRF and add the VRF back), a traffic drop may happen. Eventually it should recover.

25.0(1c) and later

CSCvz39389

After a clean reboot of Cisco Cloud APIC and an import of the configuration, a CSR might take around 45 minutes to re-establish the datapath readiness.

25.0(1c) and later

CSCvz40326

Routes to and from overlay-2 VRF may not be configured in the cloud deployment.

25.0(1c) and later

CSCvo30542

TACACS monitoring of the destination group is not supported through the GUI.

25.0(1c) and later

CSCvu64277

Stats seen on Cisco Cloud APIC are sometimes not in sync with Azure stats.

25.0(1c) and later

CSCvu66521

In the "Cloud Resources" section of the GUI, the names displayed in the "Name" column are not the same as the name of resources on the cloud. These are showing the Cloud APIC object names.

25.0(1c) and later

CSCvu72354

Adding an EPG endpoint selector fails with an error message saying the selector is already attached.

25.0(1c) and later

CSCvu78074

Route nextHop is not set to the redirect service node specified in the service graph.

25.0(1c) and later

CSCvv32664

When the CSR bandwidth needs to be increased, the user needs to undeploy all the CSRs in all the regions and redeploy with the desired bandwidth, which can cause traffic loss.

25.0(1c) and later

CSCvx16601

When the "AllowAll" flag is enabled on a service device such as a native load balancer or on the logical interface of a third party device, it is possible that to see some specific rules apart form a rule that allows all traffic from any source to any destination.

25.0(1c) and later

CSCvy06610

The eventmgr crashes when handling a fault triggered by a new cloud account.

25.0(1c) and later

CSCvy42684

Importing a configuration into Cloud APIC 5.2 displays the following error: maximum buffer length exceeded.

25.0(1c) and later

Resolved Issues

Click the bug ID to access the Bug Search tool and see additional information about the bug. The "Fixed In" column of the table specifies whether the bug was resolved in the base release or a patch release.

Bug ID

Description

Fixed in

CSCvy14025

The Next Hop Routing entry missing in the ER gateway route table for redirecting the traffic going from the consumer to the provider EPG to a service device.

25.0(1c)

CSCvy50245

Tunnels are down on one of the CSRs after terminating the CSR instance from the AWS Portal.

25.0(1c)

CSCvx67107

Third party firewalls and load balancers are not shown in the topology view.

25.0(1c)

Known Issues

Click the bug ID to access the Bug Search tool and see additional information about the bug. The "Exists In" column of the table specifies the 25.0(1) releases in which the bug exists. A bug might also exist in releases other than the 25.0(1) releases.

Bug ID

Description

Exists in

CSCvz49747

When delete followed by add operations of tenant or other resources (such as VPCs and contracts) are done within a short span of time, it is possible that the resource deployment in Google Cloud may get out of sync with the configuration on Cisco Cloud APIC. The likelihood of this happening is directly proportionate to the scale of configuration and how quickly the operations are done.

We may see resources either not created or not deleted on Google Cloud to match the user configuration on Cisco Cloud APIC.

25.0(1c) and later

CSCvy77233

Routes for subnets that are not yet configured in Google Cloud may become visible on an external device. When you configure routes to be advertised to an external device, but don't actually configure subnets in the cloud that you intend to advertise the routes for, those routes are still advertised.

Remote router may see routes that are advertised even when the subnets are not yet configured.

The traffic will get dropped because the subnets are not actually configured.

25.0(1c) and later

CSCvx98260

When delete followed by add operations of tenant or other resources (such as VPCs and contracts) are done within a short span of time, it is possible that the resource deployment in Google Cloud may get out of sync with the configuration on Cisco Cloud APIC. The likelihood of this happening is directly proportionate to the scale of configuration and how quickly the operations are done.

We may see resources either not created or not deleted on Google Cloud to match the user configuration on Cisco Cloud APIC.

25.0(1c) and later

CSCvo06626

When a cloudExtEpg matches on a 0/0 network and has a bi-directional contract with two cloud EPGs, such as cloudEpg1 and CloudEpg2, this can result in inadvertent communication between endpoints in cloudEpg1 and cloudEpg2 without a contract between the two EPGs themselves.

25.0(1c) and later

CSCvo55112

Logs are lost upon stopping the Cloud APIC instance.

25.0(1c) and later

CSCvo95998

There is traffic loss after a Cloud APIC upgrade. Traffic will eventually converge, but this could take a few minutes.

25.0(1c) and later

CSCvq11780

Creating VPN connections fail with the "invalidCidr" error in AWS or the "More than one connection having the same BGP setting is not allowed" error in Azure.

25.0(1c) and later

CSCvq76039

When a fault is raised in the Cloud APIC, the fault message will be truncated and will not include the entire cloud message description.

25.0(1c) and later

CSCvr01341

REST API access to the Cloud APIC becomes delayed after deleting a tenant with scaled EPGs and endpoints. The client needs to retry after receiving the error.

25.0(1c) and later

CSCvu05329

The Ctx Oper managed object is not deleted after the attachment is deleted.

25.0(1c) and later

CSCvu81355

Traffic gets dropped after downgrading to the 5.0(1) release. Cloud Services Router has incompatible configurations due to an issue with reading configurations using SSH.

25.0(1c) and later

CSCvu88006

On the Dashboard, fewer VNet peerings are shown than expected.

25.0(1c) and later

CSCvv81647

When an invalid Cloud Services Router license token is configured after initially configuring a valid token, the Cloud Services Router fails the license registration and keeps using the old valid token. This failure can only be found from the CSR event log.

25.0(1c) and later

CSCvw05821

Redirection and UDR does not take effect when traffic coming through an express route and destined to a service end point is redirected to a native load balancer or firewall.

25.0(1c) and later

CSCvw07392

Inter-site VxLAN traffic drops for a given VRF table when it is deleted and re-added. Packet capture on the CSR shows "Incomplete Adjacency" as follows:

Punt 1 Count Code Cause 1 10 Incomplete adjacency <<<<<<<

Drop 1 Count Code Cause 1 94 Ipv4NoAdj

25.0(1c) and later

CSCvw07781

There is complete traffic loss for 180 seconds.

25.0(1c) and later

CSCvw24376

Inter region traffic is black-holed after the delete trigger for contracts/filter. It was observed that the TGW entry pointing to the remote region TGW is missing for the destination routes. On further debugging it was found that post delete trigger as part of re-add flow, when a describe call is sent to AWS got a reply with the state of this entry as "active" because of which a new create request is not being sent.

25.0(1c) and later

CSCvw39814

Infra VPC subnet route table entry for 0.0.0.0/0 route with TGW attachment as nh, is left as a stale entry upon being undeployed. There is no functional impact. Upon being redeployed, this entry is updated with the correct TGW attachment ID as nh.

25.0(1c) and later

CSCvw40737

SSH to a virtual machine's public IP address fails, despite the NSG allowing the traffic inbound. SSH to the private IP address of the virtual machine from within the VNet works.

25.0(1c) and later

CSCvw40818

After upgrading Cloud APIC, the Cloud Services Routers will be upgraded in two batches. The even set of CSRs are triggered for upgrade first. After their upgrade is complete and all of the even CSRs are datapathReady, only then the odd set of CSRs will be triggered for upgrade. When even one of the upgrade of the even CSRs fail and they don't become datapathReady, the odd set of CSRs will not be triggered for upgrade. This is the behavior followed to avoid any traffic loss.

25.0(1c) and later

CSCvw48190

When Cloud APIC is restart, the VPN connection from a tenant's VNets will get deleted and re-created, one by one. This can be seen in the Azure activity logs. It should not impact traffic, as all connections are not deleted at the same time.

25.0(1c) and later

CSCvw49898

When the downgrading from the 5.2(1) release to the 5.0(2) release, traffic loss is expected until all of the CSRs are downgraded back to the 17.1 release. The traffic loss occurs because when the CSRs are getting downgraded to the 17.1 release, the CSR NIC1s will be in the backendPools and traffic from the spokes will still be forwarded to the native load balancer. The traffic gets blackholed until the CSRs get fully programmed with all the configurations in the 17.1 release.

25.0(1c) and later

CSCvw50918

Upon downgrading Cloud APIC, VPN connections between Cloud APIC and the cloud (AWS/Azure VPN gateway) will be deleted and re-created, causing traffic loss. Traffic loss is based on how quickly the VPN connections are deleted and re-created in AWS due to AWS throttling.

25.0(1c) and later

CSCvw51544

A user who is assigned a large number of security domains may not be able to create other Cisco ACI policies.

25.0(1c) and later

CSCvw55088

A user who is assigned a large number of security domains may not be able to create other Cisco ACI policies.

25.0(1c) and later

CSCvx91010

When TGW Connect is disabled, traffic loss is observed for about 8 minutes.

25.0(1c) and later

CSCvy10936

Downgrading Cisco Cloud APIC from release 5.2(1) to 5.1(2) may cause CSRs to not be downgraded. The CSR release for 5.2(1) is 17.3.2, and the CSR version for release 5.1(2) is 17.3.1. After the Cisco Cloud APIC downgrade, the CSR version should be downgraded to 17.3.1, but it will not happen due to this bug.

25.0(1c) and later

CSCvy12722

Loss of traffic between a cloud and Cisco ACI On-Premises deployment.

25.0(1c) and later

CSCvy13369

After upgrading AWS, infra vPC peering does not get deleted.

25.0(1c) and later

CSCvy19286

There is traffic loss after downgrading from 5.2(1) to 5.1(2).

25.0(1c) and later

CSCvy28890

There is a loss in SSH connectivity to the Cisco Cloud APIC across reboots. But, after a few minutes, the connection should come back and users will be able to SSH in to the Cisco Cloud APIC again.

25.0(1c) and later

CSCvy28896

There is an increase in the connector's memory utilization. All of the CSR workflows rerunning might happen even after the setup is in the steady state.

25.0(1c) and later

CSCvy30314

After upgrading the Cisco Cloud APIC, on the TGW route tables, the default route (0.0.0.0/0) does not point to infra VPC attachment or is missing. In this case, traffic intended to get forwarded to the CSR will be dropped or forwarded to an invalid next-hop.

25.0(1c) and later

CSCvy33435

There is intersite traffic loss when TGW Connect is enabled.

25.0(1c) and later

CSCvy34180

Cloud Intersite traffic is dropped due to the CSR in the cloud site not advertising the EVPN routes.

25.0(1c) and later

CSCvy45517

The Cisco Cloud APIC GUI shows the total allowed count for CtxProfile, VRF (fvCtx), EPGs, and contracts. These numbers have been validated only for Azure-based deployments. For AWS deployments, the numbers supported are much lower.

25.0(1c) and later

CSCvz20282

An upgrade to or downgrade from the Cloud APIC 5.2(1g) release to any release while using "Ignore Compatibility Check: no" will fail. The following fault is raised: "The upgrade has an upgrade status of Failed Due to Incompatible Desired Version."

25.0(1c) and later

Compatibility Information

This section lists the compatibility information for the Cisco Cloud APIC software. In addition to the information in this section, see the appropriate Cisco Application Policy Infrastructure Controller Release Notes and Cisco Multi-Site Orchestrator Release Notes for compatibility information for those products.

· Cloud APIC release 25.0(1) is compatible with Cisco Nexus Dashboard Orchestrator, release 3.(5).

· Cloud APIC does not support IPv6.

· AWS does not support using iBGP between a virtual gateway and a customer gateway.

· Cloud APIC supports the following AWS regions:

o Asia Paciﬁc (Mumbai)

o Asia Paciﬁc (Osaka-Local)

o Asia Paciﬁc (Seoul)

o Asia Paciﬁc (Singapore)

o Asia Paciﬁc (Sydney)

o Asia Paciﬁc (Tokyo)

o AWS GovCloud (US-Gov-West)

o Canada (Central)

o EU (Frankfurt)

o EU (Ireland)

o EU (London)

o EU (Stockholm)

o South America (São Paulo)

o US East (N. Virginia)

o US East (Ohio)

o US West (N. California)

o US West (Oregon)

· Cloud APIC supports the following Azure regions:

o Australiacentral

o Australiacentral2

o Australiaeast

o Australiasoutheast

o Brazilsouth

o Canadacentral

o Canadaeast

o Centralindia

o Centralus

o Eastasia

o Eastus

o Eastus2

o Francecentral

o Germanywestcentral

o Japaneast

o Japanwest

o Koreacentral

o Koreasouth

o Northcentralus

o Northeurope

o Norwayeast

o Southafricanorth

o Southcentralus

o Southeastasia

o Southindia

o Switzerlandnorth

o Uaenorth

o Uksouth

o Ukwest

o Westcentralus

o Westeurope

o Westindia

o Westus

o Westus2

· Cloud APIC supports the following Azure Government cloud regions:

o US DoD Central

o US DoD East

o US Gov Arizona

o US Gov Texas

o US Gov Virginia

· Cloud APIC supports all Google Cloud regions.

Related Content

See the Cisco Cloud Application Policy Infrastructure Controller page for the documentation.

See the Cisco Application Policy Infrastructure Controller (APIC) page for the verified scability, Cisco Application Policy Infrastructure Controller (APIC), and Cisco Multi-Site Orchestrator (MSO) documentation.

The documentation includes installation, upgrade, configuration, programming, and troubleshooting guides, technical references, release notes, and knowledge base (KB) articles, as well as other documentation. KB articles provide information about a specific use case or a specific topic.

By using the "Choose a topic" and "Choose a document type" fields of the APIC documentation website, you can narrow down the displayed documentation list to make it easier to find the desired document.


Introduction

If you have a private cloud, you might run part of your workload on a public cloud. However, migrating workload to the public cloud requires working with a different cloud provider interface and learning different ways to set up connectivity and define security policies. Meeting these challenges can result in increased operational cost and loss of consistency. Cisco Cloud Application Policy Infrastructure Controller (APIC) can be used to solve the these problems by extending a Cisco Multi-Site fabric to Amazon Web Services (AWS) or Microsoft Azure public clouds. You can also mix AWS and Azure in your deployment.

This document describes the features, issues, and limitations for the Cisco Cloud APIC software. For the features, issues, and limitations for the Cisco APIC, see the Cisco Application Policy Infrastructure Controller Release Notes, Release 5.2(1). For the features, issues, and limitations for the Cisco Multi-Site Orchestrator, see the Cisco Multi-Site Orchestrator Release Notes, Release 3.3(1).

For more information about this product, see "Related Content."

Note: The documentation set for this product strives to use bias-free language. For the purposes of this documentation set, bias-free is defined as language that does not imply discrimination based on age, disability, gender, racial identity, ethnic identity, sexual orientation, socioeconomic status, and intersectionality. Exceptions may be present in the documentation due to language that is hardcoded in the user interfaces of the product software, language used based on RFP documentation, or language that is used by a referenced third-party product.

Date

Description

October 8, 2021

Removed mention of the supported Cisco ACI product releases. Cloud APIC does not depend on any specific Cisco ACI product releases.

August 13, 2021

Removed open issues CSCvy30852, CSCvy07759, and CSCvy41881.

Moved open issues CSCvy12722, CSCvy28896, CSCvy28890, and CSCvw48190 to known issues.

August 6, 2021

Release 5.2(1h) became available. Added known issue CSCvz20282.

June 11, 2021

Added open issue CSCvy14025, CSCvy50245, and CSCvy42684.

June 7, 2021

Release 5.2(1g) became available.

New Software Features

Feature

Description

Support for importing existing Azure brownfield cloud VNets into Cisco Cloud APIC

This release provides support for importing existing brownfield Azure cloud VNets (VNets that were not configured through Cisco Cloud APIC) into Cisco Cloud APIC.

For more information, see Importing Existing Brownfield Azure Cloud VNets Into Cisco Cloud APIC.

Support for Amazon Web Services (AWS) Transit Gateway Connect in Cisco Cloud APIC

Support for Amazon Web Services (AWS) Transit Gateway Connect in Cisco Cloud APIC. By using the AWS Transit Gateway Connect feature:

· Only one AWS Transit Gateway is deployed per hub network per region

· Equal-cost multi-path (ECMP) routing is enabled to all the CSRs in a region

For more information, see Cisco Cloud APIC for AWS Installation Guide, Release 5.2(x).

Support for manual upgrades for CSRs, independent from the Cisco Cloud APIC upgrades

You can manually trigger an upgrade of the CSRs, independent of the Cisco Cloud APIC upgrades. Prior to release 5.2(1), the CSRs were upgraded automatically whenever you trigger an upgrade for the Cisco Cloud APIC.

For more information, see:

Cisco Cloud APIC for AWS Installation Guide, Release 5.2(x)

Cisco Cloud APIC for Azure Installation Guide, Release 5.2(x)

Support for communication with an external site from regions without a CSR in AWS

You can have communication with an external site in regions without a CSR in AWS.

For more information, see Cisco Cloud APIC for AWS User Guide, Release 5.2(x).

Support for private IP addresses for CSR interfaces and Cisco Cloud APIC in AWS

You can assign a private IP address to a Cisco Cloud Services Router (CSR) and Cisco Cloud APIC in AWS.

For more information, see:

Cisco Cloud APIC for AWS Installation Guide, Release 5.2(x)

Cisco Cloud APIC for AWS User Guide, Release 5.2(x)

Support for VNet peering across Azure Active Dirtectories (ADs) to allow spoke VNets to be deployed in different subscriptions in different Azure ADs

Support is now available for VNet peering across Azure ADs, so spoke VNets can be deployed in different subscriptions in different Azure ADs. In releases prior to release 5.2(1), you must deploy the infra (hub) and spoke VNets in the same Azure AD, but you can deploy the infra and spoke VNets in different subscriptions within the same Azure AD.

For more information, see Configuring VNet Peering for Cloud APIC for Azure.

Changes in Behavior

There are no changes in behavior in this release.

Open Issues

Click the bug ID to access the Bug Search tool and see additional information about the bug. The "Exists In" column of the table specifies the 5.2(1) releases in which the bug exists. A bug might also exist in releases other than the 5.2(1) releases.

Bug ID

Description

Exists in

CSCvo30542

TACACS monitoring of the destination group is not supported through the GUI.

5.2(1g) and later

CSCvu64277

Stats seen on Cisco Cloud APIC are sometimes not in sync with Azure stats.

5.2(1g) and later

CSCvu66521

In the "Cloud Resources" section of the GUI, the names displayed in the "Name" column are not the same as the name of resources on the cloud. These are showing the Cloud APIC object names.

5.2(1g) and later

CSCvu72354

Adding an EPG endpoint selector fails with an error message saying the selector is already attached.

5.2(1g) and later

CSCvu78074

Route nextHop is not set to the redirect service node specified in the service graph.

5.2(1g) and later

CSCvv32664

When the CSR bandwidth needs to be increased, the user needs to undeploy all the CSRs in all the regions and redeploy with the desired bandwidth, which can cause traffic loss.

5.2(1g) and later

CSCvx16601

When the "AllowAll" flag is enabled on a service device such as a native load balancer or on the logical interface of a third party device, it is possible that to see some specific rules apart form a rule that allows all traffic from any source to any destination.

5.2(1g) and later

CSCvx67107

Third party firewalls and load balancers are not shown in the topology view.

5.2(1g) and later

CSCvy06610

The eventmgr crashes when handling a fault triggered by a new cloud account.

5.2(1g) and later

CSCvy14025

The Next Hop Routing entry missing in the ER gateway route table for redirecting the traffic going from the consumer to the provider EPG to a service device.

5.2(1g) and later

CSCvy42684

Importing a configuration into Cloud APIC 5.2 displays the following error: maximum buffer length exceeded.

5.2(1g) and later

CSCvy50245

Tunnels are down on one of the CSRs after terminating the CSR instance from the AWS Portal.

5.2(1g) and later

Resolved Issues

Click the bug ID to access the Bug Search tool and see additional information about the bug. The "Fixed In" column of the table specifies whether the bug was resolved in the base release or a patch release.

Bug ID

Description

Fixed in

CSCvt52797

Some cloud-to-cloud tunnels are operationally down in external-facing CSRs.

5.2(1g)

CSCvt72525

Upon increasing the scale of Certificate Signing Requests (CSRs), a create subnet request fails and a fault is raised in the Cisco Cloud APIC.

5.2(1g)

CSCvt88137

Some of the TGW attachments to non-infra tenant VPCs might be deleted and not get recreated in the case of quickly enabling, disabling, and re-enabling the hub network to the CloudCtxProfile.

5.2(1g)

CSCvu72020

The GUI cannot properly display the service graph association in EPG communication, due to a mismatched tenant name.

5.2(1g)

CSCvw21595

When deploying a service graph on Cisco Cloud APIC, faults F3764 and F3763 appear. The ALB is deployed on AWS, but the security group is not.

5.2(1g)

CSCvw27056

A CSR's management Interface is down or inaccessible through SSH.

5.2(1g)

CSCvw36844

No UDR entries will be present if extEPG is consumer on a graph on which REDIRECT is enabled.

5.2(1g)

CSCvw57813

After a Cisco Cloud APIC upgrade, all of the Cloud Service Routers will be upgraded in two batches, even and odd. State of the cuurent batch of CSRs that are upgraded are persisted in the Cisco Cloud APIC. When the Cisco Cloud APIC is rebooted mid-upgrade of the CSRs, the state information of the upgrade will be lost. This results in the halt of the CSR upgrade process.

5.2(1g)

CSCvw58899

After a configuration import, an fvCtx managed object may have a different vrfIndex value. This would cause the configuration in the CSRs to be modified, thereby leading to traffic drops.

5.2(1g)

CSCvw60314

After upgrading one of the Cloud APIC sites to the 5.2(1) release, intersite traffic loss occurs. This is due to intersite IPSec tunnels being deleted and recreated by MSO. Traffic recovers in about 30 to 40 minutes after the tunnels are recreated.

5.2(1g)

CSCvx06278

Rules in the firewall may be missing for traffic that has a cloud site-external EPG as the consumer/source to a provider EPG/destination when redirect is enabled in the cloud graph.

5.2(1g)

Known Issues

Click the bug ID to access the Bug Search tool and see additional information about the bug. The "Exists In" column of the table specifies the 5.2(1) releases in which the bug exists. A bug might also exist in releases other than the 5.2(1) releases.

Bug ID

Description

Exists in

CSCvo06626

When a cloudExtEpg matches on a 0/0 network and has a bi-directional contract with two cloud EPGs, such as cloudEpg1 and CloudEpg2, this can result in inadvertent communication between endpoints in cloudEpg1 and cloudEpg2 without a contract between the two EPGs themselves.

5.2(1g) and later

CSCvo55112

Logs are lost upon stopping the Cloud APIC instance.

5.2(1g) and later

CSCvo95998

There is traffic loss after a Cloud APIC upgrade. Traffic will eventually converge, but this could take a few minutes.

5.2(1g) and later

CSCvq11780

Creating VPN connections fail with the "invalidCidr" error in AWS or the "More than one connection having the same BGP setting is not allowed" error in Azure.

5.2(1g) and later

CSCvq76039

When a fault is raised in the Cloud APIC, the fault message will be truncated and will not include the entire cloud message description.

5.2(1g) and later

CSCvr01341

REST API access to the Cloud APIC becomes delayed after deleting a tenant with scaled EPGs and endpoints. The client needs to retry after receiving the error.

5.2(1g) and later

CSCvu05329

The Ctx Oper managed object is not deleted after the attachment is deleted.

5.2(1g) and later

CSCvu81355

Traffic gets dropped after downgrading to the 5.0(1) release. Cloud Services Router has incompatible configurations due to an issue with reading configurations using SSH.

5.2(1g) and later

CSCvu88006

On the Dashboard, fewer VNet peerings are shown than expected.

5.2(1g) and later

CSCvv81647

When an invalid Cloud Services Router license token is configured after initially configuring a valid token, the Cloud Services Router fails the license registration and keeps using the old valid token. This failure can only be found from the CSR event log.

5.2(1g) and later

CSCvw05821

Redirection and UDR does not take effect when traffic coming through an express route and destined to a service end point is redirected to a native load balancer or firewall.

5.2(1g) and later

CSCvw07392

Inter-site VxLAN traffic drops for a given VRF table when it is deleted and re-added. Packet capture on the CSR shows "Incomplete Adjacency" as follows:

Punt 1 Count Code Cause 1 10 Incomplete adjacency <<<<<<<

Drop 1 Count Code Cause 1 94 Ipv4NoAdj

5.2(1g) and later

CSCvw07781

There is complete traffic loss for 180 seconds.

5.2(1g) and later

CSCvw24376

Inter region traffic is black-holed after the delete trigger for contracts/filter. It was observed that the TGW entry pointing to the remote region TGW is missing for the destination routes. On further debugging it was found that post delete trigger as part of re-add flow, when a describe call is sent to AWS got a reply with the state of this entry as "active" because of which a new create request is not being sent.

5.2(1g) and later

CSCvw39814

Infra VPC subnet route table entry for 0.0.0.0/0 route with TGW attachment as nh, is left as a stale entry upon being undeployed. There is no functional impact. Upon being redeployed, this entry is updated with the correct TGW attachment ID as nh.

5.2(1g) and later

CSCvw40737

SSH to a virtual machine's public IP address fails, despite the NSG allowing the traffic inbound. SSH to the private IP address of the virtual machine from within the VNet works.

5.2(1g) and later

CSCvw40818

After upgrading Cloud APIC, the Cloud Services Routers will be upgraded in two batches. The even set of CSRs are triggered for upgrade first. After their upgrade is complete and all of the even CSRs are datapathReady, only then the odd set of CSRs will be triggered for upgrade. When even one of the upgrade of the even CSRs fail and they don't become datapathReady, the odd set of CSRs will not be triggered for upgrade. This is the behavior followed to avoid any traffic loss.

5.2(1g) and later

CSCvw48190

When Cloud APIC is restart, the VPN connection from a tenant's VNets will get deleted and re-created, one by one. This can be seen in the Azure activity logs. It should not impact traffic, as all connections are not deleted at the same time.

5.2(1g) and later

CSCvw49898

When the downgrading from the 5.2(1) release to the 5.0(2) release, traffic loss is expected until all of the CSRs are downgraded back to the 17.1 release. The traffic loss occurs because when the CSRs are getting downgraded to the 17.1 release, the CSR NIC1s will be in the backendPools and traffic from the spokes will still be forwarded to the native load balancer. The traffic gets blackholed until the CSRs get fully programmed with all the configurations in the 17.1 release.

5.2(1g) and later

CSCvw50918

Upon downgrading Cloud APIC, VPN connections between Cloud APIC and the cloud (AWS/Azure VPN gateway) will be deleted and re-created, causing traffic loss. Traffic loss is based on how quickly the VPN connections are deleted and re-created in AWS due to AWS throttling.

5.2(1g) and later

CSCvw51544

A user who is assigned a large number of security domains may not be able to create other Cisco ACI policies.

5.2(1g) and later

CSCvw55088

A user who is assigned a large number of security domains may not be able to create other Cisco ACI policies.

5.2(1g) and later

CSCvx91010

When TGW Connect is disabled, traffic loss is observed for about 8 minutes.

5.2(1g) and later

CSCvy10936

Downgrading Cisco Cloud APIC from release 5.2(1) to 5.1(2) may cause CSRs to not be downgraded. The CSR release for 5.2(1) is 17.3.2, and the CSR version for release 5.1(2) is 17.3.1. After the Cisco Cloud APIC downgrade, the CSR version should be downgraded to 17.3.1, but it will not happen due to this bug.

5.2(1g) and later

CSCvy12722

Loss of traffic between a cloud and Cisco ACI On-Premises deployment.

5.2(1g) and later

CSCvy13369

After upgrading AWS, infra vPC peering does not get deleted.

5.2(1g) and later

CSCvy19286

There is traffic loss after downgrading from 5.2(1) to 5.1(2).

5.2(1g) and later

CSCvy28890

There is a loss in SSH connectivity to the Cisco Cloud APIC across reboots. But, after a few minutes, the connection should come back and users will be able to SSH in to the Cisco Cloud APIC again.

5.2(1g) and later

CSCvy28896

There is an increase in the connector's memory utilization. All of the CSR workflows rerunning might happen even after the setup is in the steady state.

5.2(1g) and later

CSCvy30314

After upgrading the Cisco Cloud APIC, on the TGW route tables, the default route (0.0.0.0/0) does not point to infra VPC attachment or is missing. In this case, traffic intended to get forwarded to the CSR will be dropped or forwarded to an invalid next-hop.

5.2(1g) and later

CSCvy33435

There is intersite traffic loss when TGW Connect is enabled.

5.2(1g) and later

CSCvy34180

Cloud Intersite traffic is dropped due to the CSR in the cloud site not advertising the EVPN routes.

5.2(1g) and later

CSCvy45517

The Cisco Cloud APIC GUI shows the total allowed count for CtxProfile, VRF (fvCtx), EPGs, and contracts. These numbers have been validated only for Azure-based deployments. For AWS deployments, the numbers supported are much lower.

5.2(1g) and later

CSCvz20282

An upgrade to or downgrade from the Cloud APIC 5.2(1g) release to any release while using "Ignore Compatibility Check: no" will fail. The following fault is raised: "The upgrade has an upgrade status of Failed Due to Incompatible Desired Version."

5.2(1g) and later

Compatibility Information

This section lists the compatibility information for the Cisco Cloud APIC software. In addition to the information in this section, see the Cisco Application Policy Infrastructure Controller Release Notes, Release 5.2(1) and Cisco Multi-Site Orchestrator Release Notes, Release 3.3(1) for compatibility information for those products.

· Cloud APIC release 5.2(1) is compatible with Multi-Site Orchestrator, release 3.3(1) or above.

· Cloud APIC does not support IPv6.

· AWS does not support using iBGP between a virtual gateway and a customer gateway.

· Cloud APIC supports the following AWS regions:

o Asia Paciﬁc (Mumbai)

o Asia Paciﬁc (Osaka-Local)

o Asia Paciﬁc (Seoul)

o Asia Paciﬁc (Singapore)

o Asia Paciﬁc (Sydney)

o Asia Paciﬁc (Tokyo)

o AWS GovCloud (US-Gov-West)

o Canada (Central)

o EU (Frankfurt)

o EU (Ireland)

o EU (London)

o South America (São Paulo)

o US East (N. Virginia)

o US East (Ohio)

o US West (N. California)

o US West (Oregon)

· Cloud APIC supports the following Azure regions:

o Australiacentral

o Australiacentral2

o Australiaeast

o Australiasoutheast

o Brazilsouth

o Canadacentral

o Canadaeast

o Centralindia

o Centralus

o Eastasia

o Eastus

o Eastus2

o Francecentral

o Japaneast

o Japanwest

o Koreacentral

o Koreasouth

o Northcentralus

o Northeurope

o Southcentralus

o Southeastasia

o Southindia

o Uksouth

o Ukwest

o Westcentralus

o Westeurope

o Westindia

o Westus

o Westus2

· Cloud APIC supports the following Azure Government cloud regions:

o US DoD Central

o US DoD East

o US Gov Arizona

o US Gov Texas

o US Gov Virginia

Related Content

See the Cisco Cloud Application Policy Infrastructure Controller page for the documentation.

See the Cisco Application Policy Infrastructure Controller (APIC) page for the verified scability, Cisco Application Policy Infrastructure Controller (APIC), and Cisco Multi-Site Orchestrator (MSO) documentation.

The documentation includes installation, upgrade, configuration, programming, and troubleshooting guides, technical references, release notes, and knowledge base (KB) articles, as well as other documentation. KB articles provide information about a specific use case or a specific topic.

By using the "Choose a topic" and "Choose a document type" fields of the APIC documentation website, you can narrow down the displayed documentation list to make it easier to find the desired document.


Introduction

If you have a private cloud, you might run part of your workload on a public cloud. However, migrating workload to the public cloud requires working with a different cloud provider interface and learning different ways to set up connectivity and define security policies. Meeting these challenges can result in increased operational cost and loss of consistency. Cisco Cloud Application Policy Infrastructure Controller (APIC) can be used to solve the these problems by extending a Cisco Multi-Site fabric to Amazon Web Services (AWS) or Microsoft Azure public clouds. You can also mix AWS and Azure in your deployment.

This document describes the features, issues, and limitations for the Cisco Cloud APIC software. For the features, issues, and limitations for the Cisco APIC, see the Cisco Application Policy Infrastructure Controller Release Notes, Release 5.1(3). For the features, issues, and limitations for the Cisco Multi-Site Orchestrator, see the Cisco Multi-Site Orchestrator Release Notes, Release 3.2(1).

For more information about this product, see "Related Content."

Note: The documentation set for this product strives to use bias-free language. For the purposes of this documentation set, bias-free is defined as language that does not imply discrimination based on age, disability, gender, racial identity, ethnic identity, sexual orientation, socioeconomic status, and intersectionality. Exceptions may be present in the documentation due to language that is hardcoded in the user interfaces of the product software, language used based on RFP documentation, or language that is used by a referenced third-party product.

Date

Description

February 12, 2021

Moved bug CSCvw49898 from the Open Issues table to the Known Issues table.

Added bug CSCvt88137 to the Open Issues table.

February 1, 2021

Release 5.1(3e) became available.

New Software Features

Feature

Description

Ability to map network security groups (NSGs) to subnets

NSGs are now mapped to subnets instead of EPGs. Prior to release 5.1(3), NSGs were mapped to EPGs.

For more information, see the Cisco Cloud APIC for Azure User Guide, Release 5.1(x).

Support for non-Cisco ACI on-premises destinations over express route

Support is now available to represent non-Cisco ACI on-premises destinations that are reachable over an Azure express route as site-ext EPGs.

For more information, see the Cisco Cloud APIC for Azure User Guide, Release 5.1(x).

Support for 40G throughput with Cisco Cloud Services Routers by increasing the maximum number of supported CSRs to 8

Up to 40G throughput is supported by version 17.3 Cisco cloud services routers. The maximum throughput for a CSR depends on the instance type. 40G throughput can be achieved because the maximum number of CSRs supported per region has been increased from 4 to 8.

For more information, see the Cisco Cloud APIC for Azure User Guide, Release 5.1(x) the Cisco Cloud APIC for Azure Installation Guide, Release 5.1(x).

Cloud services support using cloud service EPGs

You can now automate network segmentation and security policies for cloud native services and third-party services using the new cloud service EPG.

For more information, see the Cisco Cloud APIC for Azure User Guide, Release 5.1(x).

Support for assigning only a private IP address to access a Cisco Cloud Services Router

Assigning a public IP address to the Cloud APIC and Cisco Cloud Services Router (CSR) is now optional. Cloud APIC and Cisco CSRs can now operate with only a private IP address.

For more information, see the Cisco Cloud APIC for Azure User Guide, Release 5.1(x).

Restricting access using restricted security domains

Security has been enhanced for a user by restricting access using restricted security domains.

For more information, see the Cisco Cloud APIC for Azure User Guide, Release 5.1(x).

Support for third-party load balancers

A third party load-balancer is a non-cloud native Layer 4 to Layer 7 services load balancer. You can now use third-party load balancers for Azure deployments.

For more information, see the Cisco Cloud APIC for Azure User Guide, Release 5.1(x).

Layer 4 to Layer 7 services redirect support for cloud native and third-party services

The Layer 4 to Layer 7 service redirect feature for Cisco Cloud APIC now supports cloud native and third-party services.

For more information, see the Cisco Cloud APIC for Azure User Guide, Release 5.1(x).

Support for Layer 4 to Layer 7 services redirect and non-redirect service graphs for site-ext EPGs over an express route

You can now use Layer 4 to Layer 7 services redirect and non-redirect service graphs for site-ext EPGs over an express route.

Support for segmentation using multiple VRF tables per VNet

Segmentation is now supported using multiple VRF tables per Azure virtual network (VNet).

Support for custom naming for Layer 4 to Layer 7 service deployments

You can now provide custom names to cloud resources, such as network load balancers, application load balancers, and device application security groups.

Changes in Behavior

There are no changes in behavior in this release.

Open Issues

Click the bug ID to access the Bug Search tool and see additional information about the bug. The "Exists In" column of the table specifies the 5.1(3) releases in which the bug exists. A bug might also exist in releases other than the 5.1(3) releases.

Bug ID

Description

Exists in

CSCvo30542

TACACS monitoring of the destination group is not supported through the GUI.

5.1(3e) and later

CSCvt52797

Some cloud-to-cloud tunnels are operationally down in external-facing CSRs.

5.1(3e) and later

CSCvt72525

Upon increasing the scale of Certificate Signing Requests (CSRs), a create subnet request fails and a fault is raised in the Cisco Cloud APIC.

5.1(3e) and later

CSCvt88137

Some of the TGW attachments to non-infra tenant VPCs might be deleted and not get recreated in the case of quickly enabling, disabling, and re-enabling the hub network to the CloudCtxProfile.

5.1(3e) and later

CSCvu64277

Stats seen on Cisco Cloud APIC are sometimes not in sync with Azure stats.

5.1(3e) and later

CSCvu66521

In the "Cloud Resources" section of the GUI, the names displayed in the "Name" column are not the same as the name of resources on the cloud. These are showing the Cloud APIC object names.

5.1(3e) and later

CSCvu72020

The GUI cannot properly display the service graph association in EPG communication, due to a mismatched tenant name.

5.1(3e) and later

CSCvu72354

Adding an EPG endpoint selector fails with an error message saying the selector is already attached.

5.1(3e) and later

CSCvu78074

Route nextHop is not set to the redirect service node specified in the service graph.

5.1(3e) and later

CSCvv32664

When the CSR bandwidth needs to be increased, the user needs to undeploy all the CSRs in all the regions and redeploy with the desired bandwidth, which can cause traffic loss.

5.1(3e) and later

CSCvv81647

When an invalid Cloud Services Router license token is configured after initially configuring a valid token, the Cloud Services Router fails the license registration and keeps using the old valid token. This failure can only be found from the CSR event log.

5.1(3e) and later

CSCvw07392

Inter-site VxLAN traffic drops for a given VRF table when it is deleted and re-added. Packet capture on the CSR shows "Incomplete Adjacency" as follows:

Punt 1 Count Code Cause 1 10 Incomplete adjacency <<<<<<<

Drop 1 Count Code Cause 1 94 Ipv4NoAdj

5.1(3e) and later

CSCvw21595

When deploying a service graph on Cisco Cloud APIC, faults F3764 and F3763 appear. The ALB is deployed on AWS, but the security group is not.

5.1(3e) and later

CSCvw24376

Inter region traffic is black-holed after the delete trigger for contracts/filter. It was observed that the TGW entry pointing to the remote region TGW is missing for the destination routes. On further debugging it was found that post delete trigger as part of re-add flow, when a describe call is sent to AWS got a reply with the state of this entry as "active" because of which a new create request is not being sent.

5.1(3e) and later

CSCvw27056

A CSR's management Interface is down or inaccessible through SSH.

5.1(3e) and later

CSCvw36844

No UDR entries will be present if extEPG is consumer on a graph on which REDIRECT is enabled.

5.1(3e) and later

CSCvw40737

SSH to a virtual machine's public IP address fails, despite the NSG allowing the traffic inbound. SSH to the private IP address of the virtual machine from within the VNet works.

5.1(3e) and later

CSCvw48190

When Cloud APIC is restart, the VPN connection from a tenant's VNets will get deleted and re-created, one by one. This can be seen in the Azure activity logs. It should not impact traffic, as all connections are not deleted at the same time.

5.1(3e) and later

CSCvw48256


The primary CIDR of CtxProfile defines the VPC/VNet and hence cannot be changed/modified without deleting the vPC. For example, in the AWS console, you cannot simply modify or delete the primary CIDR. You must delete the entire vPC. However, in the Cisco ACI policy, a configuration post that modifies the primary CIDR will lead to an incorrect state of creating a new vPC while not deleting the old one.

Because the GUI does not allow the primary CIDR change operation (that is, deselecting the primary option from one CIDR and selecting the primary CIDR box for the other CIDR), this problem will not happen if the operation is performed using GUI.

If the operation is done using the REST API, a fault is raised for the new primary CIDR and the related subnet is not deployed in the cloud. The old CIDR is not cleaned up as well.

5.1(3e) and later

CSCvw55088

A user who is assigned a large number of security domains may not be able to create other Cisco ACI policies.

5.1(3e) and later

CSCvw57813

After a Cisco Cloud APIC upgrade, all of the Cloud Service Routers will be upgraded in two batches, even and odd. State of the cuurent batch of CSRs that are upgraded are persisted in the Cisco Cloud APIC. When the Cisco Cloud APIC is rebooted mid-upgrade of the CSRs, the state information of the upgrade will be lost. This results in the halt of the CSR upgrade process.

5.1(3e) and later

CSCvw58899

After a configuration import, an fvCtx managed object may have a different vrfIndex value. This would cause the configuration in the CSRs to be modified, thereby leading to traffic drops.

5.1(3e) and later

CSCvw60314

After upgrading one of the Cloud APIC sites to the 5.1(3) release, intersite traffic loss occurs. This is due to intersite IPSec tunnels being deleted and recreated by MSO. Traffic recovers in about 30 to 40 minutes after the tunnels are recreated.

5.1(3e) and later

CSCvx06278

Rules in the firewall may be missing for traffic that has a cloud site-external EPG as the consumer/source to a provider EPG/destination when redirect is enabled in the cloud graph.

5.1(3e) and later

Resolved Issues

Click the bug ID to access the Bug Search tool and see additional information about the bug. The "Fixed In" column of the table specifies whether the bug was resolved in the base release or a patch release.

Bug ID

Description

Fixed in

CSCvw51219

In a rare scenario, in a Cisco ACI Multi-Pod fabric that has one pod with multiple spine switches, if one spine switche is power cycled due to a cold reboot (power cable removal/power failure) or gets clean reloaded, then after the spine switche has come back up, sometimes a few of the remote endpoint entries become missing from the spine switches.

5.1(3e)

Known Issues

Click the bug ID to access the Bug Search tool and see additional information about the bug. The "Exists In" column of the table specifies the 5.1(3) releases in which the bug exists. A bug might also exist in releases other than the 5.1(3) releases.

Bug ID

Description

Exists in

CSCvo06626

When a cloudExtEpg matches on a 0/0 network and has a bi-directional contract with two cloud EPGs, such as cloudEpg1 and CloudEpg2, this can result in inadvertent communication between endpoints in cloudEpg1 and cloudEpg2 without a contract between the two EPGs themselves.

5.1(3e) and later

CSCvo55112

Logs are lost upon stopping the Cloud APIC instance.

5.1(3e) and later

CSCvo95998

There is traffic loss after a Cloud APIC upgrade. Traffic will eventually converge, but this could take a few minutes.

5.1(3e) and later

CSCvq11780

Creating VPN connections fail with the "invalidCidr" error in AWS or the "More than one connection having the same BGP setting is not allowed" error in Azure.

5.1(3e) and later

CSCvq76039

When a fault is raised in the Cloud APIC, the fault message will be truncated and will not include the entire cloud message description.

5.1(3e) and later

CSCvr01341

REST API access to the Cloud APIC becomes delayed after deleting a tenant with scaled EPGs and endpoints. The client needs to retry after receiving the error.

5.1(3e) and later

CSCvu81355

Traffic gets dropped after downgrading to the 5.0(1) release. Cloud Services Router has incompatible configurations due to an issue with reading configurations using SSH.

5.1(3e) and later

CSCvu88006

On the Dashboard, fewer VNet peerings are shown than expected.

5.1(3e) and later

CSCvw05821

Redirection and UDR does not take effect when traffic coming through an express route and destined to a service end point is redirected to a native load balancer or firewall.

5.1(3e) and later

CSCvw39814

Infra VPC subnet route table entry for 0.0.0.0/0 route with TGW attachment as nh, is left as a stale entry upon being undeployed. There is no functional impact. Upon being redeployed, this entry is updated with the correct TGW attachment ID as nh.

5.1(3e) and later

CSCvw40818

After upgrading Cloud APIC, the Cloud Services Routers will be upgraded in two batches. The even set of CSRs are triggered for upgrade first. After their upgrade is complete and all of the even CSRs are datapathReady, only then the odd set of CSRs will be triggered for upgrade. When even one of the upgrade of the even CSRs fail and they don't become datapathReady, the odd set of CSRs will not be triggered for upgrade. This is the behavior followed to avoid any traffic loss.

5.1(3e) and later

CSCvw49898

When the downgrading from the 5.1(3) release to the 5.0(2) release, traffic loss is expected until all of the CSRs are downgraded back to the 17.1 release. The traffic loss occurs because when the CSRs are getting downgraded to the 17.1 release, the CSR NIC1s will be in the backendPools and traffic from the spokes will still be forwarded to the native load balancer. The traffic gets blackholed until the CSRs get fully programmed with all the configurations in the 17.1 release.

5.1(3e) and later

CSCvw50918

Upon downgrading Cloud APIC, VPN connections between Cloud APIC and the cloud (AWS/Azure VPN gateway) will be deleted and re-created, causing traffic loss. Traffic loss is based on how quickly the VPN connections are deleted and re-created in AWS due to AWS throttling.

5.1(3e) and later

CSCvw51544

A user who is assigned a large number of security domains may not be able to create other Cisco ACI policies.

5.1(3e) and later

Compatibility Information

This section lists the compatibility information for the Cisco Cloud APIC software. In addition to the information in this section, see the Cisco Application Policy Infrastructure Controller Release Notes, Release 5.1(3) and Cisco Multi-Site Orchestrator Release Notes, Release 3.2(1) for compatibility information for those products.

· Cloud APIC release 5.1(3) supports the following Cisco Application Centric Infrastructure (ACI) product releases:

o Cisco Multi-Site Orchestrator, release 3.2(1)

o Cisco APIC, release 5.1(3)

o Cisco NX-OS for ACI-mode switches, release 15.1(3)

· Cloud APIC does not support IPv6.

· AWS does not support using iBGP between a virtual gateway and a customer gateway.

· Cloud APIC supports the following AWS regions:

o Asia Paciﬁc (Mumbai)

o Asia Paciﬁc (Osaka-Local)

o Asia Paciﬁc (Seoul)

o Asia Paciﬁc (Singapore)

o Asia Paciﬁc (Sydney)

o Asia Paciﬁc (Tokyo)

o AWS GovCloud (US-Gov-West)

o Canada (Central)

o EU (Frankfurt)

o EU (Ireland)

o EU (London)

o South America (São Paulo)

o US East (N. Virginia)

o US East (Ohio)

o US West (N. California)

o US West (Oregon)

· Cloud APIC supports the following Azure regions:

o Australiacentral

o Australiacentral2

o Australiaeast

o Australiasoutheast

o Brazilsouth

o Canadacentral

o Canadaeast

o Centralindia

o Centralus

o Eastasia

o Eastus

o Eastus2

o Francecentral

o Japaneast

o Japanwest

o Koreacentral

o Koreasouth

o Northcentralus

o Northeurope

o Southcentralus

o Southeastasia

o Southindia

o Uksouth

o Ukwest

o Westcentralus

o Westeurope

o Westindia

o Westus

o Westus2

· Cloud APIC supports the following Azure Government cloud regions:

o US DoD Central

o US DoD East

o US Gov Arizona

o US Gov Texas

o US Gov Virginia

Related Content

See the Cisco Cloud Application Policy Infrastructure Controller page for the documentation.

See the Cisco Application Policy Infrastructure Controller (APIC) page for the verified scability, Cisco Application Policy Infrastructure Controller (APIC), and Cisco Multi-Site Orchestrator (MSO) documentation.

The documentation includes installation, upgrade, configuration, programming, and troubleshooting guides, technical references, release notes, and knowledge base (KB) articles, as well as other documentation. KB articles provide information about a specific use case or a specific topic.

By using the "Choose a topic" and "Choose a document type" fields of the APIC documentation website, you can narrow down the displayed documentation list to make it easier to find the desired document.


Introduction

If you have a private cloud, you might run part of your workload on a public cloud. However, migrating workload to the public cloud requires working with a different cloud provider interface and learning different ways to set up connectivity and define security policies. Meeting these challenges can result in increased operational cost and loss of consistency. Cisco Cloud Application Policy Infrastructure Controller (APIC) can be used to solve the these problems by extending a Cisco Application Centric Infrastructure (ACI) Multi-Site fabric to Amazon Web Services (AWS) or Microsoft Azure public clouds. You can also mix AWS and Azure in your deployment.

This document describes the features, issues, and limitations for the Cisco Cloud APIC software. For the features, issues, and limitations for the Cisco APIC, see the Cisco Application Policy Infrastructure Controller Release Notes, Release 5.1(2). For the features, issues, and limitations for the Cisco ACI Multi-Site Orchestrator, see the Cisco ACI Multi-Site Orchestrator Release Notes, Release 3.1(1).

For more information about this product, see "Related Content."

Note: The documentation set for this product strives to use bias-free language. For the purposes of this documentation set, bias-free is defined as language that does not imply discrimination based on age, disability, gender, racial identity, ethnic identity, sexual orientation, socioeconomic status, and intersectionality. Exceptions may be present in the documentation due to language that is hardcoded in the user interfaces of the product software, language used based on RFP documentation, or language that is used by a referenced third-party product.

Date

Description

February 12, 2021

Release 5.1(2g) became available. Added the open and known issues and new software features for this release.

From the Open Issues section, removed bug CSCvw51219. This bug was erroneously included.

Moved bug CSCvw49898 from the Open Issues table to the Known Issues table.

January 20, 2021

In the Open Issues section, added bug CSCvx06278.

November 27, 2020

Release 5.1(2e) became available.

New Software Features

Feature

Description

Allow All Traffic option is available for third-party firewalls and Azure network load balancers

Beginning with release 5.1(2g), the Allow All Traffic option is available for third-party firewalls and Azure network load balancers deployed as pass-through devices on a redirect-enabled service graph.

IP-based rules for inter-VRF contracts in the same VNet

Beginning with release 5.1(2g), if two EPGs have a contract and are in the same VNet but belong to different VRFs, IP-based rules are now used to enable communication between those hosted VRFs in that VNet.

Prior to release 5.1(2g), if two EPGs had a contract and were in the same VNet, but belonged to different VRFs, ASG-based rules were used to enable communication between those hosted VRFs in that VNet.

Ability to map network security groups (NSGs) to subnets

NSGs are now mapped to subnets instead of EPGs. Prior to release 5.1(2), NSGs were mapped to EPGs.

For more information, see the Cisco Cloud APIC for Azure User Guide, Release 5.1(x).

Support for non-Cisco ACI on-premises destinations over express route

Support is now available to represent non-Cisco ACI on-premises destinations that are reachable over an Azure express route as site-ext EPGs.

For more information, see the Cisco Cloud APIC for Azure User Guide, Release 5.1(x).

Support for 40G throughput with Cisco Cloud Services Routers by increasing the maximum number of supported CSRs to 8

Up to 40G throughput is supported by version 17.3 Cisco cloud services routers. The maximum throughput for a CSR depends on the instance type. 40G throughput can be achieved because the maximum number of CSRs supported per region has been increased from 4 to 8.

For more information, see the Cisco Cloud APIC for Azure User Guide, Release 5.1(x) the Cisco Cloud APIC for Azure Installation Guide, Release 5.1(x).

Cloud services support using cloud service EPGs

You can now automate network segmentation and security policies for cloud native services and third-party services using the new cloud service EPG.

For more information, see the Cisco Cloud APIC for Azure User Guide, Release 5.1(x).

Support for assigning only a private IP address to access a Cisco Cloud Services Router

Assigning a public IP address to the Cloud APIC and Cisco Cloud Services Router (CSR) is now optional. Cloud APIC and Cisco CSRs can now operate with only a private IP address.

For more information, see the Cisco Cloud APIC for Azure User Guide, Release 5.1(x).

Restricting access using restricted security domains

Security has been enhanced for a user by restricting access using restricted security domains.

For more information, see the Cisco Cloud APIC for Azure User Guide, Release 5.1(x).

Support for third-party load balancers

A third party load-balancer is a non-cloud native Layer 4 to Layer 7 services load balancer. You can now use third-party load balancers for Azure deployments.

For more information, see the Cisco Cloud APIC for Azure User Guide, Release 5.1(x).

Layer 4 to Layer 7 services redirect support for cloud native and third-party services

The Layer 4 to Layer 7 service redirect feature for Cisco Cloud APIC now supports cloud native and third-party services.

For more information, see the Cisco Cloud APIC for Azure User Guide, Release 5.1(x).

Support for Layer 4 to Layer 7 services redirect and non-redirect service graphs for site-ext EPGs over an express route

You can now use Layer 4 to Layer 7 services redirect and non-redirect service graphs for site-ext EPGs over an express route.

Support for segmentation using multiple VRF tables per VNet

Segmentation is now supported using multiple VRF tables per Azure virtual network (VNet).

Support for custom naming for Layer 4 to Layer 7 service deployments

You can now provide custom names to cloud resources, such as network load balancers, application load balancers, and device application security groups.

Changes in Behavior

· Beginning with release 5.1(2), VNet peering is enabled by default at the global level and cannot be disabled. Prior to release 5.1(2), you could enable and disable VNet peering at the global level through the First Time Setup window.

Open Issues

Click the bug ID to access the Bug Search tool and see additional information about the bug. The "Exists In" column of the table specifies the 5.1(2) releases in which the bug exists. A bug might also exist in releases other than the 5.1(2) releases.

Bug ID

Description

Exists in

CSCvo30542

TACACS monitoring of the destination group is not supported through the GUI.

5.1(2e) and later

CSCvt52797

Some cloud-to-cloud tunnels are operationally down in external-facing CSRs.

5.1(2e) and later

CSCvt72525

Upon increasing the scale of Certificate Signing Requests (CSRs), a create subnet request fails and a fault is raised in the Cisco Cloud APIC.

5.1(2e) and later

CSCvt88137

Some of the TGW attachments to non-infra tenant VPCs might be deleted and not get recreated in the case of quickly enabling, disabling, and re-enabling the hub network to the CloudCtxProfile.

5.1(2e) and later

CSCvu64277

Stats seen on Cisco Cloud APIC are sometimes not in sync with Azure stats.

5.1(2e) and later

CSCvu66521

In the "Cloud Resources" section of the GUI, the names displayed in the "Name" column are not the same as the name of resources on the cloud. These are showing the Cloud APIC object names.

5.1(2e) and later

CSCvu72020

The GUI cannot properly display the service graph association in EPG communication, due to a mismatched tenant name.

5.1(2e) and later

CSCvu72354

Adding an EPG endpoint selector fails with an error message saying the selector is already attached.

5.1(2e) and later

CSCvu78074

Route nextHop is not set to the redirect service node specified in the service graph.

5.1(2e) and later

CSCvv32664

When the CSR bandwidth needs to be increased, the user needs to undeploy all the CSRs in all the regions and redeploy with the desired bandwidth, which can cause traffic loss.

5.1(2e) and later

CSCvv81647

When an invalid Cloud Services Router license token is configured after initially configuring a valid token, the Cloud Services Router fails the license registration and keeps using the old valid token. This failure can only be found from the CSR event log.

5.1(2e) and later

CSCvw07392

Inter-site VxLAN traffic drops for a given VRF table when it is deleted and re-added. Packet capture on the CSR shows "Incomplete Adjacency" as follows:

Punt 1 Count Code Cause 1 10 Incomplete adjacency <<<<<<<

Drop 1 Count Code Cause 1 94 Ipv4NoAdj

5.1(2e) and later

CSCvw21595

When deploying a service graph on Cisco Cloud APIC, faults F3764 and F3763 appear. The ALB is deployed on AWS, but the security group is not.

5.1(2e) and later

CSCvw24376

Inter region traffic is black-holed after the delete trigger for contracts/filter. It was observed that the TGW entry pointing to the remote region TGW is missing for the destination routes. On further debugging it was found that post delete trigger as part of re-add flow, when a describe call is sent to AWS got a reply with the state of this entry as "active" because of which a new create request is not being sent.

5.1(2e) and later

CSCvw27056

A CSR's management Interface is down or inaccessible through SSH.

5.1(2e) and later

CSCvw36844

No UDR entries will be present if extEPG is consumer on a graph on which REDIRECT is enabled.

5.1(2e) and later

CSCvw40737

SSH to a virtual machine's public IP address fails, despite the NSG allowing the traffic inbound. SSH to the private IP address of the virtual machine from within the VNet works.

5.1(2e) and later

CSCvw48190

When Cloud APIC is restart, the VPN connection from a tenant's VNets will get deleted and re-created, one by one. This can be seen in the Azure activity logs. It should not impact traffic, as all connections are not deleted at the same time.

5.1(2e) and later

CSCvw48256


The primary CIDR of CtxProfile defines the VPC/VNet and hence cannot be changed/modified without deleting the vPC. For example, in the AWS console, you cannot simply modify or delete the primary CIDR. You must delete the entire vPC. However, in the Cisco ACI policy, a configuration post that modifies the primary CIDR will lead to an incorrect state of creating a new vPC while not deleting the old one.

Because the GUI does not allow the primary CIDR change operation (that is, deselecting the primary option from one CIDR and selecting the primary CIDR box for the other CIDR), this problem will not happen if the operation is performed using GUI.

If the operation is done using the REST API, a fault is raised for the new primary CIDR and the related subnet is not deployed in the cloud. The old CIDR is not cleaned up as well.

5.1(2e) and later

CSCvw55088

A user who is assigned a large number of security domains may not be able to create other Cisco ACI policies.

5.1(2e) and later

CSCvw57813

After a Cisco Cloud APIC upgrade, all of the Cloud Service Routers will be upgraded in two batches, even and odd. State of the cuurent batch of CSRs that are upgraded are persisted in the Cisco Cloud APIC. When the Cisco Cloud APIC is rebooted mid-upgrade of the CSRs, the state information of the upgrade will be lost. This results in the halt of the CSR upgrade process.

5.1(2e) and later

CSCvw58899

After a configuration import, an fvCtx managed object may have a different vrfIndex value. This would cause the configuration in the CSRs to be modified, thereby leading to traffic drops.

5.1(2e) and later

CSCvw60314

After upgrading one of the Cloud APIC sites to the 5.1(2) release, intersite traffic loss occurs. This is due to intersite IPSec tunnels being deleted and recreated by MSO. Traffic recovers in about 30 to 40 minutes after the tunnels are recreated.

5.1(2e) and later

CSCvx06278

Rules in the firewall may be missing for traffic that has a cloud site-external EPG as the consumer/source to a provider EPG/destination when redirect is enabled in the cloud graph.

5.1(2e) and later

CSCvx16601

When the "AllowAll" flag is enabled on a service device such as a native load balancer or on the logical interface of a third party device, it is possible that to see some specific rules apart form a rule that allows all traffic from any source to any destination.

5.1(2g) and later

Resolved Issues

Click the bug ID to access the Bug Search tool and see additional information about the bug. The "Fixed In" column of the table specifies whether the bug was resolved in the base release or a patch release.

Bug ID

Description

Fixed in

CSCvu02115

If CSRs are undeployed and redeployed in a non-Cloud APIC home region, this results in a delete and re-add of the infra VPC. If there are other CloudContextProfiles (user tenant VRF tables) pointing to the hub network (transit gateway), then when the CSRs are redeployed, traffic from the transit gateway to a CSR may be dropped. In this case, the transit gateway will remain undeleted because the user tenant VPC is still using the transit gateway. The traffic drop might occur because when the infra VPC is redeployed, it might get a different set of CIDRs allocated to it.

5.1(2e)

CSCvu03950

An operational fault related to SSH connectivity to the Cloud Services Router is seen in the GUI. This fault indicates that Cloud Services Router connectivity has been lost and that configurations can no longer go to that Cloud Services Router.

5.1(2e)

CSCvu17097

Inter tenant shared services traffic is impacted after tenant delete and add.

5.1(2e)

CSCvu52738

A secure LDAP test user does not make use of secure LDAP for a test user liveness check.

5.1(2e)

CSCvu63858

The inner table view of a contract might have incomplete information for the consumer EPGs.

5.1(2e)

CSCvu76275

Duplicate rules can be seen in the Azure on the Network Security Group of an EPG providing a contract with an ALB or NLB attached. There is no functional impact. The duplicate rule seen will be an inbound rule that allows the ALB/NLB subnet to talk to the provider EPG application security group.

5.1(2e)

CSCvu80939

The route table entry to the provider subnet is not created in the consumer's route table.

5.1(2e)

CSCvu81750

Inter-site BGP sessions are down after a policy-based upgrade of Cloud APIC from the 5.0(1) release to the 5.1(2) release.

5.1(2e)

CSCvu84182

Faults containing the following keywords are observed and VGW does not get deleted in Azure:

· InUseSubnetCannotBeDeleted

· NetCfgInvalidSubnet

· VirtualNetworkGatewayCannotBeDeleted

5.1(2e)

CSCvv19470

A rule to allow traffic from a consumer cloudEPg to a firewall's untrust interface will be missing if the firewall's untrust connector uses a tag-based selector.

This symptom will be seen only if an NLB is the first node of the service chain and if the graph is not performing any redirect.

5.1(2e)

Known Issues

Click the bug ID to access the Bug Search tool and see additional information about the bug. The "Exists In" column of the table specifies the 5.1(2) releases in which the bug exists. A bug might also exist in releases other than the 5.1(2) releases.

Bug ID

Description

Exists in

CSCvo06626

When a cloudExtEpg matches on a 0/0 network and has a bi-directional contract with two cloud EPGs, such as cloudEpg1 and CloudEpg2, this can result in inadvertent communication between endpoints in cloudEpg1 and cloudEpg2 without a contract between the two EPGs themselves.

5.1(2e) and later

CSCvo55112

Logs are lost upon stopping the Cloud APIC instance.

5.1(2e) and later

CSCvo95998

There is traffic loss after a Cloud APIC upgrade. Traffic will eventually converge, but this could take a few minutes.

5.1(2e) and later

CSCvq11780

Creating VPN connections fail with the "invalidCidr" error in AWS or the "More than one connection having the same BGP setting is not allowed" error in Azure.

5.1(2e) and later

CSCvq76039

When a fault is raised in the Cloud APIC, the fault message will be truncated and will not include the entire cloud message description.

5.1(2e) and later

CSCvr01341

REST API access to the Cloud APIC becomes delayed after deleting a tenant with scaled EPGs and endpoints. The client needs to retry after receiving the error.

5.1(2e) and later

CSCvu81355

Traffic gets dropped after downgrading to the 5.0(1) release. Cloud Services Router has incompatible configurations due to an issue with reading configurations using SSH.

5.1(2e) and later

CSCvu88006

On the Dashboard, fewer VNet peerings are shown than expected.

5.1(2e) and later

CSCvw05821

Redirection and UDR does not take effect when traffic coming through an express route and destined to a service end point is redirected to a native load balancer or firewall.

5.1(2e) and later

CSCvw39814

Infra VPC subnet route table entry for 0.0.0.0/0 route with TGW attachment as nh, is left as a stale entry upon being undeployed. There is no functional impact. Upon being redeployed, this entry is updated with the correct TGW attachment ID as nh.

5.1(2e) and later

CSCvw40818

After upgrading Cloud APIC, the Cloud Services Routers will be upgraded in two batches. The even set of CSRs are triggered for upgrade first. After their upgrade is complete and all of the even CSRs are datapathReady, only then the odd set of CSRs will be triggered for upgrade. When even one of the upgrade of the even CSRs fail and they don't become datapathReady, the odd set of CSRs will not be triggered for upgrade. This is the behavior followed to avoid any traffic loss.

5.1(2e) and later

CSCvw49898

When the downgrading from the 5.1(2) release to the 5.0(2) release, traffic loss is expected until all of the CSRs are downgraded back to the 17.1 release. The traffic loss occurs because when the CSRs are getting downgraded to the 17.1 release, the CSR NIC1s will be in the backendPools and traffic from the spokes will still be forwarded to the native load balancer. The traffic gets blackholed until the CSRs get fully programmed with all the configurations in the 17.1 release.

5.1(2e) and later

CSCvw50918

Upon downgrading Cloud APIC, VPN connections between Cloud APIC and the cloud (AWS/Azure VPN gateway) will be deleted and re-created, causing traffic loss. Traffic loss is based on how quickly the VPN connections are deleted and re-created in AWS due to AWS throttling.

5.1(2e) and later

CSCvw51544

A user who is assigned a large number of security domains may not be able to create other Cisco ACI policies.

5.1(2e) and later

CSCvw97632

There is traffic loss when Cloud APIC is being downgraded from release 5.1(2e) to 5.0(2i).

5.1(2e) and later

Compatibility Information

This section lists the compatibility information for the Cisco Cloud APIC software. In addition to the information in this section, see the Cisco Application Policy Infrastructure Controller Release Notes, Release 5.1(2) and Cisco ACI Multi-Site Orchestrator Release Notes, Release 3.1(1) for compatibility information for those products.

· Cloud APIC release 5.1(2) supports the following Cisco ACI product releases:

o Cisco ACI Multi-Site Orchestrator, release 3.1(1)

o Cisco APIC, release 5.1(2)

o Cisco NX-OS for ACI-mode switches, release 15.1(2)

· Cloud APIC does not support IPv6.

· AWS does not support using iBGP between a virtual gateway and a customer gateway.

· Cloud APIC supports the following AWS regions:

o Asia Paciﬁc (Mumbai)

o Asia Paciﬁc (Osaka-Local)

o Asia Paciﬁc (Seoul)

o Asia Paciﬁc (Singapore)

o Asia Paciﬁc (Sydney)

o Asia Paciﬁc (Tokyo)

o AWS GovCloud (US-Gov-West)

o Canada (Central)

o EU (Frankfurt)

o EU (Ireland)

o EU (London)

o South America (São Paulo)

o US East (N. Virginia)

o US East (Ohio)

o US West (N. California)

o US West (Oregon)

· Cloud APIC supports the following Azure regions:

o Australiacentral

o Australiacentral2

o Australiaeast

o Australiasoutheast

o Brazilsouth

o Canadacentral

o Canadaeast

o Centralindia

o Centralus

o Eastasia

o Eastus

o Eastus2

o Francecentral

o Japaneast

o Japanwest

o Koreacentral

o Koreasouth

o Northcentralus

o Northeurope

o Southcentralus

o Southeastasia

o Southindia

o Uksouth

o Ukwest

o Westcentralus

o Westeurope

o Westindia

o Westus

o Westus2

· Cloud APIC supports the following Azure Government cloud regions:

o US DoD Central

o US DoD East

o US Gov Arizona

o US Gov Texas

o US Gov Virginia

Related Content

See the Cisco Cloud Application Policy Infrastructure Controller page for the documentation.

See the Cisco Application Policy Infrastructure Controller (APIC) page for the verified scability, Cisco Application Policy Infrastructure Controller (APIC), and Cisco ACI Multi-Site Orchestrator (MSO) documentation.

The documentation includes installation, upgrade, configuration, programming, and troubleshooting guides, technical references, release notes, and knowledge base (KB) articles, as well as other documentation. KB articles provide information about a specific use case or a specific topic.

By using the "Choose a topic" and "Choose a document type" fields of the APIC documentation website, you can narrow down the displayed documentation list to make it easier to find the desired document.


Introduction

The Layer 3 Out (L3Out) in Cisco Application Centric Infrastructure (Cisco ACI) is the set of configurations that define connectivity to outside of ACI via routing. The goal of this document is to explain thoroughly Cisco ACI design concepts and options related to the ACI L3Out. This document does not provide step-by-step configuration examples for all scenarios. Instead, its focus is on understanding the key concepts. Hence, the recommendation is to read this document with some basic understanding of ACI along with decent knowledge of standard routing protocols such as OSPF, EIGRP, BGP and MP-BGP.

Cisco ACI Layer 3 Out overview

The ACI fabric is formed from multiple components. Some of these components include bridge domains (BDs) and endpoint groups (EPGs) to provide Layer (L2) connectivity or default gateway functions for a group of endpoints. Another one is the Layer 3 Out (L3Out, or external routed network in Cisco APIC GUI prior to the APIC Release 4.2), which is to provide Layer 3 (L3) connectivity between servers connected to ACI and other network domains outside of the ACI fabric through routing protocol or static route.

Cisco ACI was originally built to be a stub network in a data center to manage endpoints. The ACI Layer 3 Out (L3Out) was initially designed only as a border between the stub network formed by ACI and the rest of the network, such as intranet, Internet, WAN, etc., not as a transit network.


ACI fabric as a stub network

Due to this stub nature, traffic traversing from one L3Out to another through the ACI network was originally not supported.

Beginning with the APIC Release 1.1, however, Cisco ACI introduced the Transit Routing feature, which allows the ACI fabric to be a transit network so that traffic can traverse from one L3Out to another L3Out. Please refer to the “L3Out Transit Routing” section for details.


ACI fabric as a transit network

Note:

L3Out, essentially, connects a network device that has other subnets behind it. In ACI BD/EPG, every IP address is learned as an endpoint with /32 (or /128 for IPv6). Hence, connecting a network device that contains multiple subnets behind it to ACI via a BD/EPG will end up with an endpoint that has a huge number of /32-IP-addresses, which is not efficient and will likely hit a scalability limit. Please refer to the “L3Out and regular endpoints” section in the “ACI Fabric Endpoint Learning” white paper for this as well.

Basic components of L3Out

The L3Out provides the necessary configuration objects for five key functions:

1. Learn external routes via routing protocols (or static routes)

2. Distribute learned external routes (or static routes) to other leaf switches

3. Advertise ACI internal routes (BD subnets) to outside ACI

4. Advertise learned external routes to other L3Outs (Transit Routing)

5. Allow traffic to arrive from or be sent to external networks via L3Out by using a contract


The five basic components of L3Out

In the following, each step is briefly explained. For detailed information about each step, please refer to later sections or the Cisco APIC Layer 3 Networking Configuration Guide.

1. Learn external routes on border leaf switches

Figure 4, below, depicts each component in an L3Out under a tenant (Tenant > Networking > External Routed Networks > L3Out). The parts in bold are the mandatory components to configure a routing protocol and learn external routes from an external network device. At least one L3Out EPG is also required to deploy a routing protocol and related interface parameters on leaf switches even though the L3Out EPG itself is a security construct like the EPGs, and is not a routing protocol configuration.


The basic components of L3Out in GUI (APIC 3.2 Release)

The following steps are the summary to deploy a routing protocol on an ACI border leaf with the components shown in Figure 4.

1. Root of L3Out

a. Select a routing protocol to deploy (such as OSPF or BGP)

b. Select a VRF to deploy the routing protocol

c. Select a L3Out Domain to define which range of VLANs and interfaces the L3Out configurations are allowed to use.

This domain itself is configured via Fabric Access Policies.

2. Node Profile

a. Select leaf switches on which the routing protocol is deployed.

(These are called border leaf switches)

b. Configure the Router-ID for the routing protocol on each leaf.

(Unlike a normal router, Cisco ACI does not automatically assign a Router-ID based on the IP addresses on the switch.)

3. Interface Profile

a. Configure leaf Interfaces on which the routing protocol runs.

This step consists of entering the following configurations: interface type (SVI, routed-port, or subinterface), interface ID, IP addresses, and so on.

b. Select a policy for the interface-level routing protocol parameters (such as hello interval).

In most deployments, the default configuration (policy) is used.

4. External EPG (L3Out EPG)

a. An empty external EPG is enough just to deploy a routing protocol and interface parameters such as IP address or SVI itself, and to establish routing protocol peering with neighbor routers.

Details on how to use external EPGs will be covered later.

The details on each component, such as the node profile or routing protocol options, are covered in later sections, such as “L3Out node and interface profiles” or “L3Out BGP”. Once the routing protocol is deployed on border leaf switches and a neighborship is established with external devices, those border leaf switches can learn external routes.

At this point, the external routes are only present on these border leaf switches and the ACI fabric has yet to distribute those routes to other leaf switches (See the next section, “Distribute external routes within the ACI fabric.”)

2. Distribute external routes within the ACI fabric

ACI uses Multi-Protocol BGP (MP-BGP) with VPNv4 in the ACI infra VRF (overlay-1 VRF) to distribute external routes from a border leaf to other leaf switches. Similar to other configurations/components in the ACI infra VRF such as ISIS between each switch, this configuration is also automated in the background. The only two configurations that users need to perform are as follows:

● Select the BGP AS number.

◦ This is the AS number to represent the entire ACI fabric. It is used for infra MP-BGP between leaf and spines, and for BGP in user L3Outs to establish BGP peers with external devices.

● Select spine switches as BGP Route Reflectors.

◦ Each leaf switch will be a BGP client for the selected route-reflector spine switches.

◦ This MP-BGP is per pod. Ensure that each pod has at least one route-reflector spine. Two route reflectors per pod is recommended.

◦ This Route Reflector for internal MP-BGP (VPNv4) and the External Route Reflector between pods for Multi-Pod MP-BGP (VPNv4, eVPN) are two different configurations.

Once these two components are configured under System > System Settings > BGP Route Reflector and assigned to Fabric Pod Profile under Fabric > Fabric Policies > Pods, the distribution of external routes will occur with MP-BGP, and the external routes will appear on non–border leaf switches as iBGP routes pointing to the border leaf switches TEP IP addresses (see Figure 5 and Figure 6). Please check the “Infra MP-BGP” section for more details.


ACI BGP AS number and MP-BGP route-reflector spines in APIC GUI (Release 3.2)


Pod Profile and Policy Group for BGP route reflector in APIC GUI (Release 3.2)

3. Advertise internal routes (BD subnets) to external devices

Once the MP-BGP route reflector policy is configured and assigned to a Pod Profile, all leaf switches should have external routes in their routing table for a given VRF. For external devices to have reachability to servers connected to ACI, ACI needs to advertise the BD subnets to outside.

Figure 7, below, depicts the summary of the most basic method to advertise a BD subnet, which is via associating a L3Out to the BD under Tenant > Networking > Bridge Domaine > BD > L3 Configurations tab.


Advertise internal routes (BD subnets) in GUI (APIC Release 3.2)

The key points here are as follows:

● Mark a BD subnet with an “Advertised Externally” scope.

● Associate the BD with the L3Out(s) that need(s) to advertise the BD subnet to the outside.

These two configurations internally create a route-map rule on the border leaf switches to redistribute the BD subnet (static/direct route) into the routing protocol of the associated L3Out.

If the BD happens to be deployed on the same border leaf, the redistribution happens via the route-map rule, and it will be advertised. However, that is usually not the case. Please remember that BD subnets are not distributed via MP-BGP, which is only for external routes. A contract between an EPG in the BD and the L3Out is required. Once the contract is configured, APIC knows the L3Out needs to talk to someone in the BD and installs the BD subnet on the border leaf switches. Then the redistribution happens with the route map mentioned above. Users typically do not need to pay attention to these details because a contract is required anyway to allow the traffic.

Please check the “ACI BD subnet advertisement” section for details.

4. Advertise external routes to other external devices (Transit Routing)

In case the communication needs to be between two L3Outs instead of a normal EPG, advertising external routes from one L3Out to another is required. This is called Transit Routing.

Advertising external routes to outside (Transit Routing) can be achieved with a single check box “Export Route Control Subnet” scope in L3Out Subnet (Tenant > Networking > External Routed Networks > L3Out > Networks > L3Out EPG > Subnets).


Export Route Control Subnet for Transit Routing GUI (APIC Release 3.2)

When this scope “Export Route Control Subnet” is selected, a route-map rule is created on the border leaf switches to redistribute the configured subnet (10.0.0.0/8 in Figure 8) from other L3Outs (routing protocol or static route) into the routing protocol for this L3Out. The redistribution happens from MP-BGP when the two L3Outs are on different border leaf switches. If the two L3Outs are on the same border leaf, redistribution happens directly between the routing protocols for each L3Out. If the two L3Outs use the same routing protocol on the same border leaf, other methods than redistribution are used.

Since the route-map rule uses an IP prefix-list, the subnet with “Export Route Control Subnet” scope needs to be exactly the same as what is in the routing protocol database. For example, 10.0.0.0/8 with “Export Route Control Subnet” scope exports only a route “10.0.0.0/8” but not 10.0.0.0/16. For aggregation/summarization, please check the “L3Out subnet scope options” section or the “L3Out Transit Routing” section for details.

Caution:

An external route with “Export Route Control Subnet” scope is advertised from the configured L3Out. This scope should not be configured on an L3Out that is learning the same route, because it would mean the L3Out tries to advertise the route back to its learning source. This could potentially cause a loop.

“External Subnets for the External EPG” scope, on the other hand, is to be configured on the L3Out that is learning the route. Hence, having these two scopes in the same L3Out is likely an undesired configuration. Please check step 5 below for details on “External Subnets for the External EPG” scope.

5. Allow traffic with a contract

The previous sections described the necessary configurations for the routing protocols to exchange routes between ACI and the external network. However, even if forwarding could theoretically work from a routing-table perspective, in ACI no traffic can flow across EPGs without a contract. This applies to L3Out EPG as well.

The key point here is how ACI classifies external routes to apply a contract. A normal EPG is classified based on a VLAN and a leaf interface from which the packet came in. In the case of L3Out, the classification of the traffic in the L3Out EPG is based on prefix matching. For this, the “External Subnets for the External EPG” scope on an L3Out subnet (Tenant > Networking > External Routed Networks > L3Out > Networks > L3Out EPG > Subnets) is used.


External subnets for the External EPG for contract in GUI (APIC Release 3.2)

Unlike the “Export Route Control Subnet” scope, the scope “External Subnets for the External EPG” does not have any impact on the routing table. It simply defines how to classify the traffic based on the source or destination IP address in order to apply a contract. Even when the routing table has only a default route 0.0.0.0/0, users still can configure more specific subnets, such as 20.0.0.0/8 with “External Subnets for the External EPG” under an L3Out EPG (EPG1 under Tenant > Networking > External Routed Networks > L3Out > Networks in Figure 9) and 30.0.0.0/8 with “External Subnets for the External EPG” under another L3Out EPG to apply a different set of contracts. This scope is not implemented with IP prefix-lists like “Export Route Control Subnet”. Hence, the matching is based on a longest prefix match (LPM). For example, a source for a packet with source IP 20.1.1.1 will be classified into L3Out EPG1 in Figure 9 due to 20.0.0.0/8 with “External Subnets for the External EPG” scope.

When a traffic IP does not match any of the subnets in the “External Subnets for the External EPG” scope in the VRF (please note that this scope is per VRF instead of L3Out; see the Caution below for details), the traffic will likely be dropped as there is no L3Out EPG with a contract in the VRF for the IP. If there is a 0.0.0.0/0 with “External Subnets for the External EPG” scope somewhere in the same VRF, that L3Out EPG with 0.0.0.0/0 will be the fallback for all traffic in that VRF, from a contract perspective.

Once the traffic classification is configured with “External Subnets for the External EPG” scope, users just need to configure a contract between the L3Out EPG and any components that need to communicate with the L3Out.

Please check the “L3Out subnet scope options” section or the “L3Out contracts” section for details.

Caution:

These L3Out subnet scopes are per VRF. Hence, even if a subnet 10.0.0.0/8 is learned from L3Out A, and traffic with source IP 10.0.0.1 is coming from L3Out A, the traffic could be classified into an L3Out EPG under L3Out B (let’s call it L3Out EPG B) if L3Out EPG B has 10.0.0.0/8 with “External Subnets for the External EPG” scope instead of L3Out EPG A, for some reason. This may depend on which leaf the contract rule is applied. One of the factors to decide which leaf applies the contract is Policy Control Enforcement Direction in the “L3Out contracts” section.

When the same subnet is configured with “External Subnets for the External EPG” scope in multiple L3Out EPGs in the same VRF, the configuration will be rejected. However, 0.0.0.0/0 is an exception. This does not mean 0.0.0.0/0 with “External Subnets for the External EPG” scope should be configured in multiple L3Out EPGs. It is strongly recommended NOT to do that to avoid traffic being allowed unexpectedly. See the “L3Out contracts” section for details.

Infra MP-BGP

This section covers the details on how Multi-Protocol BGP (MP-BGP) in the ACI fabric infra distributes the external routes learned from the L3Out to all leaf switches.

Figure 5 and Figure 6 in the previous section show the configuration (BGP AS and BGP route-reflector spines) in the APIC GUI for the infra MP-BGP. Once the configuration is done, the MP-BGP (the blue part in Figure 10) is deployed on the leaf and spine switches.


Infra MP-BGP architecture

The following explains each component in Figure 10:

1. BGP IPv4/v6 Address Family (AF) is deployed on all leaf switches (both border and non–border leaf switches) in all user VRFs.

2. BGP VPNv4/v6 Address Family is also deployed on all leaf and route reflector spine switches in infra VRF (overlay-1 VRF).

a. All leaf switches establish iBGP sessions with route-reflector spine switches in infra VRF.

b. All leaf switches exchange their VPNv4/v6 routes through route reflector spines in infra VRF.

3. Once an L3Out is deployed on a leaf, the BGP IPv4/v6 AF on the same border leaf automatically creates a redistribution rule for all the routes from the routing protocol of the L3Out within the same user VRF.

a. This redistribution is called “Interleak”. If the L3Out is using BGP, no redistribution (interleak) is required for routes learned via BGP because the BGP process for the L3Out and for the infra MP-BGP is the same.

4. The redistributed IPv4/v6 routes are exported from the user VRF to the infra VRF as VPNv4/v6.

5. On other leaf switches, the VPNv4/v6 routes distributed through route-reflector spines are imported from the infra VRF to the user VRF as IPv4/v6.

a. On each leaf, BGP IPv4/v6 AF has export and import rules with the route target (RT) to exchange routes with VPNv4/v6 AF. The RT is in the form of “<ACI BGP AS>:<VRF VNID>”. This means that each VRF has the same RT on all leaf switches, and all VPNv4/v6 routes in the same VRF share the same RT. With this, a border leaf exports IPv4/v6 external routes into VPNv4/v6 AF with an RT, and each user VRF on other leaf switches can import VPNv4/v6 routes from its own VRF into IPv4/v6 AF based on the RT without incorrectly importing VPNv4/v6 routes from other VRFs.

Note:

Set rules can be applied to the route map for redistribution (interleak) from L3Out into BGP IPv4/v6 AF via an Interleak Route Profile. See the “Route Profile on Interleak” in the “L3Out Route Profile / Route Map” section for details.

Root component of L3Out

As described in the previous “Basic components of L3Out” section, the L3Out contains components called Logical Node/Interface Profile and Networks as its child objects. The details for each child component will be covered in each section later. Instead, this section covers the root component of L3Out.

In the root component of the L3Out, the most important configurations are VRF, external routed domain, and routing protocol.

● VRF This is the VRF on which the L3Out and its routing protocol are deployed. This could be a VRF in the same tenant or a VRF in a common tenant.

● External routed domain This is the domain to allow the L3Out to use a set of interfaces and VLANs. The domain itself is configured under “Fabric > Access Policies > Physical and External Domains > External Routed Domains” along with the VLAN pool and the Attachable Access Entity Profile (AEP).

● Routing protocol This is the routing protocol that is deployed with the L3Out on the node and interface specified by the Logical Node/Interface Profile. Cisco ACI allows only one routing protocol per L3Out with one exception. BGP and OSPF can be configured in the same L3Out as an exception in order to be able to use OSPF as the IGP for BGP. Once the routing protocol is selected, some parameters such as OSPF area number or EIGRP AS number configurations show up in the same window. The details for each routing protocol parameters are covered in each routing protocol section later (BGP, OSPF, and EIGRP).

Although configurations other than the above three are optional, Figure 11 shows the GUI example for the L3Out root component followed by a quick description of each L3Out-specific option.


L3Out root component in APIC GUI (Release 3.2)

● Provider Label This is for the GOLF (Giant OverLay Forwarding) feature. This label is to be configured on a GOLF L3Out in the infra tenant. Please check the “GOLF” section in the Cisco APIC Layer 3 Networking Configuration Guide.

● Consumer Label This is also for the GOLF feature. This label is to be configured on an L3Out in a user tenant/VRF where it needs to communicate with external devices behind GOLF. This label must match the provider label for the GOLF L3Out in the infra tenant. It allows the L3Out in the user tenant/VRF to apply its L3Out EPG (L3Out Networks in GUI) to the GOLF L3Out. Please check the “GOLF” section in the Cisco APIC Layer 3 Networking Configuration Guide.

● PIM This stands for Protocol Independent Multicast. Users may not need to manually toggle this option as it is typically configured on a VRF component, and this checkbox is automatically toggled when necessary. Please check the “IP multicast” section in the Cisco APIC Layer 3 Networking Configuration Guide for details.

● Route Control Enforcement This is to enable Import Route Control, which allows users to configure Import Route Control Subnet scope for a subnet under the L3Out EPG (L3Out Networks in GUI). Please check the section L3Out subnet scope options for details.

● Route Profile for Interleak This is to customize a route map that is used to redistribute external routes into the infra MP-BGP. Please check the subsection “Route Profile on Interleak” in the section “L3Out Route Profile / Route Map”.

L3Out Node and Interface Profiles

The main function of the L3Out Node and Interface Profiles is to specify which switch nodes should be border leaf switches and which interfaces should speak a routing protocol. Other functions for these two profiles are static routes, interface-level routing parameters, etc., as shown in Figure 12.


Logical Node Profiles / Logical Interface Profiles GUI (APIC Release 3.2)

Node and Interface Profiles Design

With Node and Interface Profiles, you can achieve the same configurations in multiple ways as long as the node IDs in the Node Profiles and Interface Profiles match.


L3Out Node Profile configuration patterns

Figure 13 illustrates three different ways (here called “patterns”) to configure node-101 e1/1 and node-102 e1/1 to be part of the L3Out1 and speaking the routing protocol defined in L3Out1.

● Pattern 1: Both interfaces are in the same Logical Interface Profile A under a Logical Node Profile A.

● Pattern 2: Each interface is in its own Logical Interface Profile A and B under the same Logical Node Profile A.

● Pattern 3: Each interface is in its own Logical Interface Profile A and B under their respective Logical Node Profile A and B.

All these configuration patterns are correct and program the ACI border leaf identically.

Note:

When IPv4 and IPv6 addresses need to be configured on the same interface, the Logical Interface Profiles for IPv4 and IPv6 need to be different while the Logical Node Profiles can still be shared.

Logical Node Profile details

This subsection goes through each option under the Logical Node Profile.


Logical Node Profile options in GUI (APIC Release 3.2)

● Node ID This is a node ID where the routing protocol from the L3Out should be deployed. This node is called a border leaf.

● Router ID This is a per-VRF router ID for the routing protocol defined in the L3Out on this node. The same principals of the router ID from a normal router apply to Cisco ACI as well.

This is equivalent to the following CLI if it were on a standalone Cisco NX-OS device. This is just for comparison and not an actual NX-OS–style CLI to configure on an APIC.

router ospf default vrf VRF1 router-id 1.1.1.1

● Use Router ID as Loopback Address Enable this option to create a loopback interface on this node with the router ID as its IP address. This is typically not required unless BGP peers need to source from a loopback with the router ID as their IP address.

This is equivalent to the following CLI if it were on a standalone NX-OS device. This is just for comparison and not an actual NX-OS–style CLI to configure on an APIC.

router ospf default vrf VRF1 router-id 1.1.1.1

interface loopback10 vrf member VRF1 ip address 1.1.1.1/32

This option is ignored when loopback interfaces are configured manually in the next option below.

● Loopback Addresses This is to create loopback interfaces on this node manually with arbitrary IP addresses. This is typically not required unless BGP peers need to source from a loopback IP address.

● Static Routes This is to create a static route on this node. The next-hop IP for the static route should be connected to a L3Out. When a next-hop IP is not configured, a static route with a null next-hop is created on the node. A static route configured here is distributed to other leaf switches via infra MP-BGP, like external routes learned from a routing protocol. Please check the section “L3Out static routes” for details.

Note:

If there are two L3Outs with the same routing protocol on the same node in the same VRF, the router ID on the Node Profiles on both L3Outs need to match. This is because using different router IDs in multiple L3Outs with the same routing protocol on the same node is equivalent to trying and entering the following two configurations on one standalone NX-OS device.

router ospf default router ospf default vrf VRF1 AND vrf VRF1 router-id 1.1.1.1 router-id 2.2.2.2

Please note that these CLIs are just for comparison and are not actual NX-OS–style CLIs to configure on an APIC.

Logical Interface Profile details

This subsection covers the main configuration options under the Logical Interface Profile.


Logical Interface Profile and Protocol Interface Profile in GUI (APIC Release 3.2)

Figure 15 provides an overview of the options available in the Logical Interface Profiles. The main purpose of the Interface Profile is to create and configure interfaces to run the routing protocols on border leaf switches. This is similar to configuring an IP address and routing protocol commands such as ‘ip router ospf 1 area 0’ on a standalone NX-OS device. This is performed by configuring Interface Type (see Figure 15) and by configuring a Protocol Interface Profile. Without a protocol profile, the interfaces will not join the routing protocol (see each routing-protocol section for details: BGP, OSPF, and EIGRP). In addition to the Interface Type and the Protocol Interface Profile, one may need to configure the General tab in the Logical Interface Profile for optional interface-level features such as Data Plane Policing, NetFlow, PIM Interface Policy, Internet Group Management Protocol (IGMP), and so on. Bidirectional forwarding detection (BFD) can be configured under a Logical Interface Profile as well. See the “L3Out BFD” section for details on BFD.

The following paragraphs cover each Interface Type and its parameters. Please refer to the appropriate section for each routing protocol for details on the Protocol Interface Profile and other options.

The L3Out allows you to configure the following types of interfaces:

1. Routed Sub-Interface

2. Routed Interface

3. SVI

4. Floating SVI (introduced in APIC Release 4.2)

The design considerations related to the interfaces of an L3Out are almost the same as for a normal router or Layer 3 switch.

When a physical port is already used as a trunk port by an EPG, the same port cannot be used as a Routed Sub-Interface or a Routed Interface (L3 port) by an L3Out since the interface is already configured as a switchport (L2 port) by an EPG.

Figure 16 illustrates the meaning of the common parameters for a Routed Sub-Interface. Most of these parameters can also be found for the other interface types; the exception is the VLAN parameter, which can be configured for Routed Sub-Interfaces and for SVIs, but not for a Routed Interface.


Logical Interface Profile common parameters

The following list provides additional details for each parameter type:

● Path Type There are three Path types available in an L3Out, as shown in the following table:

Path Type

Description

Supported I/F Type

Port

a physical port such as eth1/1 on a single leaf switch

Routed Interface Sub-Interface SVI

Direct Port Channel

a normal port-channel on a single leaf switch

Routed Interface* Sub-Interface* SVI

Virtual Port Channel

a vPC that spans across two leaf switches

SVI

● Node This is to specify a border leaf for the interface. When selecting PC or vPC as the Path Type, this option is not available and not required since users need to select the name of a PC/vPC Interface Policy Group as a Path, and this configuration already includes the node information. Ensure that the node ID here matches the one in the parent Logical Node Profile.

● Path This is the interface ID, such as eth1/1 for Path Type Port, or the name of a PC/vPC Interface Policy Group for Path Type PC or vPC.

● Encap This is the VLAN ID for the interface configured in the Path fields. This VLAN ID is sometimes referred to as encap or access-encap VLAN as opposed to some internal IDs such as PI-VLAN (Platform Independent VLAN). When an SVI is used, this VLAN ID needs to be included in the VLAN Pool under the External Routed Domain (L3Domain) associated to the L3Out. When a routed or subinterface is used, a VLAN Pool under the L3 Domain is not required.

◦ When a routed interface is used, this field is not required and does not appear.

◦ When a subinterface is used, a subinterface is created with this Encap VLAN.

◦ When an SVI is used, this Encap VLAN is trunked on the interface and an SVI for the VLAN is created on the specified leaf. Although various type of SVI configurations are supported such as one SVI with multiple trunk interfaces (Figure 17), when the same interface needs to trunk two different VLANs for two different SVIs, each SVI needs to be configured in different Logical Interface Profiles.

● IPv4 Primary / IPv6 Preferred Address This is the main IP address on the subinterface, routed interface, or SVI. This IP address is used to peer with other routing protocol speakers.

● IPv4 Secondary / IPv6 Additional Addresses (optional) This is useful to define additional IP addresses when a common IP is required on two border-leaf switches so that external devices can point to a single IP with a static route.

● MAC Address (optional) This is a MAC address for the subinterface, routed interface, or SVI. In most cases, this field can be left as the default. For SVIs with the same VLAN ID, the same MAC address (default or non-default) must be used across ACI switches. This does not apply to SVIs across different L3Outs with SVI Encap Scope Local because those belong to different flooding domains even with the same VLAN ID.

● MTU (bytes) (optional) This is the MTU (maximum transmission unit) value in bytes for the subinterface, routed interface, or SVI. This may need to be adjusted depending on the routing protocol, since the default “inherit” means that APIC configures the interface with the ACI default MTU of 9000 bytes. Most router interfaces do not use jumbo frames as a default, and routing protocols such as OSPF and EIGRP do not establish peering correctly unless the MTUs between the router interfaces match.

● Link-local Address (optional) This is an IPv6 link-local address for the subinterface, routed interface, or SVI. By default, ACI creates an IPv6 link-local address from each leaf’s system MAC address in EUI-64 format.

Note:

The following command illustrates how to check the system MAC address of a leaf switch to calculate the IPv6 link-local address, if it is needed.

leaf1# show sprom backplane | grep 'MAC Address'

MAC Addresses : 01-23-45-67-89-ab


Note:

In the case of an SVI, even though the IP address is configured per Path, this does not mean the SVI IP address needs to be configured per L2 interface. In case the same SVI and its IP address need to be deployed on multiple L2 interfaces, the configuration shown in Figure 17 will achieve that.


How to configure the same VLAN on two different Paths as an SVI

L3Out bridge domain

When an L3Out SVI is instantiated, Cisco ACI creates a bridge domain (BD) internally for the SVI to provide a Layer 2 flooding domain. This BD is called the L3Out BD or external BD, and is not visible to the user as a normal BD in APIC. An L3Out BD is created internally for each access-encap VLAN for an L3Out SVI while a normal BD can contain multiple access-encap VLANs all mapped to the same flooding domain. This L3Out BD may span across multiple border leaf switches if other border leaf switches also use the same access-encap VLAN for the L3Out SVI in the same L3Out.


L3Out BD and access-encap VLAN (in the same L3Out)

In Figure 18, a single L3Out has two different access-encap VLANs, 10 and 20, with multiple Node and Interface Profiles. The picture shows that all three routers with the same access-encap VLAN 10 belong to the same L3Out BD1. This shows that the L3Out BD is independent of the Node Profile and the Interface Profile. The instantiation of the L3Out BD depends exclusively on the encap VLAN ID.


L3Out BD and access-encap VLAN (in different L3Outs)

Figure 19 shows that an L3Out BD is created per access-encap VLAN within the L3Out. Even if two L3Outs use the same access-encap VLAN, each L3Out creates its own L3Out BD. Because of this, multiple L3Outs that use the same access-encap VLAN ID cannot coexist on the same border leaf. This behavior can be changed with the SVI Encap Scope option under the L3Out SVI.


L3Out BD and routing protocol neighbors

Figure 20 shows that external routers connected to the same L3Out BD will exchange protocol hellos through ACI and become neighbors to each other on top of the ACI border leaf switches. However, if the routing protocol is BGP, this does not matter since a BGP peer is not limited within a Layer 2 domain.

SVI Encap Scope

The SVI Encap Scope option was introduced in APIC Release 2.3(1). This option is located under Tenant > Networking > External Routed Networks > L3Out > Logical Node Profiles > Logical Interface Profiles > SVI tab. The configurable options are “VRF” and “Local”. The default value is “Local”, which provides the same behavior as previous ACI releases, that is what was described in the previous subsection “L3Out bridge domain”.


L3Out SVI Encap Scope in GUI (APIC Release 3.2)

SVI Encap Scope “VRF” allows multiple L3Outs in the same VRF to share an L3Out BD, which means to share the same access-encap VLAN even on the same leaf. The main scenarios of this feature are;

● Scenario 1: Multiple routing protocols on the same SVI on the same leaf

● Scenario 2: Granular route control over each BGP peer on the same leaf (by using a dedicated L3Out for each BGP peer)

Details of each scenario are explained below.

Scenario 1: Multiple routing protocols on a same SVI on the same leaf


Multiple routing protocols on a same SVI with SVI Encap Scope

As mentioned in the “L3Out bridge domain” subsection, by default or with SVI Encap Scope “Local”, each L3Out allocates an L3Out BD/SVI per access-encap VLAN. Hence, two L3Outs with the same access-encap VLAN cannot coexist on the same border leaf. In ACI, each L3Out can be configured only for one routing protocol. This means that by default one L3Out SVI on a given leaf cannot run multiple routing protocols. BGP and OSPF is an exception as ACI allows the configuration of OSPF and BGP in the same L3Out to provide IGP reachability for BGP.

With SVI Encap Scope “VRF”, it is possible to configure two routing protocols on the same leaf on the same interface for the same VLAN encapsulation by configuring the same SVI parameters, like the IP addresses on two L3Outs, as shown in Figure 22.

Note:

The Encap Scope “VRF” option also helps when two different OSPF areas need to be deployed on the same SVI on the same leaf switch, because you can now configure two L3Outs, one per OSPF area, on the same leaf and SVI.

Scenario 2: Granular route control over each BGP peer on the same leaf


Regular BGP route control (left) and granular BGP route control with SVI Encap Scope VRF (right)

If you want to configure route control with BGP, you need to use the configuration called “Export Route Control Subnet”. This is a per-L3Out configuration. In the case of BGP, ACI internally creates a route map per each L3Out and per leaf, to apply the route-control policy configured on the APIC. See the “L3Out Transit Routing” section for details.

Because of this, when there are multiple BGP peers and different route-control policies need to be applied to each peer, a separate L3Out is required for each BGP peer. Creating an L3Out for each peer is not a problem when each BGP peer is connected to different border leaf switches. However, if all the BGP peers are connected to the same border leaf, by default, it is feasible only by using different VLANs/SVIs for each BGP peer. This is because it was not allowed to use the same access-encap VLAN on the same leaf for two different L3Outs.

Although having two different VLANs for each BGP peer may be doable, many times there are multiple BGP peers behind a single router or switch connected to a border leaf due to the nature of BGP peers that can be multi-hop L3 adjacencies, as Figure 23 shows. In such situations, it would be difficult to have two different VLANs for each BGP peer.

Starting with APIC Release 2.3(1) with an SVI Encap Scope “VRF” option, multiple L3Outs in the same VRF can share the same access-encap VLAN/SVI because an L3Out BD, which is per access-encap VLAN, can span across multiple L3Outs. As a result, multiple BGP L3Outs can be deployed on the same border leaf with the same VLAN/SVI so that different route-control rules can be used for each BGP peer behind the same VLAN/SVI (see the right side of Figure 23).

Note:

This route control per peer with SVI Encap Scope “VRF” option is only for BGP, because ACI creates a route map per VRF and per leaf for OSPF and EIGRP instead of per L3Out, as in BGP. See the “L3Out subnet scope options” or “L3Out Transit Routing” sections for details on route-control policy such as “Export Route Control Subnet”.


Note:

For both scenarios 1 and 2, the SVI parameters such as IP and MTU for the same leaf need to be the same on both L3Outs. This is because the parameters must be applied on the same SVI on the border leaf, and there cannot be any conflicts.


Note:

Scenario 2 (route control per BGP peers) can also be achieved with the feature “route map per BGP peer”, which was introduced in APIC Release 4.2(1).

SVI Auto State

The SVI Auto State option was introduced in APIC Release 2.2(3) and 3.1(1). It is not available on 3.0(x). This option is located under Tenant > Networking > External Routed Networks > L3Out > Logical Node Profiles > Logical Interface Profiles > SVI tab. The option is disabled by default, which provides the same behavior as previous ACI releases.


L3Out SVI Auto State in GUI (APIC Release 3.2)

In ACI, an SVI on a leaf is always up regardless of its VLAN member ports’ status. Although this is typically not a problem, it could pose a problem when using static routes. Enabling SVI Auto State allows a border leaf to bring an L3Out SVI down when all its VLAN member ports are down. The following is an example use case for SVI Auto State.


SVI Auto State disabled, with static route

Figure 25 shows a problem with static routes when SVI Auto State is disabled. L3Out 1 in Figure 25 has SVI 10 configured on border leaf switches 1 and 2 with vPC and SVI Auto State disabled. L3Out 1 also has static route 1.0.0.0/24 configured on both leaf 1 and leaf 2. L3Out 2 in Figure 25, on the other hand, is deployed on leaf 3 with “Export Route Control Subnet” for 1.0.0.0/24, which tries to redistribute 1.0.0.0/24 if it is in the routing table of leaf 3. In this situation, even if the vPC interfaces connected to the external router 10.0.0.1 on leaf 1 and leaf 2 go down, and there are no member ports for VLAN 10, the L3Out SVI 10 on both leaf 1 and leaf 2 remain up, because SVI Auto State is disabled. Hence the static route with a next-hop in the SVI 10 subnet remains in each routing table. Leaf 3 can still see the route received via MP-BGP, and it redistributes and advertises it out even though the ACI fabric no longer has any reachability for that route. One of the options to avoid this is to enable SVI Auto State on L3Out SVI 10.


SVI Auto State enabled, with static route

Figure 26 shows how enabling SVI Auto State helps with the problem mentioned in Figure 25. When SVI Auto State is enabled and all the VLAN member interfaces (the vPC interfaces in Figure 26) go down, the L3Out SVI 10 on leaf 1 and leaf 2 also goes down. This results in leaf 1 and leaf 2 removing the static route with a next-hop in the SVI 10 subnet. Hence, leaf 3 no longer sees 1.0.0.0/24 via MP-BGP, and the redistribution and advertisement for an unreachable route stop.

Note:

The problem mentioned in Figure 25 can be avoided by using BFD for the static route as an alternative option to enabling SVI Auto State.


Note:

When Auto State is enabled on the L3Out SVI with vPC, SVIs are brought down only when VLAN member ports on both the vPC pair of leaf switches are down.

L3Out static routes

In Cisco ACI, static routes are configured as part of L3Out. Static routes are configured on each Logical Node Profile under “Tenant > Networking > External Routed Networks > L3Out > Logical Node Profiles > Node > Static Routes”. If only static routes are required without any dynamic routing protocols, users can leave the dynamic routing protocol checkbox on the root L3Out component blank and configure only the Logical Node Profile with Static Routes and the Logical Interface Profile. This still requires associating the VRF and the External Routed Domain on the root L3Out component.


L3Out static-route configuration in GUI (APIC Release 3.2)


L3Out static route in GUI (APIC Release 4.1)

● Prefix This is to configure a prefix itself for the static route.

● Preference (Base Preference) This is to configure the administrative distance (AD) for the static route. The AD can be set either per static route or per next-hop. This preference option (right beneath the Prefix field) is to configure the AD to be used as a fallback in case a Preference for the next-hop IP address is not specified or zero.

● Route Control This is to enable BFD (bidirectional forwarding detection) on the static route. Please refer to the “L3Out BFD” section for details.

● Track Policy This is for the IP SLA feature that was introduced from APIC Release 4.1(1). It sets a track policy that monitors the reachability of a group of IP addresses as an indicator of the validity of the static route. The tracking configuration can be performed in multiple ways or at multiple levels of granularity: as part of the static route or per next-hop address. The track policy at the static-route level (this field) retrieves information about the reachability of a set of IP addresses from the routing table, and if the reachability condition is met, the static route is kept in the routing table.

The validity of each individual next-hop can also be monitored separately with two similar configurations: an IP SLA Policy or a Track Policy under each Next Hop Address. The IP SLA policy for the next-hop address defines how to check the reachability of the next-hop itself (that is, which protocols to use). The track policy for next-hop address instead defines which IP addresses to check (other than the next-hop address itself) in order for the next-hop to be considered valid.

The Track Policy of the static route itself takes precedence when both the static route and each next-hop have Track Policy or IP SLA Policy. See the “IP SLA tracking for L3Out static routes” subsection, below, for details.

● Next Hop Addresses

This is to configure next-hop IP addresses for the static route. One or more next-hops can be configured for the same static route prefix. Beginning in APIC Release 1.2(2), a Null-0 next-hop is automatically created when there is no Next Hop Addresses entry configured.

◦ Next Hop IP IP address to be used as a next-hop for the static route

◦ Preference Administrative distance for this next-hop IP. If it is zero or unspecified, ACI uses the Base Preference to program the hardware.

◦ Next Hop Type This can be either None or Prefix. None is for the NULL interface. Next Hop IP must be 0.0.0.0/0 for None. Prefix is to specify the actual next-hop IP instead of NULL (0.0.0.0/0).

This option was introduced in APIC Release 4.1(1) to have a NULL next-hop and non-NULL next-hops for one static route at the same time. Prior to Release 4.1(1), a static route could have either only a NULL next-hop or only non-NULL next-hops.

◦ IP SLA Policy This option sets IP SLA policy directly on the next-hop IP instead of using a Track Policy. This is to track the next-hop availability by probing the next-hop IP itself instead of other IP addresses that are grouped and monitored by a track policy. When the SLA condition is not met, the next-hop is removed from the routing table.

A Next Hop Addresses entry can have either an IP SLA Policy or a Track Policy, not both.

The IP SLA Policy and the Track Policy were introduced from APIC Release 4.1(1). See the “IP SLA tracking for L3Out static routes” subsection for details.

◦ Track Policy This configuration defines a group of IP addresses whose reachability is used by ACI to decide whether this next-hop entry for the static route should be kept in the routing table or not, instead of directly monitoring the next-hop IP. The monitoring of the IP addresses defined in the Track Policy is performed using the protocol defined in the IP SLA Policy nested in this track policy. When the reachability condition in the track policy is met, the next-hop is kept in the routing table.

A Next Hop Addresses entry can have either IP SLA Policy or Track Policy, not both.

The IP SLA Policy and the Track Policy were introduced from APIC Release 4.1(1). See the “IP SLA tracking for L3Out static routes” subsection for details.

IP SLA tracking for L3Out static routes

This feature was introduced in APIC Release 4.1(1). This feature checks the validity of an L3Out static route by probing a group of IP addresses. A track list defines which IP addresses to monitor. This list contains track members that consist of probe IP addresses and IP SLA method, and a threshold for a value that is calculated based on the status of each track member. Based on the threshold, a track list brings down or up the static route itself or the next-hop depending on where the configuration is attached. A track list can be attached to either the static route itself and/or to its next-hop, and the static route and/or the next-hop is kept in the routing table when a track list shows that there is enough reachability based on the threshold condition configured in the track list. When a track list is configured on both a static route and its next-hops, and the track list for the static route itself meets the threshold for the down condition, the entire static route is removed from the routing table regardless of the track list status of the next-hops of the static route.


IP SLA for L3Out static routes

As Figure 29 depicts, ACI can probe the next-hop IP itself, external IPs, and endpoint IPs that may be relevant for the static route.


Example of a track list (IP SLA) for L3Out static routes

Figure 30 is an example of track list components and configuration. This track list has two track members (probe IPs). One is for the external IP 10.10.0.1 behind L3Out BGP1 (this is configured as scope of the track member in the APIC GUI) with ICMP as the protocol used by the probing traffic, which is sent every 60 seconds. Another is for the external IP 10.10.0.2 behind L3Out BGP 1 with TCP destination port 22 as the probing traffic sent every 60 seconds. As for the threshold condition, this track list uses percentages, with Up 51 percent and Down 50 percent. This means the track list is marked as down when only 50 percent of the track members are up. In this example, if one of the track members becomes unreachable, the percentage goes down to 50 percent, and the track list is marked as down. When the ratio of track members that are up reaches 51 percent, the track list is marked as up again. In this example, if both track members become reachable again, the percentage goes up to 100 percent, and the track list is marked as up again.

This track list needs to be attached to either a static route or its next-hop configurations to take effect. In order to associate the track list to a static route or to a next-hop, the Track Policy field is used in the static route or its next-hop configurations.


IP SLA (Track List) for L3Out Static Route in GUI (APIC release 4.1)

Track Lists

● Type of Track List threshold This can be either Percentage or Weight. This parameter cannot be changed once a track list is created. Percentage This is the ratio of the reachable track members over the total number of members. Weight This is the sum of the weights for the reachable track members.

● Up Value (percentage or weight) When the percentage or weight reaches this value, the track list is marked as up if it was down prior to this. Static routes or next-hops using the track list will be brought back up in the routing table accordingly.

● Down Value (percentage or weight) When the percentage or weight reaches this value, the track list is marked as down if it was up prior to this. Static routes or next-hops using the track list will be brought down in the routing table accordingly.

Track Member (probe IP)

The track member is the configuration that defines an IP to which ACI sends a probe, which protocol should be used to verify the reachability of this IP, and where (L3Out, BD, etc..) ACI should try and reach the track member.

● Destination IP The target IP address to probe. It can be the next-hop IP of the static route, an external IP, or an endpoint IP.

● Scope of Track Member The component (L3Out or BD) on which the destination IP should exist.

● IP SLA Policy The IP SLA Monitoring Policy that defines how to probe the destination IP in terms of which protocol to use and/or which L4 port.

IP SLA Monitoring Policy

● SLA Frequency (sec) The frequency in seconds to probe the track member IP. The default value is 60 seconds.

● SLA Type The type that defines which protocol is used for the probing packet. For an L3Out static route SLA, the supported options are ICMP or TCP with destination port.

Note:

Configuring an IP SLA policy or a track list on a next-hop under an L3Out static route are functionally equivalent. The IP SLA policy on a next-hop is just a shortcut where APIC internally creates a track list with one track member with the next-hop IP as the probe IP (destination IP) and the IP SLA policy. This is equivalent to manually configuring the track list and associating it to the Track Policy.

L3Out BGP


ACI and BGP AS

L3Out requires infra MP-BGP in which users configure route reflectors and the BGP AS number. This BGP AS number for infra MP-BGP is the ACI BGP AS, and an L3Out with BGP automatically belongs to the same BGP AS. Hence, external devices need to peer with the ACI BGP AS (as shown in Figure 32) unless ACI uses a local-as configuration in the BGP Peer Connectivity Profile to make its BGP AS look like something else to the peer. EBGP connectivity support was added beginning with APIC Release 1.1(1).

The supported method/IGP for BGP peering IP reachability for both iBGP and eBGP is as follows:

● Direct connection

● Static route

● OSPF

Supported source interfaces for BGP peering for both iBGP and eBGP are as follows;

● Loopback interface in Logical Node Profile

● Routed Interface, Routed Sub-interface, and SVI in Logical Interface Profile

Note:

A BGP session will be sourced only from a primary IP address of each interface even when secondary IP addresses are configured on the interface.

Basic configuration example


iBGP configuration diagrams

Figure 33 illustrates configuration examples for iBGP with peering on loopback and non-loopback interfaces. The key components to configure BGP in an ACI L3Out are the following:

● Enable BGP on the root of the L3Out.

● Configure BGP Peer Connectivity Profiles.

◦ Source is loopback: Configure under the Node Profile.

◦ Source is non-loopback: Configure under the Interface Profile.

● Configure static route (or OSPF) for BGP peer reachability.

◦ This is only when the peer is multiple-hops away, as in the case of loopback peering.

Just as with any other L3Out configuration, users need to associate VRF and External Routed Domain on the L3Out root as well.

Note:

The BGP Peer Connectivity Profiles contain many options. However, the minimum iBGP configuration just requires the neighbor IP address. For details on other options, please see “BGP protocol options” in this section.


EBGP configuration diagrams

Figure 34 illustrates the configuration example for eBGP with peering on loopback and non-loopback interfaces. Most of the configurations are the same as the iBGP configuration in Figure 33. There are two more requirements specific to eBGP peering under the BGP Peer Connectivity Profile.

● Remote AS (not optional for eBGP)

● EBGP multihop (This is only when the peer is multiple-hops away, as in the case of loopback peering.)

Note:

The BGP Peer Connectivity Profiles contain many options. However, the minimum eBGP configuration just requires the neighbor IP address, Remote AS Number, and eBGP multihop. For details on other options, please see “BGP protocol options” in this section.

Figure 35 and Figure 36 show the APIC GUI configuration for eBGP peering with a loopback, based on the configuration example in Figure 34 (eBGP peering with loopback). In case users need to use OSPF instead of a static route as the IGP, users can enable and configure OSPF in the same L3Out, or in another L3Out on the same border leaf. If OSPF and BGP are enabled in the same L3Out, OSPF is programmed only to advertise its L3Out loopback and interfaces. See the “L3Out OSPF” section for the OSPF configuration, since it is the same with or without BGP.


EBGP peering with loopback in GUI (APIC Release 3.2)


BGP Peer Connectivity Profile for eBGP peering in GUI (APIC Release 3.2)

Limitations and guidelines

● The BGP AS number for infra MP-BGP and route reflector is used for BGP L3Outs in the entire fabric.

● EBGP-peering support was added beginning with APIC Release 1.1(1).

● OSPF or static route is supported as IGP for BGP peer reachability. When OSPF is enabled in the BGP L3Out, OSPF is programmed only to advertise its L3Out loopback and interface subnets. Routes learned from OSPF are not distributed to other leaf switches via MP-BG.

● Sourcing BGP sessions via secondary IPs is not supported.

● L3Out BGP does not have an equivalent configuration for the “network <subnet>” command from standalone Cisco NX-OS. Instead, all configurations for subnet advertisement to outside are implemented via redistribution.

● By default, all routing protocols, static routes, and L3Out interface subnets are redistributed to BGP. This default redistribution is required for infra MP-BGP.

● Even though almost all types of routes are redistributed to the L3Out BGP, ACI does not advertise any of them to the outside, by default. To advertise a subnet, appropriate configurations are required such as BD subnet advertisement or Transit Routing. This is implemented by internally utilizing an outbound route-map for BGP peers.

● The outbound BGP peer route-map is per L3Out instead of per BGP peers. Hence, the subnet advertisement configuration in one L3Out will be applied to all BGP peers in the same L3Out. Please see the “BD subnet advertisement” section or the “Internal route-map for Transit Routing” section for details about internal route-map implementation. This was enhanced in APIC Release 4.2, which supports per-BGP peer route-map.

● There is an inbound BGP peer route-map per L3Out as well; this is for Import Route Control Enforcement. The same limitation as for outbound route-maps apply. This was enhanced in APIC Release 4.2, which supports per-BGP peer route-map.

● The outbound or inbound route-map for BGP peers can be checked using the following command:

Leaf1# show bgp ipv4 unicast neighbors vrf TK:VRF1 | egrep 'BGP nei|Inb|Outb'

BGP neighbor is 9.9.9.9, remote AS 65009, ebgp link, Peer index 1

Inbound route-map configured is imp-L3Out-BGP-peer-2916353, handle obtained

Outbound route-map configured is exp-L3Out-BGP-peer-2916353, handle obtained

The route-map name is in a form of “imp-L3Out-<L3Out name>-peer-<VRF VNID>” or “exp-L3Out-<L3Out name>-peer-<VRF VNID>”.

BGP protocol options – neighbor level


BGP Peer Connectivity Profile in GUI (APIC Release 3.2)

This subsection goes over all BGP protocol options per neighbor that can be configured on the BGP Peer Connectivity Profile under the Logical Node Profile or the Logical Interface Profile located under “Tenant > Networking > External Routed Networks > L3Out”. For various BGP set rules, see the “L3Out Route Profile / Route Map” section.

● Dynamic Neighbor (Prefix Peers) This feature was introduced from APIC Release 1.2(2). This feature is to dynamically establish BGP peering with multiple neighbors by configuring a subnet (10.0.0.0/30 in Figure 38) instead of an individual IP address. This allows the L3Out to dynamically establish BGP peering with any IPs in the subnet. However, BGP with dynamic neighbor configuration does not start a BGP session by itself. Hence, the other side needs to explicitly configure the ACI border leaf IP to start a BGP session.


BGP Dynamic Neighbor

The standalone NX-OS equivalent command is the following:

router bgp 65001

vrf TK:VRF1

neighbor 10.0.0.0/30

This feature is called Prefix Peers in standalone NX-OS.

● BGP Controls Send Community and Send Extended Community have been supported from the first APIC release 1.0. The other options were introduced in later APIC releases.

◦ Allow Self AS This feature was introduced in APIC Release 1.1(1) as a part of eBGP peering support. This option allows ACI to receive routes from the eBGP neighbor even if the routes have ACI BGP AS number in its AS_PATH. This option is valid only for eBGP peers.

The standalone NX-OS equivalent commands are the following:

router bgp 65001

neighbor 9.9.9.9

address-family ipv4 unicast

allowas-in <number>

The <number> in the above equivalent command is the maximum count of Self AS occurrences in the AS_PATH. This <number> option is covered by Allowed Self AS Count option, described below.

◦ AS Override This feature was introduced in APIC Release 3.1(2). It allows ACI to overwrite a remote AS in the AS_PATH with ACI BGP AS. In ACI, it is typically used when performing Transit Routing from eBGP L3Out to another eBGP L3Out with the same AS number. Otherwise, an eBGP peer device may not accept the route from ACI because of AS_PATH loop prevention. When this option is enabled, Disable Peer AS Check option also needs to be enabled. This option is valid only for eBGP peers.

The standalone NX-OS equivalent commands are the following:

router bgp 65001

neighbor 9.9.9.9

address-family ipv4 unicast

as-override

◦ Disable Peer AS Check This feature was introduced in APIC Release 1.1(1) as a part of eBGP peering support. It allows ACI to advertise a route to the eBGP peer even if the most recent AS in the AS_PATH of the route is the same as the remote AS for the eBGP peer. Without this option, ACI does not advertise such routes, just as a standalone NX-OS does not. If the remote AS in the AS_PATH is not the most recent one, the route advertisement is not affected by this option, and the route is advertised without any additional configurations. This option is valid only for eBGP peers.

The standalone NX-OS equivalent commands are the following:

router bgp 65001

neighbor 9.9.9.9

address-family ipv4 unicast

disable-peer-as-check

◦ Next-hop Self This feature allows ACI to update the next-hop when advertising a route from eBGP peer to iBGP peer. By default, route advertisement between iBGP peers keep the original next-hop of the route while the one between eBGP peers always updates the next-hop with a self IP.

The standalone NX-OS equivalent commands are the following:

router bgp 65001

neighbor 9.9.9.9

address-family ipv4 unicast

next-hop-self

◦ Send Community This feature has been supported from the first APIC release 1.0. This option needs to be enabled for ACI L3Out to advertise routes with a BGP Community attribute, such as AS2:NN format. Otherwise, the BGP Community attribute is stripped when routes are advertised to the outside. See the “L3Out Route Profile / Route Map” section for details about how to set or match communities on L3Out.

The standalone NX-OS equivalent commands are the following:

router bgp 65001

neighbor 9.9.9.9

address-family ipv4 unicast

send-community {standard}

◦ Send Extended Community This feature has been supported from the first APIC release 1.0. This option needs to be enabled for ACI L3Out to advertise routes along with the BGP Extended Community attribute, such as RT:AS2:NN, RT:AS4:NN, etc. Otherwise, the BGP Extended Community attribute is stripped when routes are advertised to the outside. See the “L3Out Route Profile / Route Map” section for details about how to set or match extended communities on L3Out.

The standalone NX-OS equivalent commands are the following:

router bgp 65001

neighbor 9.9.9.9

address-family ipv4 unicast

send-community extended

● Password / Confirm Password This feature has been supported from the first APIC release 1.0. When configured, L3Out BGP uses MD5 authentication on BGP TCP session. Password configuration can be reset via “Reset Password” by right clicking the BGP Peer Connectivity Profile or via the edit/action dropdown as shown in Figure 39.


BGP Peer Connectivity Profile (Reset Password)

The standalone NX-OS equivalent commands are the following:

router bgp 65001

neighbor 9.9.9.9

password <your password>

● Allowed Self AS Count This feature was introduced from APIC Release 1.1(1) as a part of eBGP peering support. This feature is to set the maximum count for the Allow Self AS option under BGP controls. See above for details on the Allow Self AS option.

● Peer Controls

◦ Bidirectional Forwarding Detection (BFD) This feature was introduced in APIC Release 1.2(2). It is used to enable BFD on the BGP neighbor. See the “L3Out BFD” section for details. The standalone NX-OS equivalent commands are the following:

router bgp 65001

vrf TK:VRF1

neighbor 9.9.9.9

bfd

◦ Disable Connected Check This feature was introduced in APIC Release 1.1(1) as a part of eBGP peering support. For eBGP peering, BGP process checks whether the neighbor IP is on the same subnet as any of its local interfaces to see if the neighbor IP is directly connected. If not, it automatically assumes the TTL needs to be larger than 1. Hence, when BGP is peering via loopbacks with directly connected routers, the BGP peering will be rejected without the eBGP multihop TTL being set to 2 or larger, even though TTL 1 is technically enough. Disable Connected Check can be used in such a scenario as an alternative to increasing the eBGP multihop TTL in cases where there is a security concern in increasing TTL unnecessarily. The standalone NX-OS equivalent commands are the following:

router bgp 65001

neighbor 9.9.9.9

disable-connected-check

● EBGP Multihop TTL This feature was introduced in APIC release 1.1(1) as a part of eBGP peering support. For eBGP peering, BGP control packets use a TTL of 1 by default. This needs to be increased via this option in case the neighbor IP is multihops away. If the required TTL is 1, but the neighbor IP is not in directly connected subnets (for example, the neighbor IP is a loopback IP on a directly connected router), Disable Connected Check under Peer Controls can be used instead. The standalone NX-OS equivalent commands are the following:

router bgp 65001

neighbor 9.9.9.9

ebgp-multihop <number>

● Weight This feature was introduced in APIC Release 1.2(2). This sets a default value of Cisco proprietary BGP path attribute Weight on all the routes advertised to this neighbor. The standalone NX-OS equivalent commands are the following:

router bgp 65001

neighbor 9.9.9.9

address-family ipv4 unicast

weight <number>

● Private AS Control This feature was introduced in APIC Release 1.2(2). These options are valid only when ACI BGP AS is a public AS number.

◦ Remove private AS In outgoing eBGP route updates to this neighbor, remove all private AS numbers from the AS_PATH when the AS_PATH has only private AS numbers. If the neighbor remote AS is in the AS_PATH, this option is not applied.

◦ Remove all private AS In outgoing eBGP route updates to this neighbor, remove all private AS numbers from the AS_PATH regardless of whether a public AS number is included in the AS_PATH. If the neighbor remote AS is in the AS_PATH, this option is not applied. To enable this option, Remove private AS needs to be enabled.

◦ Replace private AS with local AS In outgoing eBGP route updates to this neighbor, replace all private AS numbers in the AS_PATH with ACI local AS regardless of whether a public AS or the neighbor remote AS is included in the AS_PATH. To enable this option, Remove all private AS needs to be enabled.

The standalone NX-OS equivalent commands are the following:

router bgp 65001

neighbor 9.9.9.9

address-family ipv4 unicast

remove-private-as

remove-private-as all

remove-private-as replace-as

● BGP Peer Prefix Policy (maximum prefix) This feature was introduced in APIC Release 1.2(1). This option is to set an action to take when the number of received prefixes from this neighbor exceeds the configured maximum number. Note that the number of received prefixes is calculated by the number of prefixes and their next hops. If one prefix has two next hops, it is counted as two entries. This option is activated by attaching the BGP Peer Prefix Policy (Figure 40) to the BGP Peer Connectivity Profile.


BGP Peer Prefix Policy in GUI (APIC Release 3.2)

◦ Action These are the actions that will be taken when the number of received prefixes from this neighbor exceeded the configured value.

● Log: A fault F1215 is raised to warn users that the number of received prefixes has exceeded the maximum. If the maximum number of prefixes is set to 10, the fault is raised when 11 prefixes are learned.

● Reject: A fault F1215 is raised to warn users that the number of received prefixes has exceeded the maximum. No more prefixes are learned from this neighbor until the number of received prefixes is reduced. If the maximum number of prefixes is set to 10, the fault is raised when 11 prefixes are learned and the 12th prefix is rejected.

● Restart: The BGP peer is shut down due to the maximum prefix violation, and a fault F1214 is raised. The BGP peer will be re-established after the configured interval if the number of received prefixes drops below the maximum number. When this action is selected, “Restart Time (min)” configuration becomes available. If the maximum number of prefixes is set to 10, the BGP peer is shut down when 11 prefixes are learned.

● Shutdown: The BGP peer is shut down due to the maximum prefix violation, and a fault F1214 is raised. If the maximum number of prefixes is set to 10, the BGP peer is shut down when 11 prefixes are learned.

◦ Maximum number of prefixes When the number of received prefixes exceeds this number, the configured action is taken. The default value is 20,000 prefixes.

◦ Threshold (percentage) When the number of received prefixes exceeds the threshold, a warning (eventRecord) is raised as a precautionary warning. If the maximum number of prefixes is 10 and the threshold is 70 percent, a warning is raised when 8 prefixes are learned. The default value is 75 percent.

The standalone NX-OS equivalent commands are the following:

router bgp 65001

vrf TK:VRF1

neighbor 9.9.9.9

address-family ipv4 unicast

maximum-prefix <prefix number> <threshold %>

maximum-prefix <prefix number> <threshold %> restart <min>

maximum-prefix <prefix number> <threshold %> warning-only

● Remote AS This feature was introduced in APIC release 1.1(1) as a part of eBGP peering support. This is required for eBGP peering to specify the AS number of the neighbor. When it is blank, it automatically uses the ACI BGP AS number. Hence, this field is optional for iBGP peering.

The standalone NX-OS equivalent commands are the following:

router bgp 65001

vrf TK:VRF1

neighbor 9.9.9.9

remote-as <AS #>

● Local AS / Local AS Config This feature was introduced in APIC Release 1.1(1) as a part of eBGP peering support. This feature is used when L3Out needs to disguise its own BGP AS with the configured local AS to peer with this neighbor. When this feature is used, for this neighbor it will look like there is one more AS (local AS) between itself and the ACI BGP AS. Hence, the neighbor will peer with the configured local AS instead of the real ACI BGP AS. In such situations, both the local AS and the real ACI BGP AS are added to the AS_PATH of routes advertised to the neighbor. The local AS is also prepended to routes learned from the neighbor.

The following additional options are available, as in a standalone NX-OS:

◦ no-prepend This option prevents ACI from prepending the local AS in the AS_PATH of routes learned from this neighbor.

◦ no-prepend, replace-as This option allows ACI to add only a local AS, instead of both a local AS and a real ACI BGP AS, to AS_PATH of routes advertised to this neighbor on top of the no-prepend option effect.

◦ no-prepend, replace-as, dual-as This option allows the neighbor to peer with both a local AS and a real ACI BGP AS on top of the no-prepend and replace-as option effect. However, note that the neighbor receives route updates with AS_PATH with AS that it is peering with, regardless of whether it is a local AS or a real ACI BGP AS.

The standalone NX-OS equivalent commands are the following:

router bgp 65001

vrf TK:VRF1

neighbor 9.9.9.9

local-as <AS #>

local-as <AS #> no-prepend

local-as <AS #> no-prepend replace-as

local-as <AS #> no-prepend replace-as dual-as

BGP protocol options – L3Out/node level

● BGP Protocol Profile This is a profile to apply a BGP Timer Policy and BGP Best Path Control Policy per node via the Logical Node Interface Profile.


BGP Protocol Profile in GUI (APIC Release 3.2)

◦ BGP Timers This can be applied per VRF as well as per node. The details are in the “BGP protocol options – VRF level” subsection, below.

◦ AS-Path Policy This is to apply AS-Path Policy (BGP Best Path Control Policy) per node. This option was introduced in APIC Release 3.2(7). APIC Release 4.0, 4.1, and 4.2(1) do not support this option. When “AS-Path Control” is enabled, it allows ECMP across different eBGP peers (that is, different AS paths). The standalone NX-OS equivalent commands are the following:

router bgp 65001

vrf TK:VRF1

bestpath as-path multipath-relax

◦ BGP Route Dampening This feature was introduced in APIC Release 1.2(2). This is used to stop advertising flapping routes. When a BGP route status changes from available to unavailable or vice versa, a penalty of 1000 is added to the route. When the penalty exceeds the Suppress Limit, the route is marked as dampened, and a router stops advertising the route. The penalty of each route will be reduced by half once the Half Life time has passed. Once the penalty goes below half of the Reuse Limit, the penalty is completely removed from the route.

In ACI, these route-dampening parameters are configured via Set Policy in Route Profile without any Match Policy. The Route Profile for BGP Route Dampening is on the tenant level instead of each individual L3Out level. Please see the “L3Out Route Profile / Route Map” section for details on Route Profile itself.


BGP Route Dampening Policy in GUI (APIC Release 3.2)

◦ Half Life (minutes) The penalty of each route will be reduced by half when the Half Life time has passed.

◦ Reuse Limit Routes will be used and advertised again once the penalty of routes go below the Reuse Limit.

◦ Suppress Limit Routes will be suppressed and not be advertised once the penalty of routes exceeds the Suppress Limit.

◦ Max Suppress Time (minutes) Routes will be unsuppressed and advertised again after Max Suppress Time, regardless of penalty. This is to ensure the prefix does not get dampened indefinitely.

The standalone NX-OS equivalent commands are the following:

route-map BGP_dampening

set dampening <Half Life> <Reuse Limit> <Suppress Limit> <Max Suppress>

router bgp 65001

vrf TK:VRF1

address-family ipv4 unicast

dampening route-map BGP_dampening

BGP protocol options – VRF level

● BGP timer policy


BGP timer policy in GUI (APIC Release 3.2)

BGP timer policy itself is located under “Tenant > Policies > BGP > BGP Timers”. It is associated to a VRF under “Tenant > Networking > VRFs”. Beginning in APIC Release 2.2(2), BGP Timer Policy can be configured via right-clicking on the Logical Node Profile to make the scope of BGP Timer Policy per Node per VRF instead of per VRF.

◦ Keepalive Interval (sec) / Hold Interval (sec) Once a BGP peer is established, keepalive messages are sent to the neighbor once in every Keepalive Interval. If no keepalive message was received within a Hold Interval, the BGP peer is considered down. The default value is 60 seconds for a Keepalive Interval and 180 seconds for a Hold Interval (= 3 x Keepalive Interval). Configured intervals take effect only after a new BGP session is established because the Hold Interval is exchanged via a BGP OPEN message and negotiated to the lower value. If the locally configured Keepalive Interval is larger than one-third (33 percent) of the negotiated Hold Interval, one-third of the negotiated Hold Interval is used as a Keepalive Interval instead of the configured value.

The standalone NX-OS equivalent commands are the following:

router bgp 65001

vrf TK:VRF1

timers bgp <keepalive interval> <hold interval>

◦ Stale Interval (sec) When a Graceful Restart is in progress, the routes previously received from the peer are still used for forwarding but marked as stale. Once the session between two routers is re-established and route information is synced again, all the stale routes are deleted and the latest routes from the latest exchange are used. The Stale Interval is a timer to delete those stale routes in case the session is not re-established within this interval. This interval is applied locally. The default value is 300 seconds.

The standalone NX-OS equivalent commands are the following:

router bgp 65001

vrf TK:VRF1

graceful-restart stalepath-time <stale interval>

◦ Graceful Restart Controls When Graceful Restart is in progress, one router may be restarting its routing process and triggering Graceful Restart. Also, its peer may be stable but simply helping the Graceful Restart operation with the restarting router. The latter is called a Graceful Restart Receiving device or Graceful Restart Helper. Cisco ACI provides only Graceful Restart Helper capability because ACI does not support stateful supervisor switchover within each individual switch node. Only a cold reboot is available. Instead, routing protocol High Availability (HA) is achieved via utilizing multiple switch nodes. Because of this,

◦ Graceful Restart Helper Enables Graceful Restart Helper Capability within the VRF. The default is enabled.

The standalone NX-OS equivalent commands are the following:

router bgp 65001

vrf TK:VRF1

no graceful-restart

graceful-restart-helper

In Graceful Restart, there are two major timers. One is called restart timer, which is configured and advertised by a restarting router to inform its peer of the maximum time it will take for the restarting router to finish restarting its routing protocol. A Graceful Restart Helper device will delete all stale routes once this timer is expired by assuming the restarting device failed to restart its routing protocol. Hence, this timer is not configured on ACI (Graceful Restart Helper). Another timer, called a stale timer, is configured and used by the Graceful Restart Helper device. Please see the Stale Interval option, above.

◦ Maximum AS Limit This feature was introduced in APIC Release 2.0(1). It discards eBGP routes that have a number of AS-path segments that exceed the specified limit. The default value is zero, which implies no maximum as limit.

The standalone NX-OS equivalent commands are the following:

router bgp 65001

vrf TK:VRF1

maxas-limit <number>

● Address Family Context


BGP Address Family Context Policy in GUI (APIC Release 3.2)

BGP Address Family Context Policy itself is located under “Tenant > Policies > BGP > BGP Address Family Context”. It is associated to a VRF under “Tenant > Networking > VRFs”.

◦ eBGP / iBGP / Local Distance This feature was introduced in APIC Release 1.2(1). Administrative Distance (AD) for BGP. The default values are as follows:

◦ eBGP: 20

◦ iBGP: 200

◦ Local: 220 (Local AD is used for aggregate discard routes when they are installed in the RIB.)

The standalone NX-OS equivalent commands are the following:

router bgp 65001

vrf TK:VRF1

address-family ipv4 unicast

distance <eBGP AD> <iBGP AD> <Local AD>

◦ eBGP / iBGP Max ECMP This feature was introduced in APIC Release 3.0(1). It configures the maximum number of paths that BGP adds to the route table for ECMP.

The standalone NX-OS equivalent commands are the following:

router bgp 65001

vrf TK:VRF1

address-family ipv4 unicast

maximum-paths <eBGP ECMP number>

maximum-paths ibgp <iBGP ECMP number>

◦ Enable Host Route Leak This feature was introduced in APIC Release 2.1(1). This is for the GOLF feature. This is enabled when users need to advertise eVPN Type-2 (host MAC/IP) routes via GOLF on top of eVPN type-5 routes (BD subnets). Please check the “GOLF” section in the Cisco APIC Layer 3 Networking Configuration Guide.

BGP route summarization

This feature was introduced in APIC Release 1.2(2). This feature is to advertise only a summarized prefix for BD subnets or Transit Routes from the ACI BGP L3Out to outside. The behavior is equivalent to “aggregate-address <prefix> summary-only” in NX-OS commands.


BGP Route Summarization in GUI (APIC Release 3.2)

BGP Routing Summarization in ACI is configured by adding a route summarization policy to an L3Out subnet with scope “Export Route Control Subnet,” because it is used to advertise (export) routes from ACI to outside. Please refer to the “L3Out Transit Routing” section for details about the “Export Route Control Subnet” scope.

By adding a route-summarization policy to the L3Out subnet, as shown in Figure 45, the border leaf will try to create a Null-0 entry for the summarized route (192.168.0.0/16 in Figure 45), which will be advertised to its BGP peers. Please be aware that, just like a normal BGP router, the summarization will not occur if no contributing routes exist in the IPv4/IPv6 BGP table for the user VRF on the border leaf.

A supported configurable option is the following:

● Generate AS-SET information When enabled, the summarized route will have an AS-PATH attribute and community information from the contributing routes.


Example of BGP Route Summarization topology

Figure 46 depicts when L3Out 3 advertises only a summarized transit subnet information (192.168.0.0/16) instead of each subnet (192.168.1.0/24 and 192.168.2.0/24). The summarized route with Null-0 next-hop is advertised only to the outside and is not advertised to other leaf switches via infra MP-BGP. In case BGP is summarizing BD subnets, a correct BD subnet advertisement configuration is required for at least one contributing BD subnet on top of the summarization configuration (see the “ACI BD subnet advertisement” section).

The standalone NX-OS equivalent commands are the following:

router bgp 65001

vrf TK:VRF1

address-family ipv4 unicast

aggregate-address <prefix> summary-only {as-set}

BGP default route advertisement

There are two methods to advertise a default route (0.0.0.0/0) from BGP L3Out to the outside.

1. Transit Routing

2. Default Route Leak Policy

Transit Routing will advertise a default route that is learned from another L3Out or possibly a static route configured on another L3Out. See the “L3Out Transit Routing” section for details on Transit Routing.

Default Route Leak Policy is equivalent to “default-originate” in standalone NX-OS.


Default Route Leak Policy for BGP in GUI (APIC Release 3.2)

Default Route Leak Policy was introduced in APIC Release 1.1(1) and can be created under an L3Out through either of the following means:

● “Create Default Route Leak Policy” from a dropdown menu at the top right in L3Out

● “Create Default Route Leak Policy” from the right-click menu in L3Out itself

Default Route Leak Policy has the following parameters:

● Always Ignore this option for BGP.

● Criteria Use “Leak Default Route in Addition” when a default route needs to be advertised on top of other routes. Use “Leak Default Route Only” when only a default route should be advertised.

When “Leak Default Route Only” is selected, deny-all is applied to outbound route-maps on each BGP peer in this L3Out.

● Scope Use “Outside” for BGP.

The standalone NX-OS equivalent commands are the following:

router bgp 65001

vrf TK:VRF1

neighbor 9.9.9.9

address-family ipv4 unicast

default-originate

L3Out OSPF

Basic OSPF configuration is much simpler than BGP in ACI because there is no need to take ACI BGP AS from infra MP-BGP into consideration.

Basic configuration example


OSPF configuration diagram

Figure 48 illustrates a configuration example for OSPF with area 1, which is NSSA (Not-So-Stubby-Area) for ACI and the external router (in this case, NX-OS). The two required components specifically for OSPF are the following:

● Area and its Area Type This implies one L3Out means one OSPF area. If multiple OSPF areas need to be configured, as shown in Figure 50, multiple L3Outs also need to be configured.

● Enable OSPF on the interface This is performed via creating OSPF I/F Policy under the Logical Interface Profile. This is equivalent to performing the standalone NX-OS command “ip router ospf default area x” on the interfaces configured in the Logical Interface Profile. In the OSPF I/F Policy, although users can configure authentication, interface network type, etc., typically all the values can be left as defaults, just as in a standalone NX-OS.

Other key points to successfully establish an OSPF neighbor on ACI is to ensure all the OSPF neighbor criteria match with their neighbors, such as MTU, network mask, area ID, area type, etc.

Figure 49 shows an example of an APIC GUI configuration.


Basic configuration of OSPF in GUI (APIC Release 3.2)

Limitations and guidelines

● Each OSPF L3Out represents one OSPF area.


L3Out OSPF Area

◦ When two OSPF L3Outs are on the same leaf, those need to be in a different OSPF area.

◦ When two OSPF L3Outs are on different leaf switches, those can be in the same OSPF area.

● When OSPF is enabled in the same L3Out as BGP, OSPF is programmed only to advertise its L3Out loopback and interface subnets. In such a case, other L3Outs cannot use OSPF on the same border leaf in the same VRF (a fault F0467 will be raised). In case OSPF needs to advertise a BD subnet or perform Transit Routing on top of helping BGP peer reachability, OSPF needs to be enabled via another L3Out, but on the same interface as the BGP L3Out with the same parameters such as IP address (also use “Encap Scope VRF” in case of SVI).

● When multiple external routers are connected to an OSPF L3Out with the same SVI/VLAN, which means in the same L3Out BD, the external routers will form a neighbor directly to each other. See Figure 20 in the “L3Out bridge domain” section for details. In such a scenario, external routers will exchange routes directly by sending OSPF LSAs through the ACI L3Out BD. Hence, a situation similar to Transit Routing with “Export Route Control Subnet” may occur without “Export Route Control Subnet”.

● When advertising a BD subnet or performing Transit Routing, routes are redistributed into OSPF LSDB (Link-State Database) via a route map that is automatically created on a border leaf. This route map is shared with EIGRP and other OSPF L3Outs on the same leaf in the same VRF. It implies that subnet advertisement configuration on one L3Out may affect other L3Outs. Hence, awareness of this implementation is required when there are other L3Outs on the same leaf in the same VRF. See Figure 93 in the “L3Out Transit Routing” section for details.

● IPv6 (OSPFv3) has been supported since APIC Release 1.1(1).

OSPF protocol options – interface level


OSPF Interface Profile and Policy in GUI (APIC Release 3.2)

The interface-level OSPF configuration from the OSPF Interface Profile is applied to all interfaces in the associated Logical Interface Profile. The OSPF Interface Policy itself is located under “Tenant > Policies > OSPF > OSPF Interface”.

● Authentication

This is the OSPF authentication on each interface level.

◦ Authentication Key: This is a password used for both Simple and MD5 authentication.

◦ Authentication Key ID: This is a key ID for MD5 authentication. This needs to match with neighbor devices.

◦ Authentication Type: No authentication, Simple, or MD5.

The standalone NX-OS equivalent commands are the following:

interface eth1/1

ip ospf authentication

ip ospf authentication-key <password>


interface eth1/1

ip ospf authentication message-digest

ip ospf message-digest-key <key id> md5 <password>

● Network Type

◦ Broadcast A network with multiple routers that can communicate over a shared medium that allows broadcast traffic, such as Ethernet. Typically used for interface type SVI. OSPF DR (designate router) / BDR (backup designated router) election occurs with this type.

◦ Point-to-point A network that exists only between two routers. Typically used for interface type routed interface or subinterface. OSPF DR / BDR election does not occur with this type.

◦ Unspecified The Network Type is unspecified and takes a default value, which is broadcast.

The standalone NX-OS equivalent commands are the following:

interface eth1/1

ip ospf network <broadcast or point-to-point>

● Priority This is a priority for OSPF DR / BDR election. A router with a higher number is selected as the DR or BDR. When the priority of the neighbors is the same, the IP address is used instead. Priority 0 means this interface is not involved in a DR election. The default value is 1.

The standalone NX-OS equivalent commands are the following:

interface eth1/1

ip ospf priority <0-255>

● Cost of Interface The OSPF cost or metric on the interface. A lower number indicates a better metric. The default value is zero, which means the cost is calculated based on the bandwidth of the interface.

The standalone NX-OS equivalent commands are the following:

interface eth1/1

ip ospf cost <1-65535>

● Interface Controls

◦ Advertise subnet This allows OSPF to advertise a loopback IP address with its subnet instead of /32 without changing the network type from loopback to point-to-point. However, in ACI, a loopback IP address is always configured with /32. Hence, this option does not do anything until non /32 can be used for a loopback IP in ACI.

The standalone NX-OS equivalent commands are the following:

interface loopback1

ip ospf advertise-subnet

◦ BFD This feature was introduced in APIC Release 1.2(2). It is used to enable BFD on the OSPF interface. See the “L3Out BFD” section for details.

The standalone NX-OS equivalent command is the following:

interface eth1/1

ip ospf bfd

◦ MTU ignore This option allows the OSPF neighbor to form even with a mismatching MTU. This option is to be enabled on an OSPF interface with a lower MTU. This is not recommended in general, because MTUs on network paths should always match, not only for OSPF, but also for any other traffic that may have very large payloads. For OSPF specifically, even though a neighbor has been established, OSPF DBD packets that may be as large as the higher MTU may be dropped on the lower MTU side.

The standalone NX-OS equivalent command is the following:

interface eth1/1

ip ospf mtu-ignore

◦ Passive participation This option is to configure the interface as an OSPF passive interface.

The standalone NX-OS equivalent command is the following:

interface eth1/1

ip ospf passive-interface

● Hello Interval (sec) The interval for OSPF hello packets. This needs to match on all OSPF neighbors. The default is 10 seconds, which is the default for broadcast and point-to-point OSPF network types.

● Dead Interval (sec) When an OSPF hello is not received within this interval, the neighbor is considered down. The default is 40 seconds, which is the default (4 x Hello Interval) for broadcast and point-to-point OSPF network types.

● Retransmit Interval (sec) The interval between OSPF LSA (link-state advertisement) retransmissions. The retransmit interval occurs while the router is waiting for LSAck from the neighbor router that it received the LSA. If no LSAck is received by the end of the interval, the LSA is resent. The default is 5 seconds.

● Transmit Delay (sec) This time is added to the LS age of each LSA when it is copied in an LSU (link-state update) packet for a flooding update. This is to take the transmit delay (the time it takes for the LSU to reach the neighbor) into account so that each LSA has an appropriate age when it reaches the neighbor. Otherwise, the neighbor may have a younger LSA than the originator of the LSA. The default value is 1 second. In a modern, fast network, this value does not need to be changed.

The standalone NX-OS equivalent commands for the above timers are the following:

interface eth1/1

ip ospf hello-interval <sec>

ip ospf dead-interval <sec>

ip ospf retransmit-interval <sec>

ip ospf transmit-delay <sec>

OSPF protocol options – L3Out level


OSPF protocol options in GUI (APIC Release 3.2)

● OSPF Area ID OSPF Area ID for all interfaces in this L3Out. Area 0 can be configured with a string “backbone” as well. Otherwise, use a number.

● OSPF Area Control

◦ Send Redistributed LSAs into NSSA area This option is for the OSPF NSSA (not-so-stubby area). This is enabled by default to align with standard OSPF behavior. When this option is disabled, redistributed routes are not sent into this NSSA area from the border leaf. This is typically used when the Originate Summary LSA option is also disabled because disabling the Originate Summary LSA option creates and sends a default route to the NSSA or stub area.

The standalone NX-OS equivalent command for disabling this option is the following:

router ospf 1

vrf TK:VRF1

area 0.0.0.1 nssa no-redistribution

◦ Originate Summary LSA This option is for OSPF NSSA or Stub area. This is enabled by default to align with standard OSPF behavior. When this option is disabled, not only Type 4 and 5, but also Type 3 LSAs are not sent into the NSSA or Stub area by the border leaf. Instead, the border leaf creates and sends a default route to the area. If there is no Type 3 LSA in this area in the first place, a default route is not created.

The standalone NX-OS equivalent command for disabling this option is the following:

router ospf 1

vrf TK:VRF1

area 0.0.0.1 nssa no-summary

◦ Suppress forwarding address in translated LSA This option is for OSPF NSSA. This is disabled by default. An OSPF NSSA ABR (area border router) translates a Type-7 LSA into a Type-5, to send it across non-NSSA areas. At this time, the IP address of the originator ASBR (autonomous system boundary router) of the redistributed route is added to the LSA as a forwarding address. Under some circumstances, an OSPF router that receives a Type-5 LSA may not have a route to the forwarding address IP, and this blocks the Type 5’s route from being installed in the route table on the router. This can be avoided by enabling this option, which keeps the ACI border leaf (OSPF NSSA ABR) from adding a forwarding address to the LSA when it does a Type 7 to Type 5 translation.

This option was introduced in APIC Release 1.2(2).

The standalone NX-OS equivalent command for enabling this option is the following:

router ospf 1

vrf TK:VRF1

area 0.0.0.1 nssa translate type7 suppress-fa

● OSPF Area Type ACI supports all three OSPF area types: Regular, NSSA, and Stub area.

● OSPF Area Cost This option sets an OSPF Cost for a default route generated by the border leaf, such when the border leaf is a OSPF Stub area that generates a default route.

The standalone NX-OS equivalent command for enabling this option is the following:

router ospf 1

vrf TK:VRF1

area 0.0.0.1 default-cost <cost>

OSPF protocol options – VRF level


OSPF timer policy

OSPF timer policy (per VRF and per address family)

This policy is used under VRF but the OSPF Timer Policy itself is located under “Tenant > Policies > OSPF > OSPF Timers”. When configured per VRF instead of per Address Family, the policy is applied to both OSPFv2 and OSPFv3. When configured per Address Family, the policy is applied only to the address family (that is, if the address family is IPv4, the policy is applied to OSPFv2). Per-Address-Family policy is preferred to per-VRF policy when both are configured.

Despite its name, OSPF Timer Policy has some configuration parameters other than timers. See the following for each parameter.

● Bandwidth Reference (Mbps) The reference bandwidth used to calculate the default metrics for an OSPF interface. The default is 40,000 Mbps (40 Gbps).

The standalone NX-OS equivalent command is the following:

router ospf 1

vrf TK:VRF1

auto-cost reference-bandwidth <number> Mbps

● Admin Distance Preference The administrative distance (AD) for OSPF. The default is 110.

The standalone NX-OS equivalent command is the following:

router ospf 1

vrf TK:VRF1

distance <number>

● Maximum ECMP The maximum number of ECMP that OSPF can install into the routing table. The default is 8 paths.

The standalone NX-OS equivalent command is the following:

router ospf 1

vrf TK:VRF1

maximum-paths <number>

● Control Knobs These two knobs were introduced in APIC Release 1.2(2).

◦ Enable name lookup for router IDs To display the router IDs as DNS names in OSPF show commands. Disabled by default. The standalone NX-OS equivalent command is the following:

router ospf 1

vrf TK:VRF1

name-lookup

◦ Prefix suppression This option is to minimize the number of routes to be advertised or installed in the routing table. Disabled by default. When enabled, the following suppressions takes place:

Type-1 LSA: A link type “Link connected to: a Stub Network” from a self-generated Type-1 LSA, which represents a connected subnet for the point-to-point link, is not advertised to neighbors.

Type-2 LSA: A self-generated LSA with an LS type “Network Links”, which represents a connected subnet for the broadcast link, is advertised with /32 Network Mask instead of the real network mask. On a platform that supports prefix suppression, this /32 LSA is not installed in the routing table.

There is no standalone NX-OS equivalent command. Instead, the IOS equivalent command is the following:

router ospf 1

prefix-suppression

● Graceful Restart Control (Graceful Restart Helper) When an OSPF router triggers a Graceful Restart, it sends an opaque LSA in OSPFv2 or a grace LSA in OSPFv3 to its neighbors. This LSA includes a grace period, which is a time that the neighbor interface holds on to the LSAs from the restarting router. The neighbor that receives a grace period from the restarting router is called a Graceful Restart Helper. ACI provides only a Graceful Restart Helper capability. During the graceful period, a graceful restart helper keeps all of the LSAs that originated from the restarting router.

There is no standalone NX-OS equivalent command. The command to enable both graceful restart router and helper capabilities at the same time on standalone NX-OS is the following:

router ospf 1

vrf TK:VRF1

graceful-restart

● Initial SPF Schedule Delay Interval (ms)

● Minimum Hold Time Between SPF Calculations (ms)

● Maximum Wait Time Between SPF Calculations (ms)

The standalone NX-OS equivalent command for SPF calculation timers is the following:

router ospf 1

vrf TK:VRF1

timers throttle spf <initial delay> <minimum hold> <maximum wait>

● LSA Group Pacing Interval (sec)

The standalone NX-OS equivalent command for LSA Group Pacing is the following:

router ospf 1

vrf TK:VRF1

timers lsa-group-pacing <msec>

● LSA Generation Throttle Start Wait Interval (ms)

● LSA Generation Throttle Hold Interval (ms)

● LSA Generation Throttle Maximum Interval (ms)

The standalone NX-OS equivalent command for LSA generation timers is the following:

router ospf 1

vrf TK:VRF1

timers throttle lsa <start> <hold> <maximum>

● Minimum Interval Between Arrival of a LSA (ms)

The standalone NX-OS equivalent command for a minimum LSA Arrival Interval is the following:

router ospf 1

vrf TK:VRF1

timers lsa-arrival <msec>

● Maximum Number of Not Self-Generated LSAs

● LSA Threshold (percentage)

● LSA Maximum Action (Log or Reject)

The standalone NX-OS equivalent command for minimum LSA Arrival Interval is the following:

router ospf 1

vrf TK:VRF1

max-lsa <max lsa> <threshold> {warning-only}

OSPF route summarization

This feature was introduced in APIC Release 1.2(2). This feature is to advertise only a summarized prefix for BD subnets and/or Transit Routes from ACI OSPF L3Out to outside. In OSPF, ACI supports two summarization methods.

● Redistributed Route Summary: This is equivalent to “summary-address <prefix>/<mask>”.

● Inter-area Route Summary: This is equivalent to “area <ID> range <prefix>/<mask>”.


OSPF route summarization in GUI (APIC Release 3.2)

OSPF route summarization in ACI is configured by adding a route summarization policy to an L3Out subnet with scope “Export Route Control Subnet”, because it is to advertise (export) routes from ACI to outside. Please refer to the “L3Out Transit Routing” section for details about the “Export Route Control Subnet” scope.

By adding a route summarization policy to the L3Out subnet as shown in Figure 54, the border leaf will try to create a Null-0 entry for the summarized route (192.168.0.0/16 in Figure 54), which will be advertised to its OSPF peers. This summarized route with a Null-0 next-hop will not be advertised to other leaf switches via infra MP-BGP. Please be aware that, just as for a normal OSPF router, the summarization will not occur if no contributing routes exist in the OSPF database for the VRF on the border leaf.

Unlike BGP or EIGRP, there is one exceptional scenario where OSPF does not use redistribution to advertise routes to outside. That is when performing Transit Routing between two OSPF L3Outs on the same border leaf (please refer to the “L3Out Transit Routing” section for details). Hence, ACI supports two methods of OSPF route summarization, as mentioned above. The detailed configuration guidance and topology for both summarization methods are as follows:

OSPF Route Summarization – Redistributed Routes

Redistributed Route Summarization is used for all OSPF summarizations except one, when there are two OSPF L3Outs on the same border leaf. OSPF Redistributed Route Summarization (192.168.0.0/16 in Figure 54 and Figure 55) uses OSPF Route Summarization Policy without the “Inter-Area Enabled” option. Without the option, this will be equivalent to “summary-address 192.168.0.0/16” in standalone NX-OS. It will try to summarize all routes within the configured subnet (192.168.0.0/16 in Figure 54 or Figure 55) from Type-5 or Type-7 LSA, which are redistributed external routes. If there is no LSA that can be summarized in this way, summarization does not happen, and no Null-0 entry for 192.168.0.0/16 is created. This means that at least one subnet needs to be redistributed into the OSPF L3Out (L3Out 3 in Figure 55) explicitly for OSPF summarization to happen.


Example of OSPF route-summarization topology (redistribute routes)

Figure 55 depicts when L3Out 3 advertises only a summarized subnet (192.168.0.0/16) for transit routes from L3Out 1 and 2 (192.168.1.0/24 and 192.168.2.0/24). To perform a summarization, at least one contributing subnet needs to be in the OSPF LSDB for L3Out 3. Hence, a Transit Routing configuration (“Export Route Control Subnet” scope) for at least one contributing subnet is required in Figure 55. If it is summarizing BD subnets, a correct BD subnet advertisement configuration is required for at least one subnet (see the “ACI BD subnet advertisement” section).

The standalone NX-OS equivalent commands are the following:

router ospf 1

vrf TK:VRF1

summary-address <prefix>/<mask>

OSPF Route Summarization – Inter-Area Routes

When there are multiple OSPF L3Outs on the same border leaf, each L3Out manages a different OSPF area. Hence, Transit Routing between those L3Outs will not use redistribution but will use area-filter instead. In such a scenario, route summarization cannot be performed using the previous option because the previous summarization option is for redistributed Type-5 and Type-7 LSAs. To address this, ACI offers Inter-Area Route Summarization as well.

Note:

If multiple OSPF L3Outs are deployed on different border leaf switches instead of on the same border leaf, then one OSPF L3Out will get the transit routes from another OSPF L3Out via infra MP-BGP. Hence it still uses redistribution and relies on Redistributed Route Summarization instead of Inter-Area Route Summarization. Please see the “L3Out Transit Routing” section for details about how OSPF L3Out implements Transit Routing.


Example of OSPF route-summarization topology (inter-area routes)

The OSPF Inter-Area Route Summarization uses OSPF Route Summarization Policy with an “Inter-Area Enabled” option. With this option, this will be equivalent to “area 0 range 192.168.0.0/16” in standalone NX-OS. Since the area-range command is configured on the source area that has routes to be summarized, this configuration on ACI is also configured on the source L3Out (“L3Out src” in Figure 56) unlike any other L3Out route summarization. Assuming the source L3Out is learning routes to be summarized (192.168.x.0/24 in Figure 56), the summarization takes place without any additional ACI configurations, and the summarized route (192.168.0.0/16 in Figure 56) will be advertised to the outside from the destination L3Out (L3Out dst).

Note:

If there are other OSPF L3Outs on the same border leaf in the same VRF, the summarized route will be advertised from all of them except the source L3Out. This is because the area-filter, which controls transit routes between OSPF L3Outs (the OSPF areas) on the same border leaf, uses the same route map for all OSPF L3Outs on the same leaf in the same VRF. In this shared route map for the area-filter, the summarized prefix (192.168.0.0/16 in Figure 56) is allowed due to the “Export Route Control Subnet” scope in the source L3Out. This means all L3Outs on the same border leaf in the same VRF tries to allow the summarized prefix due to the shared route map. This is also the reason no configuration is required on the destination L3Outs. Please see “Internal route-map for Transit Routing” in the “L3Out Transit Routing” section for details.


Note:

This is a very exceptional usage of the “Export Route Control Subnet” scope because the scope is typically configured on the destination L3Out (“L3Out dst” in this example), from which routes need to be advertised (exported) to the outside. Please see the “L3Out subnet scope options” section or the “L3Out Transit Routing” section for a description of the normal usage of “Export Route Control Subnet”.

For Inter-Area Route Summarization, a cost can be configured for the summarized route. If not specified, the source L3Out with the summarization policy uses the maximum cost from the contributing routes, which is the method defined in RFC 2328 to get the cost of summarized routes.

The standalone NX-OS equivalent commands are the following:

router ospf 1

vrf TK:VRF1

area <source area id> range <prefix>/<mask> {cost <num>}

OSPF default-route advertisement

There are multiple methods to advertise a default route (0.0.0.0/0) from OSPF L3Out to the outside. This subsection covers the basics of Default Route Leak Policy in OSPF, then provides a summary of methods used for each OSPF area type. Please remember that Default Route Leak Policy is not always the answer for advertising a default route in OSPF. For example, it does not do anything for the OSPF Stub area because Default Route Leak Policy is essentially the same as “default-information originate” or “area <ID> nssa default-information-originate.” Thus, an understanding of standard OSPF area behavior is required.


Default Route Leak Policy for OSPF in GUI (APIC Release 3.2)

Default Route Leak Policy was introduced in APIC Release 1.1(1) and can be created under an L3Out in either of the following ways:

● Choose “Create Default Route Leak Policy” from a dropdown menu at the top right in L3Out

● Choose “Create Default Route Leak Policy” from a right-click menu on L3Out itself

Default Route Leak Policy has the following parameters:

● Always This is applicable only for OSPF Regular area. If Always is set to Yes, a default route will be created in OSPF LSDB even if there is no default route in the routing table.

● Criteria Use “Leak Default Route in Addition” when a default route needs to be advertised on top of other routes. Use “Leak Default Route Only” when only a default route should be advertised.

When “Leak Default Route Only” is selected, deny-all is applied to route maps for all redistribution and area-filter in this L3Out to prevent other routes from being advertised.

● Scope Choose “Context” for OSPF Regular area. Choose “Outside” for OSPF NSSA area.

The standalone NX-OS equivalent command for OSPF Regular area is the following:

router ospf 1

vrf TK:VRF1

default-information originate [always]

The standalone NX-OS equivalent command for OSPF NSSA area is the following:

router ospf 1

vrf TK:VRF1

area 0.0.0.1 nssa default-information-originate

Default route in OSPF Stub area


Default-route advertisement in OSPF Stub area

● Default Stub Behavior (the left side in Figure 58) By default, the OSPF ABR will generate a default route, and send it along with other Type-3 LSAs into the Stub area.

● Totally Stub Behavior (the right side in Figure 58) By disabling the “Originate Summary LSA” option under the root of L3Out, the Stub area becomes a totally Stub area. Then a default route is generated, and only the default route will be advertised.

Default route in OSPF NSSA area


Default-route advertisement in OSPF NSSA area

● Totally NSSA Behavior (the top 2 diagrams) By disabling the “Originate Summary LSA” option under the root of L3Out, the NSSA area becomes a totally NSSA area. Then a default route is generated and other Type-3 LSAs will be suppressed. Type-7 LSAs can be advertised on top of the default route when a border leaf is ASBR and ABR at the same time. This Type-7 LSA can also be suppressed by disabling the “Send redistributed LSAs into NSSA area” option.

● Transit Routing (the diagram on the bottom left) This option is to export a default route from another L3Out or static route via the “Export Route Control Subnet” scope. See the “L3Out Transit Routing” section for details.

● Default Route Leak Policy (the two diagrams on the bottom right) This option is to use Default Route Leak Policy in L3Out. See above for the Default Route Leak Policy itself.

Default route in OSPF Regular area


Default-route advertisement in OSPF Regular area

● Transit Routing (the diagram on the left) This option is to export a default route from another L3Out via the “Export Route Control Subnet” scope. See the “L3Out Transit Routing” section for details.

● Default Route Leak Policy (the four diagrams on the right) This option is to use Default Route Leak Policy in L3Out. See above for the Default Route Leak Policy itself.

Note:

When Default Route Leak Policy is configured with Context scope on one L3Out, it will be applied to all L3Outs with OSPF Regular Area in the same VRF on the same border leaf. This is equivalent to “default-information originate” from standard NX-OS where it applies to all regular areas in a VRF.

L3Out EIGRP

Basic configuration example


EIGRP configuration diagram

Figure 61 illustrates an example configuration of EIGRP with AS 10. The key component is the same as a normal router, to ensure that the AS number matches. Although the MTUs are not required to be the same for EIGPR neighborship, the recommendation is to set the same values to ensure that any protocol packets such as the routes exchange after establishing neighborship will not be dropped.

Figure 62 shows an example of an APIC GUI configuration.


EIGRP basic configuration in GUI (APIC Release 3.2)

The following are the three EIGRP-specific components:

● Enable EIGRP: Check to enable EIGRP protocol on the border leaf switches in the L3Out.

● EIGRP AS Number: EIGRP AS Number used to establish a neighbor.

● EIGRP Interface Profile: To enable EIGRP on interfaces in the Logical I/F Profile. A default can be used unless fine tuning is required.

Just as with a normal router, a loopback is not specifically required for EIGRP to work.

Limitations and guidelines

● EIGRP (IPv4) has been supported since APIC Release 1.1(1).

● EIGRP (IPv6) has been supported since APIC Release 1.2(2).

● Only one EIGRP L3Out can be deployed on a border leaf per VRF. This is because one EIGRP L3Out represents one EIGRP AS.

● When multiple external routers are connected to an EIGRP L3Out with the same VLAN, which means in the same L3Out BD, external routers will form neighbors directly to each other. See Figure 20 in the “L3Out bridge domain” subsection in the “L3Out Node and Interface Profiles” section for details.

In such a scenario, external routers will exchange routes directly through the ACI L3Out BD. Hence, a situation similar to Transit Routing with “Export Route Control Subnet” could occur without “Export Route Control Subnet”.

● When advertising a BD subnet or performing Transit Routing, routes are redistributed into the EIGRP topology via a route map that is automatically created on a border leaf. This route map is shared with OSPF L3Outs on the same leaf in the same VRF. It implies that the subnet advertisement configuration on one L3Out may affect other L3Outs. Hence, awareness of this implementation is required when there are other L3Outs on the same leaf in the same VRF. See Figure 93 in the “L3Out Transit Routing” section for details.

Please check the “EIGRP protocol support” section in the Cisco APIC Layer 3 Network Configuration Guide for other limitations.

EIGRP protocol options – interface level


EIGRP Interface Profile and Policy in GUI (APIC Release 3.2)

Interface level EIGRP configuration from the EIGRP Interface Profile is applied to all interfaces in the associated Logical Interface Profile. The EIGRP Interface Policy itself is located under “Tenant > Policies > EIGRP > EIGRP Interface”.

EIGRP Interface Policy

● Control State

◦ BFD This feature was introduced in APIC Release 1.2(2). It is used to enable BFD on the EIGRP Interface. See the “L3Out BFD” section for details.

The standalone NX-OS equivalent command is the following:

interface eth1/1 ip eigrp <instance> bfd

◦ Self Nexthop This option is enabled by default. By default, EIGRP sets its local IP address as the next-hop when advertising routes. By disabling this option, the border leaf does not overwrite the next-hop and keeps the original next-hop IP.

The standalone NX-OS equivalent command is the following:

interface eth1/1

ip next-hop-self eigrp <instance>

◦ Passive This option is to configure the interfaces as an EIGRP passive interface. This option is disabled by default.

The standalone NX-OS equivalent command is the following:

interface eth1/1

ip passive-interface eigrp <instance>

◦ Split Horizon Split Horizon is a feature to prevent a routing loop by not sending EIGRP updates or queries to the interface where it was learned. This option is enabled by default.

The standalone NX-OS equivalent command is the following:

interface eth1/1

ip split-horizon eigrp <instance>

● Hello Interval (sec) / Hold Interval (sec) The Hello Interval is the interval for EIGRP Hello messages to be sent. The default is 5 seconds. The Hold Interval is advertised in Hello messages and indicates to neighbors the length of time that they should consider the sender valid. The default Hold Interval is three times the Hello Interval, which is 15 seconds.

The standalone NX-OS equivalent commands are the following:

interface eth1/1

ip hello-interval eigrp <instance> <sec>

ip hold-interval eigrp <instance> <sec>

● Bandwidth / Delay Configures the bandwidth and delay for the EIGRP metric calculation.

The standalone NX-OS equivalent commands are the following:

interface eth1/1

ip bandwidth eigrp <instance> <bandwidth>

ip delay eigrp <instance> <delay>

EIGRP Authentication


EIGRP key authentication in GUI (APIC Release 3.2)

EIGRP Authentication was introduced in APIC Release 3.2(4) though this enhancement;

CSCvk43721 EIGRP Authentication support for ACI

It is enabled per EIGRP Interface Profile with KeyChain. The supported EIGRP authentication mode is MD5.The EIGRP KeyChain Policy itself is located under “Tenant > Policies > EIGRP > EIGRP KeyChains”.

● Key ID: ID for the key to manage multiple keys in the key chain.

● Name: A name used in an object model for each key. Optional.

● Pre-shared Key: A password that needs to match with its neighbors.

● Start Time: A time when this key becomes active. If empty, it starts immediately.

● End Time: A time when this key expires. If empty, infinite is used.

The standalone NX-OS equivalent commands are the following:

key chain <keychain name>

key <id>

key-string <password>

send-lifetime <start-time> <end-time>


interface eth1/1

ip authentication mode eigrp <instance> md5

ip authentication key-chain eigrp <instance> <keychain name>

EIGRP protocol options – VRF level


EIGRP Address Family Context Policy in GUI (APIC Release 3.2)

EIGRP Address Family Context Policy

This policy is used under VRF but the EIGRP Address Family Context Policy itself is located under “Tenant > Policies > EIGRP > EIGRP Address Family Context”.

The details for each parameter are as follows:

● Active Interval (min) This is the interval the border leaf waits after sending an EIGRP query before declaring stuck in active (SIA) and resetting the neighborship. The default is 3 minutes.

The standalone NX-OS equivalent commands are the following:

router eigrp 10

vrf TK:VRF1

address-family ipv4 unicast

timers active-time <min>

● External Distance / Internal Distance Administrative distance (AD) for external and internal EIGRP routes. The default is 170 for external, 90 for internal.

The standalone NX-OS equivalent command are the following:

router eigrp 10

vrf TK:VRF1

address-family ipv4 unicast

distance <internal> <external>

● Maximum Path Limit The maximum number of ECMPs that EIGRP can install into the routing table. The default is eight paths.

The standalone NX-OS equivalent commands are the following:

router eigrp 10

vrf TK:VRF1

address-family ipv4 unicast

maximum-paths <num>

● Metric Style EIGRP calculates its metric based on bandwidth and delay along with default K values. However, the original implementation value of 32 bits cannot differentiate interfaces faster than 10 Gigabit Ethernet. This original implementation is called the classic or narrow metric. To solve this problem, a value of 64 bits with an improved formula was introduced for EIGRP. This is called the wide metric. The default is the narrow metric.

The standalone NX-OS equivalent commands are the following:

router eigrp 10

vrf TK:VRF1

address-family ipv4 unicast

metric version 64bit

EIGRP route summarization

This feature was introduced in APIC Release 1.2(2). This feature is to advertise only a summarized prefix for BD subnets and/or Transit Routes from ACI OSPF L3Out to the outside.


EIGRP Route Summarization in GUI (APIC Release 3.2)

EIGRP Routing Summarization in ACI is configured by adding a route summarization policy to an L3Out Subnet with the scope “Export Route Control Subnet” because it is to advertise (export) routes from ACI to the outside. Please refer to the “L3Out Transit Routing” section for details about the “Export Route Control Subnet” scope.

By adding a route summarization policy to the L3Out subnet, as shown in Figure 66, the border leaf will try to create a Null-0 entry for the summarized route (192.168.0.0/16 in Figure 66), which will be advertised to its EIGRP peers. This summarized route with a Null-0 next-hop will not be advertised to other leaf switches via infra MP-BGP. Please be aware that, as in a normal EIGRP router, the summarization will not occur if no contributing routes exist in the EIGRP topology table for the user VRF on the border leaf.

EIGRP summarization is implemented per interface. Hence, ACI will deploy the summarization policy on all interfaces in the L3Out.


Example of an EIGRP Route Summarization topology

Figure 67 depicts when L3Out 3 advertises only a summarized subnet (192.168.0.0/16) for transit routes from L3Out 1 and 2 (192.168.1.0/24 and 192.168.2.0/24). To perform a summarization, at least one contributing subnet needs to be in the EIGRP topology table for L3Out 3. Hence, a Transit Routing configuration (with an “Export Route Control Subnet” scope) for at least one contributing subnet is required. If it is summarizing BD subnets, a correct BD subnet advertisement configuration is required for at least one subnet (see the “ACI BD subnet advertisement” section).

The standalone NX-OS equivalent command is the following:

interface eth1/1

ip summary-address eigrp <instance> <prefix>/<mask>


Note:

When OSPF Route Summarization is configured on the same leaf in the same VRF as EIGRP L3Out, the OSPF summarized route will be advertised to EIGRP as well, even without any route summarization in the EIGRP L3Out. This is because of the following:

1. The summarized route with a Null-0 next-hop was already created in the routing table due to OSPF on the same border leaf.

2. The OSPF L3Out creates a route-map entry for the summarized route.

3. This route map is for Transit Routing or an “Export Route Control Subnet” scope; thus, it is shared between OSPF and EIGRP on the same leaf in the same VRF. Please see the Internal route-map for Transit Routing in the “L3Out Transit Routing” section for details.

For these reasons, EIGRP redistributes the OSPF summarized route on the same leaf without EIGRP Route Summarization. The same will happen when EIGRP L3Out has a route-summarization configuration, but OSPF L3Out on the same leaf in the same VRF does not.

EIGRP default route advertisement

There are two methods to advertise a default route (0.0.0.0/0) from EIGRP L3Out to the outside:

1. Transit Routing

2. Default Route Leak Policy

Transit Routing will advertise a default route that is learned from another L3Out or possibly a static route configured on another L3Out. See the “L3Out Transit Routing” section for details on Transit Routing.

Default Route Leak Policy is equivalent to “default-information originate” in standalone NX-OS.


Default Route Leak Policy for EIGRP in GUI (APIC Release 3.2)

Default Route Leak Policy was introduced in APIC Release 1.1(1) and can be created under an L3Out by either of the following ways:

● Choose “Create Default Route Leak Policy” from a dropdown menu at the top right in L3Out

● Choose “Create Default Route Leak Policy” from a right-click menu on L3Out itself

Default Route Leak Policy has the following parameters:

● Always This is equivalent to the “always” option for “default-information originate” in standalone NX-OS. If Yes, a default route is advertised even without a default route in a routing table.

● Criteria Use “Leak Default Route in Addition” when a default route needs to be advertised on top of other routes. Use “Leak Default Route Only” when only a default route should be advertised.

When “Leak Default Route Only” is selected, no redistribution rule is deployed on a border leaf for this EIGRP L3Out.

● Scope Use “Context” for EIGRP.

The standalone NX-OS equivalent commands are the following:

router eigrp 10

vrf TK:VRF1

default-information originate [always]

ACI BD subnet advertisement

This section covers the details of how ACI fabric advertises BD subnets via a routing protocol in L3Out. Please refer to the “Basic components of L3Out” section for a basic understanding and configuration.

There are three methods to accomplish BD subnet advertisement:

1. Via L3Out association to a BD (the method explained in the “Basic components of L3Out” section)

2. Via an “Export Route Control Subnet” scope in a subnet under L3Out EPG

3. Via Route Map/Profile in Export Direction with an explicit prefix-list


BD subnet advertisement methods

Figure 69 illustrates three methods to advertise BD subnet externally, with the pros and cons of each method. What is deployed on a leaf is the same for all three methods; ACI creates an IP prefix-list for a BD subnet (only if this subnet was configured with an “Advertised Externally” scope), and it inserts the prefix list into a route map on a border leaf (the details are explained below, in Figure 70 and Figure 71). Thus, it depends on the user’s preference which method to use. One simple recommendation is not to mix the three methods, because managing the mixed configuration will be very challenging.

The first method, L3Out association to a BD, is the most basic one and has been supported from the first APIC Release 1.0. It gives all controls solely to the BD (a) about which BD subnets are to be advertised (b) to which L3Out. In case the operation teams are separated between the BD and L3Out components, perhaps due to multitenancy, this method can be completed within a single team/component. On the other hand, the L3Out component itself has less visibility on which BD subnet it is advertising because the L3Out association to a BD is configured under the BD.

The second method, using an “Export Route Control Subnet” scope, allows the managing of subnets advertised to the outside via an L3Out configuration regardless of whether the subnets are BD subnets or external routes from another L3Out (Transit Routing). Although this may consolidate configurations, it may be confusing if you want to troubleshoot a configuration, and you need to distinguish BD subnets as opposed to Transit Routing subnets by looking at L3Out subnets. Please note that BD subnets still need to be marked as “Advertised Externally”.

The third method, using Route Map/Profile in Export Direction, provides a configuration methodology like that for a normal router when you configure a route map and an IP prefix-list directly. This option uses Route Profiles and Explicit Prefix Lists (match prefix criteria) instead of L3Out association to a BD or the L3Out Export Route Control Subnet. See the “L3Out Route Profile / Route Map” section for details. The same pros and cons of the second method apply to this one as well. BD subnets still need to be marked as “Advertised Externally”.

Internal route-map for BD subnet advertisement

This section explains the details of what actually happens on a border leaf to advertise a BD subnet.

For the BD subnets to be announced via the L3Out to the outside, APIC must configure the border leaf with three elements (depicted in different colors in Figure 70):

● A redistribution configuration with a route map (which may be empty in the beginning), depicted in green in the picture

● An IP prefix-list in the route map that contains the subnets that should be advertised (depicted in blue in the picture)

● The BD-subnet route pushed by APIC on the border leaf as a result of the contract between the L3Out external EPG and the EPG where the servers are (depicted in gray in the picture)


BD subnet advertisement architecture (OSPF/EIGRP)

Figure 70 explains what happens under the hood for BD subnet advertisement in OSPF and EIGRP. The green components (a route map for the redistribution from direct/static routes and advertisement of routes in the protocol database) are deployed on a border leaf regardless of whether BD subnets are configured to be advertised or not. However, without the BD subnets configured to be advertised (for instance, if there is no L3Out association to a BD), the route map for the direct/static redistribution is empty, and the BD subnet advertisement has yet to be accomplished. The blue component (the IP prefix-list for the BD subnet in the route map) is what is actually added by the BD subnet advertisement configuration, such as the L3Out association to a BD. With this, the BD subnet can be redistributed into a routing protocol and advertised to the outside if the BD subnet is deployed on the border leaf. The BD subnet can be present on the border leaf as a result of a contract between the L3Out External EPG and the EPGs for the BD, or simply because there are local endpoints attached to the BD on the border leaf.


BD subnet advertisement architecture (BGP)

Figure 71 explains what happens under the hood for BD subnet advertisements in BGP. Although most of the programming of the control plane and the data plane of the border leaf is the same as with OSPF/EIGRP, the configuration performed by APIC for BGP in the background needs a slightly different approach due to the use of Infra MP-BGP. All the direct/static routes are redistributed into BGP IPv4 AF first for infra MP-BGP, and then APIC configures which routes to advertise to its BGP peer with an outbound route-map. The blue component is to add IP prefix-list for the BD subnet into the outbound route map (in the case of OSPF/EIGRP, the IP prefix-list was added in the route map for redistribution).

Note:

The name of the route map for direct route redistribution in OSPF/EIGRP is in the form of “exp-ctx-st-<VRF VNID>” and is shared between OSPF and EIGRP on the same border leaf in the same VRF.


Leaf1# show ip ospf vrf TK:VRF1 | grep -A 4 Redist

Redistributing External Routes from

static route-map exp-ctx-st-2916353 <-- “exp-ctx-st-<VRF VNID>”

direct route-map exp-ctx-st-2916353 <-- “exp-ctx-st-<VRF VNID>”

bgp route-map exp-ctx-proto-2916353

eigrp route-map exp-ctx-proto-2916353

The name of the route map for the BGP peer outbound is in the form of “exp-L3Out-<L3Out name>-peer-<VRF VNID>” and is shared with all the BGP peers in the same L3Out.

Leaf1# show bgp ipv4 unicast neighbors vrf TK:VRF1 | egrep 'BGP nei|Outbound'

BGP neighbor is 102.0.0.9, remote AS 65009, ebgp link, Peer index 1

Outbound route-map configured is exp-L3Out-BGP-peer-2916353, handle obtained

<-- “exp-L3Out-<L3Out name>-peer<VRF VNID>”


Note:

There are many components that add an IP prefix-list into a route map on a border leaf. Hence, there is typically more than one IP prefix-list in the route map in ACI fabric.

When there is no IP prefix-list in a route map yet, the route-map name may be specified in each protocol (such as for redistribution in OSPF/EIGRP, for peer outbound in BGP) even though the route map itself may not yet exist.


Note:

A BD subnet is not distributed to other leaf switches via infra MP-BGP even though BGP IPv4/v6 AF has a permit-all redistribution route map. A BD subnet is deployed on leaf switches as a static/direct route pointing to the spine proxy TEP exclusively by APIC based on the user configuration such as deployment of EPGs or a contract on the EPGs for the BD.

L3Out subnet scope options

This section provides an overview of L3Out subnet scope options. The details of each scope will be explained in later sections.

L3Out subnet scope options such as “Export Route Control Subnet” or “External Subnets for the External EPG” are located under “Tenant > Networking > External Routed Networks > L3Out > Networks > L3Out EPG > General tab > Subnet”.


L3Out subnet scope in GUI (APIC Release 3.2)

L3Out subnet scope summary


L3Out subnet scope summary

As Figure 73 shows, the scope options along with the aggregate options are grouped into two different groups. One (the green group in Figure 73) is for options to manipulate the routing table and routing protocol via IP prefix-lists and route maps on a border leaf. Another one (the blue group in Figure 73) is for options related to contracts.

Route Control for Routing Protocol

All three scopes here (Export, Import, and Shared) create an IP prefix-list with the specified subnet on a border leaf. Hence, these scopes will affect only a route with an exact match. If you configure a subnet as 10.0.0.0/8 with these scopes, ACI applies the configuration to 10.0.0.0/8 but not to 10.0.0.0/16. In case the requirement is to match multiple subnets with one configuration entry, you need to use the Aggregate option for each scope. Please note that Aggregate option for Export and Import scopes is supported only for 0.0.0.0/0 subnet.

● Export Route Control Subnet This scope is to advertise (export) a subnet from ACI to the outside via an L3Out. Although this scope is mainly for Transit Routing, it could also be used to advertise a BD subnet, as described in the “ACI BD subnet advertisement” section.

This scope must be configured on an L3Out that is used to advertise the subnet. This scope is not for an L3Out that is learning the subnet. This scope was introduced in APIC Release 1.1(1).

Please refer to the “L3Out Transit Routing” section for details.

● Import Route Control Subnet This scope is about learning (importing) an external subnet from an L3Out. By default, this scope is disabled, hence it is grayed out in Figure 72, and a border leaf learns any routes from a routing protocol. This scope can be enabled if you need to limit external routes learned via OSPF and BGP. This option is not available for EIGRP.

To use this scope, “Import Route Control Enforcement” needs to be enabled first on a given L3Out (please see the following subsection “Route control enforcement” for details). Once “Import Route Control Enforcement” is enabled on an OSPF L3Out, a border leaf uses a table map with an IP prefix-list for the subnet with “Import Route Control Subnet” so that only those subnets can be used in the routing table even though the routes may be in the OSPF LSDB on a border leaf. When this scope is enabled on a BGP L3Out, a border leaf uses an inbound route-map with an IP prefix-list for the subnet with “Import Route Control Subnet” against all BGP peers in the L3Out. Hence, only the configured routes can be learned in the BGP table in the first place.

This scope is to be configured on an L3Out that is learning the subnet. This scope was introduced in APIC Release 1.1(1).

● Shared Route Control Subnet This scope is to leak an external subnet to another VRF. ACI uses MP-BGP and route target to leak an external route from one VRF to another. This scope creates an IP prefix-list with the subnet, which is used as a filter to export/import routes with the route target in MP-BGP.

You should configure this scope on an L3Out that is learning the subnet in the original VRF.

Please refer to the “L3Out shared service (VRF route leaking)” section for details.

Route Control for Routing Protocol (Aggregate)

As mentioned above, Export, Import, and Shared Route Control Subnet are an exact match. In case you want to match multiple subnets with one configuration, you can use the Aggregate option for each Route Control Subnet scope.

● Aggregate Export This option can be used only for 0.0.0.0/0 with “Export Route Control Subnet”. When both “Export Route Control Subnet” and “Aggregate Export” are enabled for 0.0.0.0/0, ACI creates an IP prefix-list with “0.0.0.0/0 le 32”, which matches any subnets. Thus, this option can be used when an L3Out needs to advertise (export) any routes to the outside. This scope was introduced in APIC Release 1.1(1).

When more granular aggregation is required, Route Map/Profile with an explicit IP prefix-list can be used.

● Aggregate Import This option can be used only for 0.0.0.0/0 with “Import Route Control Subnet”. When both “Import Route Control Subnet” and “Aggregate Import” are enabled for 0.0.0.0/0, ACI creates an IP prefix-list with “0.0.0.0/0 le 32”, which matches any subnets. Hence, this option can be used when an L3Out needs to learn (import) any routes from the outside. However, the same goal can be accomplished by just keeping the default L3Out configuration, which has “Import Route Control Enforcement” disabled. This scope was introduced in APIC Release 1.1(1).

When more granular aggregation is required, Route Map/Profile with an explicit IP prefix-list can be used.

● Aggregate Shared Routes This option can be used for any subnets with “Shared Route Control Subnet”. When, for example, both “Shared Route Control Subnet” and “Aggregate Shared Routes” are enabled for 10.0.0.0/8, ACI creates an IP prefix-list with “10.0.0.0/8 le 32”, which matches 10.0.0.0/8, 10.1.0.0/16, and so on.

Note:

There are three options to advertise all prefixes:

● 0.0.0.0/0 in Export Route Control Subnet with Aggregate Export

● 0.0.0.0/0 with aggregation option in a prefix list used by a non-default route profile

● 0.0.0.0/0 with aggregation option in a prefix list used by route profile “default-export”

However, in the case of OSPF or EIGRP, the first two options advertise only routes from dynamic routing protocols. BD subnets, static routes and L3Out interfacesubnets are not advertised.

For the last option - “default-export”, the behavior is different as shown below:

● Prior to ACI 6.0(1) release: “default-export” with a prefix list 0.0.0.0/0 with aggregate advertises all kinds of routes including BD subnets, static routesand L3Out interfaces if there is a L3Out to BD association.

● From ACI 6.0(1) release, “default-export” with a prefix list 0.0.0.0/0 with aggregate advertises all kinds of routes including BD subnets, static routes andL3Out interfaces regardless of a L3Out to BD association.

“default-export” is a predefined Route Profile that takes effect without being applied to L3Out EPGs or L3Out subnets, unlike a normal Route Profile.. See the “L3Out Route Profile / Route Map” section for details.

Subnet classifications for contracts

Two scopes (“External Subnets for the External EPG” and “Shared Security Import”) are used only to apply a contract. No matter what subnets are configured with these two options, it does not affect routing protocol behavior or routing tables.

In ACI, a contract is applied between EPGs. In the L3Out, these two scopes are used to classify a traffic to or from the given L3Out EPG. Internally, an ID called pcTag (a policy-control tag) is used as an identifier for each EPG and L3Out EPG. Please refer to the “L3Out contracts” section for details.

● External Subnets for the External EPG This scope is used to allow packets with the configured subnet from or to the L3Out with a contract.

It classifies a packet into the configured L3Out EPG based on the subnet so that a contract on the L3Out EPG can be applied to the packet. This scope is a longest prefix match instead of an exact match with an IP prefix-list for other scopes related to the routing protocol control. If 10.0.0.0/16 is configured with “External Subnets for the External EPG” in L3Out EPG A, any packet with an IP address in that subnet, such as 10.0.1.1, will be classified into the L3Out EPG A to apply a contract for the L3Out EPG A. This does not mean the “External Subnets for the External EPG” scope installs a route 10.0.0.0/16 in a routing table. It will create a different internal table to map a subnet to an EPG (pcTag) purely for a contract application. It does not have any effects on routing protocol behavior. Hence, if a routing protocol, or static route, does not have the route for the destination, a packet will not be forwarded even if a packet is classified into the correct L3Out EPG with the appropriate contract thanks to the “External Subnets for the External EPG” scope.

This scope is to be configured on an L3Out that is learning the subnet.

Please refer to the “L3Out contracts” section for details.

● Shared Security Import Subnet This scope is used to allow packets with the configured subnet when the packets are going across VRFs with an L3Out. A route in the routing table is leaked to another VRF with “Shared Route Control Subnet”, as mentioned above. However, another VRF has yet to know which EPG the leaked route should belong to. The “Shared Security Import Subnet” scope informs another VRF of the L3Out EPG that the leaked route belongs to. Thus, this scope can be used only when the “External Subnets for the External EPG” scope is also used; otherwise, the original VRF doesn’t know which L3Out EPG the subnet belongs to either. The APIC GUI blocks the configuration if “Shared Security Import Subnet” is configured without “External Subnets for the External EPG”. This scope is also a longest prefix match.

Please refer to the “L3Out shared service (VRF route leaking)” section for details.

Route Control Enforcement

The Route Control Enforcement option was introduced in APIC Release 1.1(1). This option is located under Tenant > Networking > External Routed Networks > L3Out. Although there are technically two options (Import and Export), Export is always enabled and cannot be disabled. Hence Route Control Enforcement can be considered Import Route Control Enforcement, which is disabled by default. This option must be enabled to use the “Import Route Control Subnet” scope for the L3Out subnet. This option is supported only for OSPF and BGP.


Route Control Enforcement in GUI (APIC Release 3.2)

When the route control import option is not selected (that is, not enabled), the L3Out learns any external routes via routing protocols and those will be installed in a routing table.

When the route control import is enabled for an OSPF L3Out, OSPF still learns any external routes; those will be in OSPF LSDB on a border leaf. However, those routes are not installed in a routing table unless the route subnet is configured with the “Import Route Control Subnet” scope. This is implemented by using the table-map feature from NX-OS OSPF. The subnet with the “Import Route Control Subnet” scope is used in a route map with an IP prefix-list for the table map to allow the subnet to be installed on a routing table.

When the route control import is enabled for a BGP L3Out, BGP stops learning any external routes by using an inbound route map for all BGP peers in the same L3Out. The subnet with the “Import Route Control Subnet” scope is used in the route map with an IP prefix-list to allow the subnet to be learned via BGP.

Note:

When there are multiple L3Outs with OSPF on the same border leaf in the same VRF, the Route Control Enforcement on both L3Outs need to match. Otherwise, a fault F0467 is raised. This is because of the following: One border leaf has only one OSPF process, and each OSPF L3Out on the same border leaf in the same VRF represents different OSPF areas in the same process. However, the table map is applied to the OSPF process level instead of per area. Hence, having OSPF areas (L3Outs) with different Route Control Enforcement configurations on the same border leaf in the same VRF creates a conflict. This also implies the “Import Route Control Subnet” scope configuration on OSPF L3Out A would be applied to the OSPF L3Out B as well if both L3Outs are deployed on the same border leaf in the same VRF.

The name of the route map for the OSPF table map is in the form of “exp-ctx-<VRF VNID>-deny-external-tag”.

L3Out contracts

The high-level overview of L3Out contracts is covered in step 5 of the “Basic components of L3Out” section. This section goes over the details of the L3Out contract architecture.

A contract in ACI is used to allow traffic between one EPG and another. A packet is typically classified into its appropriate EPG based on the incoming VLAN and incoming interface (there are some exceptions, such as IP-based EPGs for microsegmentation, but that is not covered in this document). In hardware, each EPG is represented by a number called a pcTag (a policy-control tag), and a contract is applied between those numbers.

Example: “EPGA à EPG B” means “pcTag 49150 à pcTag 49151”

The pcTag for each EPG can be checked at “Tenant > Application Profiles > Application EPGs > EPG > Policy tab > General tab > pcTag (sclass)”.

The EPG to classify traffic that enters or leaves the ACI fabric via an L3Out is typically called L3Out EPG or External EPG, located under “Tenant > Networking > External Routed Networks > L3OU > Networks > L3Out EPG”. In the APIC GUI, it is displayed as “External Network Instance Profile” (Figure 75).The L3Out does not rely on the VLAN and interface to classify a packet into the correct L3Out EPG. The classification is based on a source prefix/subnet instead. Hence, the L3Out EPG is occasionally referred as a prefix-based EPG. After a packet is classified into an L3Out EPG and assigned a pcTag based on a subnet, a contract is applied based on the source and destination EPG (pcTag) combination, just as in a normal EPG.


L3Out EPG and pcTag in GUI (APIC Release 3.2)

Note:

A pcTag is unique within a VRF by default. Hence there can be an overlap of pcTags across VRFs, which is not a problem. However, this becomes a problem when doing VRF route leaking (in other words, shared service). In such situations, ACI uses a pcTag called a global pcTag. Note that ACI generates and assigns an appropriate type of pcTag to EPGs without explicit user configurations. A pcTag range lower than 0x4000 (16384) is called global pcTag; it is unique across all VRFs. Please refer to the “L3Out shared service (VRF route leaking)” section for details.

L3Out EPG (prefix-based EPG)

This section covers details of how a packet is classified into an L3Out EPG, based on subnets.


Prefix-based classification for L3Out EPGs

An L3Out is a component to connect to external routing devices instead of an end host. This implies it has many subnets behind it and requires granular subnet classification for contract policies. The L3Out EPG allows you to classify the external traffic based on prefixes. Figure 76 depicts how you configure L3Out EPGs to match different subnets.


External Subnets for the External EPG

The traffic classification of each L3Out EPG is configured by the “External Subnets for the External EPG” scope on an L3Out subnet under an L3Out EPG (Figure 77)

External Subnets for the External EPG and prefix – pcTag mapping


L3Out prefix - pcTag mapping

When an L3Out subnet is configured with “External Subnets for the External EPG”, ACI internally creates a table that maps the prefixes and pcTags of the L3Outs. This mapping table is a longest prefix match (LPM) table; a separate mapping table is organized for each VRF. Hence, a subnet with an “External Subnets for the External EPG” scope must be unique across all L3Out EPGs in any L3Outs within the same VRF. If, in Figure 78, the L3Out EPG C had 10.0.0.0/8 with an “External Subnets for the External EPG” scope just like L3Out EPG A, ACI would not be able to tell if 10.0.0.0/8 should be mapped to pcTag 49150 (L3Out EPG A) or to pcTag 49152 (L3Out EPG C). On the other hand, in Figure 78, 10.10.0.0/16 can be configured in L3Out EPG B, since, from an LPM perspective, it is not the same entry as 10.0.0.0/8. A packet with IP 10.10.0.1 will be classified into L3Out EPG B instead of L3Out EPG A.

The following command can be used to check the prefix-pcTag mapping on each leaf switch.

Leaf1# vsh_lc -c ‘show system internal aclqos prefix’ | egrep ‘Vrf|10.0.0.0’

Vrf-Vni VRF-Id Table-Id Addr Class Shared Remote Complete

2097152 8 0x8 10.0.0.0/8 49200 0 1 No


=== use this command from APIC release 3.2(1) ===

Leaf1# vsh –c ‘show system internal policy-mgr prefix’

● Vrf-Vni: VRF VNID

● Addr: L3Out subnet/prefix with an “External Subnets for the External EPG” scope

● Class: pcTag for L3Out EPG

Caution:

When an endpoint is learned in the ACI fabric, the pcTag of the EPG for the endpoint is also stored in the endpoint table. This means the endpoint IP to the pcTag mapping is also checked when ACI checks prefix to pcTag mapping for L3Outs. This mapping is also based on using a longest prefix match. This implies, with the example of Figure 78, where L3Out EPG A has 10.0.0.0/8 with an “External Subnets for the External EPG” scope, if an external IP such as 10.1.1.1 is learned as a normal endpoint due to a traffic path design mistake or IP spoofing, etc., the packet with IP 10.1.1.1 from or to L3Out 1 will be using a pcTag for the normal EPG of the endpoint 10.1.1.1 instead of L3Out EPG A, because an endpoint is a /32 entry which is preferred to /8 entry in LPM.

To prevent such undesired endpoint learning behavior, please refer to the ACI Fabric Endpoint Learning white paper.

An exception for 0.0.0.0/0 with External Subnets for the External EPG

Users need to be careful regarding the unique behavior that results from using 0.0.0.0/0 as the subnet with an “External Subnets for the External EPG” scope. Although it is not recommended, you can configure 0.0.0.0/0 with “External Subnets for the External EPG” in multiple L3Out EPGs in the same VRF. But you cannot do the same with non-0.0.0.0/0 subnets; for example, multiple L3Out EPGs cannot be configured with the same non-0.0.0.0/0 with “External Subnets for the External EPG” in the same VRF. The reason 0.0.0.0/0 is an exception is because 0.0.0.0/0 with “External Subnets for the External EPG” does not use a pcTag for each L3Out EPG; instead, it always uses the reserved pcTag. While this configuration is allowed, an unintended contract deployment may occur by configuring 0.0.0.0/0 with “External Subnets for the External EPG” in multiple L3Out EPGs within the same VRF. Figure 79 shows this scenario.


A caveat in 0.0.0.0/0 with "External Subnets for the External EPG"

In Figure 79, 0.0.0.0/0 with an “External Subnets for the External EPG” scope is configured on both L3Out 1 and L3Out 2, where a contract to a normal EPG is only configured with L3Out EGP 2. Hence, you may expect a packet between the normal EPG and the L3Out 1 router should be dropped due to a missing contract. However, any packets between the normal EPG and the L3Out 1 router are allowed, just like packets from the L3Out 2 router. This is because the pcTag classification is not per L3Out, but it relies exclusively on the prefix-pcTag mapping table which is per VRF.

The prefix-pcTag mapping table on the border leaf switches with this configuration will have only one entry with the reserved pcTag for 0.0.0.0/0 instead of 49151 (L3Out EPG1) or 49152 (L3Out EPG2). Then, the contract between the normal EPG and L3Out EPG 2 is deployed with the reserved pcTag for 0.0.0.0/0 and the pcTag for the normal EPG. When a packet is sent out to 10.1.1.1 behind L3Out 1 from the normal EPG, the destination pcTag will be the reserved one (“XXXXX” in Figure 79) based on the prefix-pcTag mapping. Thus, a packet to L3Out EPG 1 is allowed due to a contract rule between L3Out EPG 2 and the normal EPG. If L3Out EPG 1 has 10.0.0.0/8 with an “External Subnets for the External EPG” scope on top of this, the prefix-pcTag mapping table has two entries, one for 0.0.0.0/0 with the reserved pcTag and one for 10.0.0.0/8 with 49151 (L3Out EPG1 pcTag), and the packet is classified into pcTag 49151 due to the LPM rule. The packet is then dropped due to a missing contract.

Therefore, the recommendation is to use 0.0.0.0/0 with an “External Subnets for the External EPG” scope in only one L3Out EPG per VRF, and to use more-specific subnets for other L3Out EPGs.

Note:

This behavior may depend on the Policy Control Enforcement Direction option. If nondefault “Egress” is used, contract may be applied on each border leaf. If the two L3Outs are deployed on different border leaf switches, each border leaf switch has only its own contract rules. This means an inappropriate contract is not applied, at least on each border leaf. However, there are many scenarios where contracts are applied on non–border leaf switches that may have contract rules for both L3Outs. Hence, the recommendations on the usage of 0.0.0.0/0 as the subnet with an “External Subnets for the External EPG” scope remain the same.


Note:

The reserved pcTag for 0.0.0.0/0 with an “External Subnets for the External EPG” scope depends on the direction of the packets. For the destination pcTag classification (that is, a packet from the ACI fabric to L3Out), ACI always uses pcTag 15 as a fixed value for 0.0.0.0/0. For the source pcTag classification (that is, a packet from L3Out to an endpoint in the ACI fabric), ACI uses a pcTag of the VRF or L3Out BD (an L3Out BD has the same pcTag as its VRF) for 0.0.0.0/0. The reason pcTag 15 is not used for both the source and the destination is to avoid allowing traffic where both the source and destination fall under 0.0.0.0/0 pcTag-prefix mapping. If both source and destination have the same value, the traffic would be assumed as a communication within the same EPG, and the communication would be allowed.

An exception for a directly connected subnet with 0.0.0.0/0

The previous section described how ACI classifies traffic entering the fabric from the outside via L3Out. The assumption was that this traffic originates several hops away from the fabric and is routed from the outside to a border leaf. When the devices originating the traffic are instead directly attached to the border leaf on the L3Out interface subnet (a directly connected subnet), the way traffic is classified is slightly different from what was described in the previous section; it varies depending on the leaf hardware.

Prior to covering the details, a general recommendation is to always configure the L3Out interface subnet with the “External Subnets for the External EPG” scope just in case.


L3Out contract and directly connected subnet (unexpected allow)

As shown in Figure 80, traffic between an IP in an L3Out directly connected subnet (10.0.0.11) and other IPs (for example, 192.168.1.1) may be allowed even without a contract. The other IPs could be normal endpoints (EPs) such as in this example, or IPs in other L3Outs. This is because, by default, directly connected subnets are assigned pcTag 1, which is a special pcTag to bypass a contract. This is to implicitly allow routing protocol communications in a corner case scenario. However, as shown in Figure 80, this may cause a security concern instead. Hence, this behavior is explained in detail via defect ID CSCuz12913, which also introduces a workaround configuration:

CSCuz12913 ACI: a contract is not applied to directly connected subnets on L3OUT

Using the enhancement from CSCuz12913, the workaround “configuring a non-0.0.0.0/0 subnet that covers a directly connected subnet with an ‘External Subnets for the External EPG’ scope” will force even a directly connected subnet to use the pcTag of the L3Out EPG instead of pcTag 1. To ensure that no unintended traffic passes through the ACI fabric, it is highly recommended that you explicitly configure a directly connected subnet with an “External Subnets for the External EPG” scope and utilize the enhancement from CSCuz12913. This enhancement is available only on second-generation leaf switches or later.


L3Out contract and directly connected subnet (unexpected deny)

Also, this pcTag 1 for directly connected subnets may cause unexpected drops (instead of allows) even with a contract in a corner-case situation (Figure 81). In this corner case, an L3Out BD is spanned across two border leaf switches. Traffic is forwarded from one border leaf (leaf 102) to a directly connected IP 10.0.0.11 on another leaf (leaf 101). The following is a step-by-step explanation on what happens.

1. Leaf 102 has the ARP entry for 10.0.0.11 because it is in the same L2 domain (L3Out BD).

2. Leaf 102 looks up the destination 10.0.0.11. It uses the ARP entry and resolves the next-hop MAC address prior to sending it to leaf 101.

3. At this time, the contract is bypassed on leaf 102 due to pcTag 1 for directly connected subnets. Even though 10.0.0.11 is technically not directly connected to leaf 102, it is considered connected directly since it has the ARP entry through L3Out BD.

4. The packet reaches leaf 101. The destination MAC is already of the external router (10.0.0.11) instead of the ACI MAC address. A contract has yet to be applied since it was bypassed on a previous node.

5. The lookup is based on the destination MAC address that leaf 101 knows. However, there is no contract for the MAC address in L3Out BD because the L3Out contract is based on a subnet. Hence, it is dropped on leaf 101.

To avoid this issue, the same solution from CSCuz12913 can be applied. When a non-0.0.0.0/0 subnet that covers the direct subnet is configured with an “External Subnet of the External EPG” scope, both leaf 101 and 102 will see the pcTag of the L3Out EPG instead of pcTag 1; therefore, leaf 102 will apply the contract before bridging the traffic on the external BD. The contract is properly applied based on a subnet instead of being bypassed, and the traffic will be forwarded to leaf 101 with the “policy-applied” bit set in its VXLAN header. Leaf 101 will then forward the traffic. Please note that this is only for a directly connected subnet. If the traffic is destined to another subnet behind the external router (10.0.0.11), there will be no such issue. Figure 82 shows one of the typical topology examples for this issue with Multi-Pod.


L3Out contract and directly connected subnet (unexpected deny, part 2)

Policy Control Enforcement Direction

The Policy Control Enforcement Direction option was introduced in APIC Release 1.2(1). It is located under “Tenant > Networking > VRFs > VRF”. It can be set as either “Ingress” or “Egress”. The “Egress” option is equivalent to the behavior prior to Release 1.2(1). Hence, to keep behavior consistent across upgrades, if the VRF was created prior to Release 1.2(1) and the ACI fabric is upgraded to 1.2(1) or later, the option is set to “Egress”. From the APIC Release 1.2(1) release the default configuration is “Ingress”.


Policy Control Enforcement Direction in GUI (APIC Release 3.2)

Policy Control Enforcement Direction is a feature to save TCAM resources for contracts on border leaf switches. Thus, it affects only traffic to or from the L3Out. There are no behavior changes in EPG-to-EPG traffic based on this option. Typically, users do not need to change the mode of this configuration since the default “Ingress” is the mode for saving TCAM resources.


Policy Control Enforcement Direction and contracts

When Policy Control Enforcement Direction is set to “Egress” (see the left bottom in Figure 84), the contract rules for an L3Out are deployed on both border-leaf and non–border-leaf switches. In this situation, when there are many EPGs that need to talk to the L3Out, the TCAM resources for contracts on border leaf switches could be a bottle neck because a border leaf deploys all contracts while contracts on non–border leaf switches are typically distributed to multiple leaf switches. However, when set to “Ingress”, the contract rules are deployed only on non-border leaf switches; hence, this resolves the concern about TCAM resources for contracts on border leaf switches.

In summary, when this feature is set to “Ingress”, a contract for packets from or to L3Out is always applied on a non–border leaf. Figure 85 shows the details on how and where a contract is applied in each scenario.


Policy Control Enforcement Direction and packet flow

First, let’s focus on packets from a normal EPG to an L3Out (see the top two diagrams in Figure 85). For this flow, a non–border leaf is an ingress leaf and a border leaf is an egress leaf. With the “Egress” option in this feature, a contract for this flow is always applied on a border leaf (egress leaf). With the “Ingress” option, a contract for the same flow is always applied on a non–border leaf (ingress leaf).

For the traffic flow in the opposite direction (that is, from the L3Out to a regular EPG), since the destination is a normal endpoint, whether a contract is applied on the ingress leaf (in this case, a border leaf) or the egress leaf (here a non–border leaf) depends on the remote endpoint’s learning on the ingress (border) leaf when this feature is set to “Egress”. If the remote endpoint is learned on the ingress leaf, the ingress leaf knows both the source (L3Out) and the destination (EPG) pcTag to apply a contract. Therefore, the ingress leaf can apply the contract; otherwise, the contract is applied on the egress (non–border) leaf. However, with “Ingress” mode, the ingress leaf does not have a contract rule to apply; thus, it is always applied on the egress leaf.

L3Out Transit Routing

Transit Routing was introduced in APIC Release 1.1(1). This is a feature to allow the ACI fabric to be a transit network by advertising external routes that were learned from one external routing domain to another. Prior to this feature, the ACI fabric was meant to be a pure Stub network. The “Export Route Control Subnet” scope under the L3Out EPG subnet was introduced for this feature. It is located under “Tenant > Networking > External Routed Networks > L3Out > Networks > L3Out EPG > Subnets”.


Transit Routing in GUI (APIC Release 3.2)

Note:

The “Import Route Control Subnet” scope under L3Out EPG subnet was also introduced as part of Transit Routing to provide controls not only for external routes that ACI advertises out, but also for the external routes that ACI may learn. However, the “Import Route Control Subnet” scope is not used as often because the default import behavior where ACI learns all external routes suffices in most situations.


Note:

For supported routing protocol combinations in Transit Routing, please refer to the Supported Transit Combination Matrix in the “Transit Routing” section of the Cisco APIC Layer 3 Configuration Guide.


A simple example of a Transit Routing diagram

Figure 87 is a simple example for Transit Routing without any contracts. The purpose of Figure 87 is to illustrate the fact that the “Export Route Control Subnet” scope is used to advertise routes from one L3Out to another. Please keep in mind that the “Export Route Control Subnet” scope is used only on an L3Out that needs to advertise routes and not on an L3Out that is learning routes.


An example of Transit Routing

Figure 88 is the same Transit Routing example as shown in Figure 87, with the inclusion of the contract portion for traffic filtering. L3Out 1 and 2 are in the same VRF on two different border-leaf switches. ACI fabric is learning a route 10.0.0.0/8 from L3Out 1 and 20.0.0.0/8 from L3Out 2. The requirement here is that devices in 10.0.0.0/8 and 20.0.0.0/8 can communicate with each other through ACI L3Out 1 and 2. There are two configurations to accomplish this: routing and contract, the same as in any other L3Out scenario. To complete the routing part for Transit Routing, ACI uses the “Export Route Control Subnet” scope. Here, L3Out 1 needs to advertise 20.0.0.0/8 to its peer. Hence, 20.0.0.0/8 with an “Export Route Control Subnet” scope is configured under L3Out 1. If 20.0.0.0/8 is already available on border leaf 1 because of infra MP-BGP, it is redistributed into the L3Out 1 routing protocol and advertised via L3Out 1. A common mistake is to configure 20.0.0.0/8 with an “Export Route Control Subnet” scope on L3Out 2, which is learning 20.0.0.0/8, instead of on L3Out 1, where you want the route to be advertised from. The same applies to 10.0.0.0/8 with an “Export Route Control Subnet” scope on L3Out 2. It is configured on L3Out 2, and L3Out 2 starts advertising 10.0.0.0/8 to the outside if the route is available on border leaf 2. The contract configuration is not specific to Transit Routing.

The same principle as in EPG-to-L3Out communication applies here. Users configure L3Out 1 and 2 with the “External Subnets for the External EPG” scope for the respective subnets to map them into each L3Out EPG (pcTag). A contract is applied between those two L3Out EPGs (pcTags).

Caution:

As mentioned in step 4 in the “Basic components of L3Out” section, configuring an “Export Route Control Subnet” scope and an “External Subnets for the External EPG” scope on the same subnet in the same L3Out is a mis-configuration except for OSPF Inter-Area route summarization, and can potentially create a routing loop. An “External Subnets for the External EPG” scope implies that the subnet belongs to a routing domain behind this L3Out. Yet ACI is trying to advertise (export) the subnet back to the L3Out. Even though these advertisements are normally blocked implicitly by the routing-loop-prevention mechanism in each routing protocol, there can be a situation where the advertisement may occur. Hence, this configuration should be avoided.

When exporting all subnets using an “Export Route Control Subnet” scope and an “Aggregate Export” option, the exporting subnet will, unavoidably, include its own subnet. Therefore, please perform such configurations in general with decent care, as you would for any other routing devices with redistribution.

Transit Routing topology


Types of Transit Routing

Figure 89 illustrates four main topologies of Transit Routing. The top two with multiple L3Outs are the original Transit Routing topologies. Transit Routing within a single L3Out between multiple routing devices (the bottom two) were introduced in APIC Release 2.3(1) with a limitation in contracts for 0.0.0.0/0 with an “External Subnets for the External EPG” scope. Please refer to Guidelines for Transit Routing in the “Transit Routing” section of the Cisco APIC Layer 3 Network Configuration guide or CSCuy16355 for the limitation with 0.0.0.0/0.

Note:

When Transit Routing is performed on the same border leaf across two OSPF L3Outs, one of them needs to be OSPF Area 0 because there will be a route exchange between the OSPF areas without going through infra MP-BGP.


VRF tag and Transit Routing

VRF tag was introduced as a mechanism to avoid a potential routing loop in conjunction with Transit Routing. This feature utilizes OSPF or EIGRP route tagging. Hence, this is mainly for those two protocols instead of BGP.

In ACI, every VRF has its own VRF tag, which by default is the same for all VRFs; it is 4294967295. ACI sets the VRF tag in the subnets that ACI advertises out via L3Outs. This includes both BD subnets and Transit Routes. When performing Transit Routing, OSPF or EIGRP on a border leaf use redistribution (or the OSPF area filter) to get external routes from other L3Outs, and while doing so, ACI sets the VRF tag as a route tag to the redistributed routes. When ACI sees an external route with its own VRF tag, it will not use the route in its routing table, to prevent a potential loop.

Note:

This behavior (not to use such routes) is implemented with the table-map feature from NX-OS, which is also used for the “Import Route Control Subnet” scope for OSPF and EIGRP. Hence, the VRF tag and “Import Route Control Subnet” scope share the same route map for the table map.


VRF tag in GUI (APIC Release 3.2)

Figure 90 shows how to change the VRF tag for VRF 1. The Route Tag policy is located under “Tenant > Policies > Protocol > Route Tag”. This policy is configured on each VRF that needs to change the VRF tag from a default value. In this example, the VRF tag of VRF1 was changed to 100. If the transit routes are expected to be learned in ACI again from another VRF, then the VRF tag needs to be changed from the default value. Otherwise, the route will not show up in the routing table because all VRFs use the same default VRF tag by default. The example in Figure 91 shows this issue.


Using VRF tags in Transit Routing to prevent loops

Figure 91 is an example to show how ACI uses VRF tags to prevent loops. In this example, Transit Routing is configured on L3Out 2 for the route 10.0.0.0/8 that is learned from L3Out 1. ACI sets a VRF tag to 10.0.0.0/8 when L3Out 2 gets the route from L3Out 1. This tag is carried through external routers because it is a standard route tag. When the route is advertised back to ACI for some reason, ACI knows this could be a potential loop due to the route tag. Thanks to the table map on OSPF or EIGRP, this potential loop route does not show up in the routing table. Please see the next subsection, “Internal route-map for Transit Routing,” for details on how Transit Routing is implemented (L3Out 1 to L3Out 2 in Figure 91).

Note:

There are other route tags, on top of 4294967295, that ACI uses internally for various reasons. The following ranges show such route tags. It is recommended to avoid using these manually in your network to prevent conflicts of usage.

4294966001 - 4294966512

4294967287 - 4294967295


Internal route-map for Transit Routing

This subsection goes over the details of how a border leaf implements the Transit Routing capability when an “Export Route Control Subnet” scope is configured. ACI utilizes standard routing protocol mechanisms, such as redistribution with route maps. Although users typically don’t need this level of understanding to operate an ACI fabric, it helps to understand the limitations of ACI L3Out and Transit Routing instead of having to memorize them as a list of limitations.


Route-map implementation for Transit Routing with BGP

Figure 92 illustrates all the combinations of L3Outs related to Transit Routing. The focus is advertising routes via BGP L3Out A. It does not mean that all of the other L3Outs have to be deployed at the same time.

The implementation of BGP L3Outs is slightly different compared with that of other L3Outs, such as OSPF and EIGRP, because BGP in the user VRFs is utilized for infra MP-BGP as well as for user L3Outs. Due to the infra MP-BGP implementation, the BGP IPv4/IPv6 database in a user VRF has all of the external routes from other L3Outs on the same border leaf and other border leaf switches without any Transit Routing configurations. Hence, Transit Routing control with the BGP L3Out is performed via an outbound route-map applied to BGP peer neighbor sessions on the border leaf. By default, ACI creates one outbound route-map per BGP L3Out, which is applied to all BGP peers on the same L3Out. This can be changed with the new feature to configure a BGP route-map per peer, which was introduced in APIC Release 4.2(1).

Note:

The outbound route-map for BGP is shared with BD subnet advertisement. It is important to ensure that both Transit Routing for BGP and BD subnet advertisement are configured with the correct subnets to prevent unintended advertisement. Please refer to the “BD subnet advertisement” section for details.


Route-map implementation for Transit Routing with OSPF

Figure 93 illustrates all the combinations of L3Outs related to the Transit Routing. The focus is advertising routes via OSPF L3Out A. It does not mean that all of the other L3Outs have to be deployed at the same time.

The OSPF L3Out implementation for Transit Routing is mainly relying on redistribution. In Figure 93, the external routes from another border leaf should be available on the BGP IPv4/IPv6 table because of infra MP-BGP. The same BGP table is used by the BGP L3Out B as well. Those routes in the BGP table can be used in the routing table but not yet in the OSPF database (LSDB: Link State Data Base). ACI redistributes those routes from BGP to OSPF with an “Export Route Control Subnet” scope so that OSPF can advertise them to the outside for Transit Routing. For other L3Outs such as EIGRP or static route on the same border leaf, ACI also uses redistribution. However, if you had another OSPF L3Out on the same border leaf, the redistribution would not be an option, because it is the same routing protocol. In this case, ACI uses an area-filter with an “in” direction, because each OSPF L3Out on the same border leaf must belong to a different OSPF area.

Note:

In OSPF and EIGRP, ACI internally creates two route maps: one is for routing protocols, the other is for static or direct routes. An “Export Route Control Subnet” scope deploys the same IP prefix-list entry on both route maps.

The route map for a static or direct route is shared with BD subnet advertisement because BD subnets are also static and direct routes. It is important to check that both Transit Routing and BD subnet advertisement are configured with the correct subnets, to ensure that only the intended subnets are advertised. Please refer to the “BD subnet advertisement” section for details.

Also, both route maps are shared with all OSPF and EIGRP L3Outs in the same VRF on the same border leaf.

There is an enhancement request filed to address this shared route-map between OSPF and EIGRP.

CSCuy63998 ACI: Export Subnet under EIGRP is applied to OSPF on the same node


Route-map implementation for Transit Routing with EIGRP

Figure 94 illustrates all of the combinations of L3Outs related to Transit Routing. The focus is advertising routes via EIGRP L3Out A. It does not mean that all of the other L3Outs have to be deployed at the same time.

The EIGRP L3Out implementation for Transit Routing is almost the same as for OSPF. The only difference is that Transit Routing between the same L3Outs using the same protocol on the same border leaf is not supported for EIGRP. This is because the second EIGRP L3Out cannot be configured on the same border leaf, because the EIGRP AS number is per L3Out and there cannot be two EIGRP ASs on the same switch node. Other details, such as route maps that are internally created, are the same as for the OSPF L3Out implementation.

L3Out Route Profile / Route Map

Route Profile / Route Map basics

ACI uses route maps internally for various purposes, such as infra MP-BGP, BD subnet advertisement to the outside, and Transit Routing, as mentioned in the previous sections. ACI provides users with the ability to add user-defined matching or set rules for those internal route maps. This is performed by using Route Profiles, which is sometimes referred to as Route Control Profiles or Route Maps. The following are examples of use cases for this feature:

● A route map to redistribute (export) BD subnets to the outside via L3Outs

● A route map to redistribute (export) external routes from one L3Out to another (Transit Routing)

● A route map to limit learning (importing) external routes from the outside via L3Outs

● A route map to redistribute (interleak) external routes from L3Outs to BGP for infra MP-BGP, etc.


Route Profile Structure in GUI (APIC Release 3.2)

As Figure 95 shows, there are two places to configure Route Profiles. One is at the tenant level, located under “Tenant > External Routed Networks > Route Maps/Profiles”, which was introduced in APIC Release 1.2(2). Another is at the L3Out level, located under “Tenant > External Routed Networks > {L3Out} > Route Maps/Profiles”. Both use Match and/or Set Rules from the tenant level, which is located under “Tenant > External Routed Networks > Set Rules (or Match Rules) for Route Maps”. The difference is the following:

● Tenant-level Route Profile: for interleak (redistribution from OSPF or EIGRP to the infra MP-BGP) and BGP Route Dampening

● L3Out-level Route Profile: for anything else

The components in each Route Profile represent the components in a route map on a normal router, as Figure 96 illustrates. These components are merged into route maps that are used to implement other APIC policies, such as Transit Routing.


Route Profile components

The following explains the two options of a context policy:

● Order: Decides the order of the context policies to be applied. It is equivalent to a sequence number in a normal route map. But, since this is merged into implicit route maps, the actual sequence number will not be the same as this order number.

● Action: Permit or deny action was introduced in APIC Release 2.3(1). In some earlier releases prior to this option, Set Rules was labeled as Action. This is equivalent to permit or deny in a normal route map.

Route Profile Type


Route Profile Type in GUI (APIC Release 3.2)

Route Profile Type was introduced in APIC Release 1.2(2). Prior to 1.2(2), all Route Profiles (route maps) behave in the same way as “Match Prefix AND Routing Policy”.

When a Route Profile is associated to a component such as an L3Out EPG or an L3Out subnet, the Match Rules from the Route Profiles are merged into the internal route map for the component. See other sections (such as the “L3Out Transit Routing” section) for detailed examples of route maps internally created by other components. The Route Profile Type option defines how APIC merges the configured Route Profile rules into a route map deployed from other APIC policies such as an “Export Route Control Subnet” scope under the L3Out subnet.

● Match Prefix AND Routing Policy This type will combine prefixes from the component that the Route Profile is associated to AND the match criteria configured in the Route Profile. For example, if the Route Profile is associated in the export direction to an L3Out EPG that has L3Out subnets 10.0.0.0/8 and 20.0.0.0/8 with an “Export Route Control Subnet” scope, the match clause of each internal route-map sequence on a leaf will have the match criteria of the prefixes from the associated component (10.0.0.0/8 and 20.0.0.0/8) AND from each Context Policy in the Route Profile.

● Match Routing Policy Only This type will use only the match criteria configured in the Route Profile and ignore prefixes from the component to which the Route Profile is associated. For example, if the Route Profile is associated in the export direction to an L3Out EPG that has L3Out subnets 10.0.0.0/8 and 20.0.0.0/8 with an “Export Route Control Subnet” scope, APIC will ignore 10.0.0.0/8 and 20.0.0.0/8 and overwrite the internal route-map sequences for them with a new route map using the match criteria from the context policy in the Route Profile only. Some components that do not have a subnet configuration, such as BGP Route Dampening Policy, need to be configured with this type.

Figure 98 illustrates the differences of each type in an L3Out EPG.


Comparison of Route Profile Types

As Figure 98 shows, “Type Match Routing Policy Only” (scenario 3) completely ignores the L3Out subnets with an “Export Route Control Subnet” scope. Hence, in this particular case, there is no point in configuring L3Out subnets with an “Export Route Control Subnet” scope. More guidance on which type should be used will be provided in the following sections for each scenario in associating Route Profiles.


Explicit Prefix List and Match Prefix AND Routing Policy

Figure 99 illustrates the behavior of Type “Match Prefix AND Routing Policy” in conjunction with Explicit Prefix List (that is, match prefix criteria). As shown, this does not overwrite or take the “logical AND” between L3Out Subnets and prefixes in the Explicit Prefix List. The route map just merges prefixes from both objects.

Route Profile Match and Set Rules

Route Profile Match Rule options


Route Profile Match Options in GUI (APIC Release 3.2)

Match Rules have been added to the Route Profile since the APIC Release 1.2(2). Prior to that release, only Set Rules were supported.

● Match Regex Community This is to create match criteria using the regular expression (regex) community. If the same type of community (Regular or Extended) is configured as a non-regex community match in the same Match Rule Policy, it cannot be configured as a regex community match.

The standalone NX-OS equivalent commands are the following:

ip community-list expanded <list-name> permit <regular expression>

ip extcommunity-list expanded <list-name> permit <regular expression>


route-map <rm-name>

match community <list-name>

match extcommunity <list-name>

● Match Community This is to create match criteria using the BGP community. ACI supports both a Regular and an Extended community type. The community is in a format of AS2:NN2 (two-bytes AS and two-bytes user-defined Network Number) or AS4:NN2 (four-bytes AS number and two-bytes user-defined Network Number) in an extended community. The community syntax when configured on APIC is as follows:

◦ Regular: regular:as2-nn2:<2bytes AS number>:<2 bytes Network Number>

Ex.) “regular:as2-nn2:65001:100”

◦ Extended: extended:as4-nn2:<4 bytes AS number>:<2 bytes Network Number>

Ex.) “extended:as4-nn2:65536:100”

An Extended community type supports the following two scopes;

◦ Transitive: The community will be propagated across eBGP peering (that is, across AS’s).

◦ Non Transitive: The community will not be propagated across eBGP peering (that is, across AS’s).

The standalone NX-OS equivalent commands are the following:

ip community-list standard <list-name> permit <as2:nn community>

ip extcommunity-list standard <list-name> permit 4bytegeneric {transitive | nontransitive} <as4:nn community>


route-map <rm-name>

match community <list-name>

match extcommunity <list-name>

● Match Prefix (Explicit Prefix List) This is to create match criteria using the IP prefix-list. This option was introduced in APIC Release 2.1(1). It is referred to as Explicit Prefix List, in comparison to IP prefix-lists implicitly created by other APIC policies, such for “Export Route Control Subnet”.

When the “Aggregate” option is enabled, the IP prefix-list adds “le 32” to the prefix. This prefix does not have to be 0.0.0.0/0. This can be used as an alternative for “Aggregate Export/Import” for an “Export/Import Route Control Subnet” scope in case an aggregation is required for a non-0.0.0.0/0 subnet. Please remember that “Export/Import Route Control Subnet” scope only supports 0.0.0.0/0 to use “Aggregate Export/Import” option.

The standalone NX-OS equivalent commands are the following:

ip prefix-list <list-name> permit <prefix>/<mask> {le 32}


route-map <rm-name>

match ip address prefix-list <list-name>

Route Profile Set Rule options


Route Profile Set options in GUI (APIC Release 3.2)

● Set Communities (BGP Community) This is to set the BGP Community. The syntax is the same as for Match Community, mentioned above. The available options are as follows:

◦ No Community: This is to remove an existing community.

◦ Append Community: This is to append a community to existing ones.

◦ Replace Community: This is to replace an existing with a new community.

When multiple communities need to be set, use the Additional Communities option on top of this option.

The standalone NX-OS equivalent commands are the following:

route-map <rm-name>

set community <community> {none | additive]

set extcommunity <extcommunity> 4bytes-generic transitive {additive}

● Set Route Tag This is to set a Route Tag. This does not apply for routes that are given a tag by the VRF Tag Policy such as routes advertised (exported) to the outside.

The standalone NX-OS equivalent commands are the following:

route-map <rm-name>

set tag <num>

● Set Dampening (BGP Route Dampening) This is to set parameters for BGP Route Dampening. See the “BGP Route Dampening” in the “L3Out BGP” section for details.

● Set Weight (BGP Weight) This is to set the BGP weight. If the same BGP weight needs to be set for all routes from a particular BGP peer, the weight can be set in the BGP Peer Connectivity Profile instead. See the “BGP protocol options – neighbor-level” subsection for details.

The standalone NX-OS equivalent commands are the following:

route-map <rm-name>

set weight <num>

● Set Next Hop (BGP Next Hop) This is to overwrite the next-hop IP in BGP routes.

The standalone NX-OS equivalent commands are the following:

route-map <rm-name>

set ip next-hop <next-hop ip>

● Set Preference (BGP Local Preference) This is to set the BGP local preference.

The standalone NX-OS equivalent commands are the following:

route-map <rm-name>

set local-preference <num>

● Set Metric This is to set the metric for OSPF or BGP routes, or to set the minimum bandwidth for EIGRP routes.

The standalone NX-OS equivalent commands are the following:

route-map <rm-name>

set metric <num>

● Set Metric Type (OSPF Metric Type) This is to set the OSPF external metric type (Type 1 or Type 2). OSPF uses Type 2 by default, which does not include the cost (metric) to reach the ASBR that originated the external route, whereas Type 1 includes the cost to the ASBR.

The standalone NX-OS equivalent commands are the following:

route-map <rm-name>

set [type-1 | type-2]

● Additional Communities This is used on top of Set Community when multiple communities need to be set. This option was introduced in APIC Release 2.2(2). See Set Community for available configuration parameters.

● Set AS Path (BGP AS Path) This is to prepend a BGP AS Path in BGP routes. This was introduced in APIC release 3.0(1). The available options are as follows:

◦ Prepend AS: This is to manually specify each AS number to prepend.

◦ Prepend Last-AS: This is to prepend automatically the last AS number multiple times.

The standalone NX-OS equivalent commands are the following:

route-map <rm-name>

set as-path prepend <AS> <AS> ...

set as-path prepend last-as <count>

Route Profile Match Rules AND/OR

This subsection explains how multiple match criteria within a Route Profile are handled. This is not regarding Route Profile Type “Match Prefix AND Routing Policy” or “Match Routing Policy Only”, which is regarding the L3Out subnet (Prefix) and the match criteria in the Route Profile. Instead, this is about the match criteria inside a single Route Profile.

The following Figure 102 shows when match criteria are handled as “AND” which means when multiple match criteria are configured in the same route-map sequence. In this situation, the match criteria need to be configured under a single Match Rule Policy in a single context policy.


Route Profile AND Match Rules

The following Figure 103 shows when match criteria are handled as “OR,” which means when multiple match criteria are configured in separate route-map sequences. In this situation, the match criteria need to be configured in separate context policies. The order option in each context defines which context (route-map sequence) is applied first.


Route Profile OR Match Rules (part 1)

Note:

When Match Rules need to be “OR”, one can configure the match criteria in separate Match Rules in a single context policy. However, the order of match criteria will be indeterministic in this case. Thus, choosing this configuration for “OR” is not recommended, unless the order of each match criteria does not matter.


Route Profile OR Match Rules (part 2)

Route Profile on L3Out EPG (export/import routes)


Route Profile on L3Out in the GUI (APIC Release 3.2)

In an L3Out EPG, the Route Profile under each L3Out is used to add Match and/or Set Rules to the internal route-maps used for an “Export Route Control Subnet” or “Import Route Control Subnet” scope. See the “L3Out Transit Route” section for details on each subnet scope. There are two elements in Route Profile for L3Out EPG. The first is Direction and the second is the component to which a Route Profile is associated.

● Route Profile Direction This decides which subnet type the Route Profile is applied to.

◦ Route Export Policy: The Route Profile is applied to subnets with an “Export Route Control Subnet” scope.

◦ Route Import Policy: The Route Profile is applied to subnets with an “Import Route Control Subnet” scope.

● Route Profile Association This decides the scope of the Route Profile. The following describes the behavior when the Route Profile uses Type “Match Prefix AND Routing Policy” without Explicit Prefix List (that is, match prefix criteria), which is the recommended configuration.

◦ L3Out EPG

The Route Profile is applied to all of the configured L3Out subnets with matching direction scope in this L3Out EPG. In Figure 105, the Route Profile “EXPORT_GRP_A” with Export direction on the L3Out EPG1 will be applied to both L3Out subnets 100.0.0.0/024 and 200.0.0.0/24 that have an “Export Route Control Subnet” scope. If the direction of the Route Profile was Import in Figure 105, the Route Profile is not applied to those two subnets because of their nonmatching directions.

◦ L3Out Subnet

The Route Profile is applied to this particular subnet. If the subnet scope and Route Profile direction do not match, the Route Profile is not applied.

Note:

On top of these two association levels, there is one more level. That is a special Route Profile called default-export or default-import, which will be applied to the entire L3Out and associated BDs. See the following subsection for details on default-export/import. When Route Profiles are associated to multiple levels, a more granular scope will be prioritized. This means L3Out Subnet > L3Out EPG > default-export/import.

Example configuration options

The following series of diagrams (Figure 106 through Figure 110) illustrates some recommended configurations and how the rules from the Route Profiles are applied. In these examples, L3Out 1 in the middle is trying to advertise a BD1 subnet and external routes from L3Out 2.


Route Profile Example on the L3Out subnet (Match Prefix AND Routing Policy)

In this example, the Route Profile is applied only to the L3Out subnet 10.0.0.0/24 via Route Profile A. It is also applied to a part of the redistribution for BD subnets (the orange arrow in Figure 106) since the “Export Route Control Subnet” scope could be used to advertise BD subnets. In this example, the Route Profile doesn’t do anything for BD subnet announcements because the BD subnet is different from the IP prefix-list subnet (match prefix L3Out-10 in the picture), and the advertisement is configured via the L3Out association to the BD.


Route Profile Example on the L3Out EPG (Match Prefix AND Routing Policy)

In this example, the Route Profile is applied to all of the L3Out subnets that have a matching direction scope “Export Route Control Subnet”. It is also applied to a part of the redistribution for the BD subnets (the orange arrow in Figure 107) since the “Export Route Control Subnet” scope could be used to advertise the BD subnets. In this example, the Route Profile does not do anything for BD subnet announcements because the BD subnet is different from the IP prefix-list subnet (match prefix L3Out in the picture), and the advertisement is configured via the L3Out association to the BD. In case an “Export Route Control Subnet” scope is used to advertise the BD subnets instead of the L3Out association, the rules from the Route Profile such as Set Rule A will apply to the BD subnet advertisement as well.


Route Profile Example on the L3Out default-export (Match Prefix AND Routing Policy)

In this example, the Route Profile is default-export, and its type is “Match Prefix AND Routing Policy”. Hence, ACI applies the Route Profile to all subnets related to this L3Out 1, including the BD subnets with L3Out association and an “Advertise Externally” scope.


Route Profile Example on L3Out EPG (Match Routing Policy Only)

In this example, the Route Profile is of the type “Matching Routing Policy Only”, hence ACI creates rules based purely on the Route Profile due to “Match Routing Policy Only”. The Route Profile is applied to L3Out routes (the green arrow in Figure 109) and also to the redistribution of the BD subnet (the orange arrow in Figure 109), since the Route Profile could be used to advertise the BD subnet as well (see the “ACI BD subnet advertisement” section). In this example, it does not affect the BD subnet because the prefix (match prefix A in the picture) is different from the BD subnet, and the advertisement is achieved via the L3Out association to the BD. Also, there is no point in creating an L3Out subnet with an “Export Route Control Subnet” scope in this L3Out EPG because it will be ignored in both Transit Routing and BD subnet advertisement due to the Route Profile with “Match Routing Policy Only” in the Export Direction.


Route Profile Example on L3Out default-export (Match Routing Policy Only)

In this example, ACI creates rules based purely on the Route Profile because of “Match Routing Policy Only”. Also, due to default-export, ACI overwrites any other rules related to L3Out 1. In this example, the L3Out association has no effect and is not required, since the BD subnet redistribution from the L3Out association to the BD is ignored and overtaken by default-export with “Match Routing Policy Only”. Because of this, the administrator should include the BD subnet (192.168.1.0/24) in the Explicit Prefix List in default-export (Match Prefix A in Figure 110). However, the “Advertised Externally” scope in the BD subnet is still required.

The configuration in Figure 110 is recommended in case a Route Profile is used to control Transit Routing and BD subnet advertisement.

Note:

There are multiple ways to apply Route Profiles to all possible routes:

1. Use default-export/default-import with Explicit Prefix List of 0.0.0.0/0 and an Aggregate option (Figure 110).

2. Apply a custom Route Profile with Explicit Prefix List of 0.0.0.0/0 and an Aggregate option to L3Out EPG (Figure 109).

3. Apply a custom Route Profile to L3Out EPG or L3Out subnet for 0.0.0.0/0 with an “Export/Import Route Control Subnet” scope and an “Aggregate Export/Import” option. (Supported only from APIC Release 4.2 onward.)

When using the second option, please be aware of the following behavior:

● It is applied to routes from routing protocols (the green arrow in Figure 109 and Figure 110)

● It is not applied to static routes, directly connected subnets, and BD subnets (the orange arrows in Figure 109 and Figure 110).

This is because the internal route-map for routing protocol redistribution and others (static and direct routes) are different. See the “Internal route-map for Transit Routing” in the “L3Out Transit Routing” section for details.

Route Profile Type and L3Out EPG/subnet

The following is a general recommendation regarding the combination of Route Profile Type and Explicit Prefix List. The recommendation is to use an Explicit Prefix List only with the “Match Routing Policy Only” type to avoid overlapping configurations.


L3Out Route Profile Type differences and recommendations

● With Type “Match Prefix AND Routing Policy” The recommendation is to use L3Out subnets with an “Export / Import Route Control Subnet” scope exclusively without an Explicit Prefix List (that is, match prefix criteria) to apply Set Rules or additional Match Rules such as communities. Otherwise, the configuration will not be easy to consume and maintain, because it merges prefixes from L3Out subnets and the Explicit Prefix List, as Figure 99 illustrates.

● With Type “Match Routing Policy Only” The recommendation is to use Explicit Prefix List exclusively without L3Out subnets with an “Export / Import Route Control Subnet” scope because subnets with that scope will be ignored, as Figure 98 shows. It is also recommended to use this type under L3Out always with “default-export” or “default-import” Route Profile instead of a custom Route Profile, because there is no point in using a custom Route Profile and applying it to the L3Out EPG since the L3Out EPG configurations (that is, subnets with an “Export / Import Route Control Subnet” scope) are ignored anyway. See the “default-export / default-import” subsection below for details on these two specific Route Profiles.

Note:

The recommendation regarding L3Out subnets is specific to the “Export / Import Route Control Subnet” scope. Other scopes such as “External Subnets for the External EPG” are not affected and can be used regardless of Route Profile Type.

default-export / default-import


Route Profile default-export / default-import in the GUI (APIC Release 3.2)

When creating a Route Profile / Route Map under L3Out, there are two default policy names in the dropdown menu: default-export and default-import. The configurations of these Route Profiles are the same as in other Route Profiles; the difference is that these two special Route Profiles will take effect without association to other components, such as L3Out EPG or L3Out subnet.

Note:

Prior to APIC Release 4.2, the dropdown menu for default-export and default-import was also displayed on the Tenant-level Route Profile. However, it was not applied anywhere and did not take effect. Starting with APIC Release 4.2, the dropdown menu for default-export and default-import on the Tenant-level Route Profile was removed, and these two special Route Profiles are made available only under L3Out.

When created with Type “Match Prefix AND Routing Policy”, default-export will be applied to all L3Out subnets with an “Export Route Control Subnet” scope (default-import for “Import Route Control Subnet”) without having to associate it to each L3Out subnet or L3Out EPG. The default-export Route Profile will also be applied to BD subnets with an “Advertised Externally” scope in a BD that has association to this L3Out. If each component (L3Out EPG, L3Out subnet, BD subnet) already has its own Route Profile, those Route Profiles will be prioritized over the default-export or default-import Route Profile. Figure 113 shows the internal route-map when default-export is used with type “Match Prefix AND Routing Policy” under L3Out.


L3Out default-export Route Profile with Match Prefix AND Routing Policy

When it is created with Type “Match Routing Policy Only”, it will create a route-map sequence with its own match criteria only and ignore all other internal route-map sequences for L3Out subnet with an “Export Route Control Subnet” scope (or “Import Route Control Subnet” for default-import). The default-export Route Profile will also ignore the configuration of BD subnets with an “Advertise Externally” scope in the BD to which the L3Out is associated. If each component (L3Out EPG, L3Out subnet, BD subnet) has its own Route Profile, they will be prioritized over the default-export or default-import Route Profile. Figure 114 shows the internal route-map when default-export is used with type “Match Routing Policy Only” under L3Out.


L3Out default-export Route Profile with Match Routing Policy Only

default-export for simple routing control (the recommended configuration)

This subsection explains how default-export can simplify the configuration of subnet advertisement in ACI.


Default-export for BD Subnets and Transit Routing

Figure 115 illustrates an example of how the default-export Route Profile with type “Match Routing Policy Only” simplifies the configuration to advertise subnets from the L3Out. As mentioned in the previous sections, the default-export with type “Match Routing Policy Only” ignores other subnet advertisement configurations such as an “Export Route Control Subnet” scope and the L3Out association to the BD. Then, without merging with other configurations, the default-export is applied to internal route-maps for both Static/Direct/BD subnets (the orange arrow in the picture) and routes from routing protocols (the green arrow in the picture). Hence, it can be used as a single source of controls for any subnet advertisements from the L3Out. In this example, the default-export Route Profile is configured with the BD subnet (192.168.1.0/24) and one of the external routes from L3Out 2 (10.0.0.0/24). Without any other configurations in the L3Out, the L3Out advertises those routes to the outside. Please note that the “Advertise Externally” scope in the BD subnet is still required. If set rules need to be applied to those routes, it can be easily accomplished by adding a set rule in the same Route Profile (default-export) as shown in the picture.

With this approach, the L3Out EPG can focus on the security (contracts) management with an “External Subnets for the External EPG” scope for subnets that are learned via the L3Out as opposed to the routes it is advertising to the outside. L3Out shared service (VRF route leaking) still needs to be configured in the L3Out EPG.

Please also see the “ACI BD subnet advertisement” section for comparisons of the configuration options to advertise BD subnets.

Route Profile on BD


Route Profile on BD in the GUI (APIC Release 3.2)


Route Profile on BD subnet in the GUI (APIC Release 3.2)

In the BD, the Route Profile is used to add match and/or set rules to internal route-maps used for advertising BD subnets to the outside via L3Out association to BD, which redistributes BD subnets to the L3Out routing protocol. See the “ACI BD subnet advertisement” section for details on internal route-maps for this.

Unlike the Route Profile on L3Out EPG, there is no direction in the BD. You just need to specify the L3Out that owns the Route Profile to be applied first, since a BD may be associated to multiple L3Outs.

Note:

The same recommendation on Route Profile Type from Route Profile on L3Out EPG is applied to the BD. When using an Explicit Prefix List, the recommendation is to use Type “Match Routing Policy Only”. In such scenario, it is equivalent to scenario 3 in Figure 69 from the “ACI BD subnet advertisement” section.

Route Profile on interleak


ACI interleak

As briefly mentioned in the “Infra MP-BGP” section, interleak is an internal automatic redistribution for MP-BGP from an L3Out since the first release of Cisco ACI. An option to apply a Route Profile to interleak was introduced in APIC Release 1.2(2). This option is supported for OSPF and EIGRP L3Outs. The BGP L3Out does not need interleak because the routes are already in the BGP table. Applying a Route Profile to interleak of static routes was introduced in APIC Release 4.2(1).


Route Profile for interleak in the GUI (APIC Release 3.2)

For interleak, a Route Profile is associated to an L3Out itself, as Figure 119 shows. The Route Profile used for interleak is the tenant-level Route Profile instead of the one under L3Out.


Route Profile contents for interleak

Route Profile for interleak needs to use the “Match Routing Policy Only” type; there is no point in using “Match Prefix AND Routing Policy” because there is no subnet information that it can refer to in objects unlike the case for a Route Profile on an L3Out EPG. Route Profile for interleak is intended to set a community rule that will traverse infra MP-BGP to other border leaf switches so that other border leaf switches can selectively perform Transit Routing based on the community.

Figure 121 shows an example of a Route Profile for interleak use case where L3Out 2 assigns different metrics for the same external route from two other L3Outs (1 and 3).


Route Profile for interleak use case

L3Out shared service (VRF route leaking)


L3Out shared service (VRF route leaking)

VRF route leaking with L3Out was introduced in APIC release 1.2(1). This feature allows the leaking of external routes learned via L3Out to another VRF so that it can be consumed by EPGs in another VRF. This feature is also referred to as L3Out shared service or shared L3Out because it is sharing a service behind L3Out to another VRF. Figure 122 shows the typical use case where L3Out is providing a service (10.0.0.0/24) to an EPG in another VRF (VRF 1). The EPG on the left in VRF 1 could be another L3Out in VRF1 instead that consumes the external routes from VRF 2, which is a combination of L3Out Transit Routing and shared service. See some limitations from “Shared Layer 3 Out” section in ACI Fundamentals Guide.

Note:

Prior to this feature, the term “shared service” used to imply that a user tenant used a component from the Tenant Common; for example, user tenant A using VRF A, defined in Tenant Common, for BDs in tenant A. This allowed endpoints in tenant A to belong to the same IP space (Tenant Common VRF A) as services (endpoints or L3Outs) in Tenant Common. In this case, no VRF route leaking was required for cross-tenant communication because all the components were in the same VRF A. However, since the release of VRF route leaking, “shared service” tends to imply VRF route leaking rather than sharing services from Tenant Common. VRF route leaking can be within a tenant or between tenants. The original shared service using the Tenant Common is still a valid design and configuration.


L3Out subnet shared service scope in the GUI (APIC Release 3.2)

There are two L3Out subnet scopes for the L3Out shared service:

● Shared Route Control Subnet: This is to leak the routes in the routing tables into another VRF.

● Shared Security Import Subnet: This is to leak prefix-to-pcTag mapping into another VRF. This needs to be used with an “External Subnets for the External EPG” scope.

Basic configuration example


Example diagram of a shared L3Out configuration

Figure 124 illustrates the most basic shared L3Out configuration, in which an L3Out provides a service (subnet 10.0.0.0/24) from VRF 2 to endpoints in VRF 1. As mentioned previously, there are two parts in shared L3Out: route leaking and prefix-pcTag mapping leaking for contracts.

Route leaking

This is to leak routes between routing tables in each VRF, like a normal router. The three components to make this happen are the following:

● L3Out Subnet with “Shared Route Control Subnet” scope

This is to define which external routes (or static routes) in the routing table to leak. This is the part in orange (10.0.0.0/24 in VRF 2) in Figure 124. If the route is not in the routing table, it will not be leaked.

● BD Subnet with “Shared between VRFs” and “Advertised Externally” scope

This is to define which BD subnet to leak. This is the part in blue (192.168.1.0/24 in VRF 1) in Figure 124. The “Shared between VRFs” scope is the one to leak the route to another VRF. The “Advertised Externally” scope is also required so that the leaked BD subnet will be advertised to external routers via the L3Out in VRF 2. In this case, L3Out association to the BD or any other BD subnet advertisement configuration mentioned in Figure 69 is not required.

● Contract between the L3Out EPG and an EPG in the BD

This is to define between which VRFs these routes need to be leaked, on top of the main purpose of contracts; to allow traffic. The scope of the contract needs to be Global when the two VRFs are in different tenants, or Tenant when the two VRFs are in the same tenant. The scope Application Profile is not applicable here since the L3Out EPG is not part of any application profile.

Prefix-pcTag (contract) leaking

This is to leak the prefix-pcTag mapping. By default, pcTag is unique only within a VRF, and an EPG in VRF 1 and an L3Out EPG in VRF 2 (in the example shown in Figure 124) could use the same pcTag. Hence, ACI has a concept called global pcTag that is unique across all VRFs in the ACI fabric. There are two parts in shared L3Out to utilize this global pcTag for contracts across VRFs.

● L3Out Subnet with “Shared Security Import Subnet” scope This is to define which prefix-pcTag mapping to leak. Hence, the L3Out subnet must be configured with an “External Subnets for the External EPG” scope as well to create the prefix-pcTag mapping in the first place. This is the part in green (10.0.0.0/24 to pcTag 10000) in Figure 124. See the “L3Out contracts” section for details on prefix-pcTag mapping.

● Contract between the L3Out EPG and an EPG in the BD This is to define which VRF the prefix-pcTag mapping needs to be leaked to and which EPG needs to use a global pcTag. Once a contract with the scope Global or Tenant is consumed and provided across VRFs, the pcTag of the provider EPG is changed to a global pcTag. In the example in Figure 124, the L3Out EPG in VRF 2 is assigned a global pcTag. This global pcTag is used to create a prefix-pcTag mapping in VRF 2 and leaked to VRF 1 due to the “Shared Security Import Subnet” scope.

This implies that the contract is always applied on the consumer VRF side that has the pcTag information for both the consumer and the provider sides. The provider side (VRF 2 in Figure 124) will not be aware of the pcTag (EPG) of the endpoints in the consumer VRF and will always allow leaked traffic, assuming that the consumer side will take care of it. In the example in Figure 124, there will be no prefix-pcTag mapping table entry for the endpoint 192.168.1.1, because the prefix-pcTag mapping table is only for L3Out external routes, and the endpoints use the endpoint table.

Figure 125 shows a GUI configuration summary of this basic example.


Example of a configuration of shared L3Out in the GUI (APIC Release 3.2)

Note:

A global pcTag uses a number lower than, while a normal pcTag uses a number higher than, 0x4000 (16384). The scope of a normal pcTag is per VRF so that the same number can be reused in multiple VRFs for scalability. When a normal pcTag is changed to a global pcTag due to a contract on an existing EPG, there may be a small amount of traffic disruption due to rewriting of all of the contract rules on the switches that have the new global pcTag.

Shared L3Out Subnet scopes

“Shared Route Control Subnet” and “Shared Security Import Subnet”

The “Shared Route Control Subnet” and “Shared Security Import Subnet” scopes are typically configured on the same L3Out subnet entry. However, users can configure the more granular “Shared Security Import Subnet” scope than the “Shared Route Control Subnet” scope in case different contract needs to be applied for the subset of the leaked subnets. For example, the following configuration is to leak 10.0.0.0/8 in a routing table into another VRF, but the prefix-pcTag mapping is created for 10.1.0.0/16 and 10.2.0.0/16 respectively so that different contracts can be applied for each prefix.

● 10.0.0.0/8 with a “Shared Route Control Subnet” scope

● 10.1.0.0/16 and 10.2.0.0/16 with a “Shared Security Import Subnet” scope (and an “External Subnets for the External EPG” scope)

However, a “Shared Security Import Subnet” scope cannot be less granular than a “Shared Route Control Subnet” scope, such as 10.0.0.0/4 in this example.

“Aggregate Shared Routes”

This scope is used with a “Shared Route Control Subnet” scope. Just like a “Export Route Control Subnet” scope, a “Shared Route Control Subnet” scope also internally uses an IP prefix-list, hence it is an exact match. When an “Aggregate Shared Routes” scope is enabled with a “Shared Route Control Subnet” scope, it adds “le 32” in the IP prefix-list entry that will match with any subsets of the configured subnet. Unlike an “Export Route Control Subnet” scope, this aggregate option for shared routes can be used not only for 0.0.0.0/0 but also for non-0.0.0.0/0 subnets.

Shared L3Out configuration options


Shared L3Out configurations (1. L3Out is Provider)

The first option is using L3Out as the provider. This is the most basic configuration, as explained above. The L3Out is the provider; the EPG is the consumer. See the “Basic configuration example” above for details.


Shared L3Out configurations (2. L3Out is Consumer)

The other option is to use the L3Out as the consumer and the EPG as the provider. With this option, a BD subnet is configured under EPG 1, since EPG 1 is the provider (see the configuration guide for VRF Route Leaking between normal EPGs). EPG X has an endpoint in the same subnet, but it does not have a contract with VRF 2. Please note that even though the subnet is configured under the EPG, the subnet is deployed on leaf switches as a BD SVI, and other EPGs under the same BD can also use the same subnet. This configuration where the L3Out EPG is the consumer has a caveat because of pcTag usage and how contracts are applied. With this design, even though EPG 1 is a provider, not only EPG 1 but also the L3Out EPG will use a global pcTag (EPG X still uses a normal pcTag). A caveat on how contracts are applied is described below:

● Both the consumer and the provider VRF will have the same contract rule and a contract will be applied on an ingress VRF.

● In the consumer VRF (VRF 2), the global pcTag of EPG 1 will be tied to the BD (EPG) subnet (192.168.1.0/24). This means that traffic from the L3Out EPG in VRF 2 to any IP in the 192.168.1.0/24 subnet will be allowed in VRF 2 even if the destination IP does not belong to EPG 1. For example, when the packet to 192.168.1.100 that belongs to EPG X instead of EPG 1 enters VRF 2, ACI will get a pcTag based on the leaked subnet 192.168.1.0/24. This pcTag is the leaked pcTag of EPG 1 that actually has the contract for the shared L3Out. Then, VRF2 allows the packet using the leaked pcTag, even though the packet destination does not belong to the leaked EPG.

The recommendation to avoid this problem is to configure a smaller subnet that only includes the IP addresses of EPG 1 instead of the entire BD subnet. “No Default SVI Gateway” should be enabled for this smaller EPG subnet to avoid having an unnecessary secondary IP address on the BD SVI. Figure 128 illustrates an example of a smaller EPG subnet. In this case, the BD subnet 192.168.1.254/24 should be configured under the BD instead without the “Advertised Externally” and “Shared between VRFs” scopes so that the BD can still provide the pervasive gateway for EPGs such as EPG X.


Smaller subnet under an EPG with No Default SVI Gateway


Shared L3Out configurations (3. L3Out is Provider and Consumer / Transit Routing)

The third option consists in configuring Transit Routing between different VRFs. This is a Transit Routing and shared service design combined. This configuration has been supported since APIC Release 2.2(2). To complete the communication, a normal Transit Routing configuration (an “Export Route Control Subnet” scope or a Route Profile such as default-export for the leaked route) also needs to be configured on each VRF so that the leaked routes can be advertised to the outside.

Advanced shared L3Out configuration options

Advanced configuration 1 (L3Out EPG separation)

Figure 130 illustrates a bad example of a configuration, where only a subset of external routes accessible between VRFs. In this example, the requirements are the following:

● VRF 2 has intra-VRF communication between L3Out (10.0.0.0/24) and an EPG (172.16.1.1)

● Only half of the L3Out routes (10.0.0.128/25) should be able to communicate between VRFs


Shared L3Out advanced configuration 1 (an incorrect example)

The problem is that, not only 10.0.0.128/25, but also the entire 10.0.0.0/24 from VRF 2 can reach 192.168.1.1 in VRF 1 even though the “Shared Security Import Subnet” scope is configured only for 10.0.0.128/25. This is because 10.0.0.0/24 is configured with “External Subnets for the External EPG” in the same L3Out EPG. This causes both prefixes (10.0.0.0/24 and 10.0.0.128/25) to be mapped to the single global pcTag 10000 in VRF 2. When a packet (source IP 10.0.0.1, destination IP 192.168.1.1) arrives from the L3Out in VRF 2, the source IP 10.0.0.1 is classified into the global pcTag 10000 by the ingress provider VRF (VRF 2). Hence, even though the consumer VRF (VRF 1) is not aware of the prefix-pcTag mapping for IP 10.0.0.1 (which is outside of 10.0.0.128/25), the source of the packet is already classified into the global pcTag 10000 by VRF 2 when the packet reaches VRF 1, which is a consumer that will apply the contract. A contract for 10.0.0.128/25 will then be applied based on the pcTag 10000 even though 10.0.0.1 is outside of 10.0.0.128/25 and will allow the packet to be sent to the destination 192.168.1.1. This is only for the provider-to-consumer direction. The opposite direction (192.168.1.1 to 10.0.0.1) will be dropped in the consumer VRF (VRF 1).

To avoid allowing such unintended traffic between VRFs, the configuration needs to be changed to the one shown in Figure 131.


Shared L3Out advanced configuration 1 (a correct example)

In Figure 131, the configuration uses a different L3Out EPG for intra-VRF communication (10.0.0.0/24). Because of this, when a packet (source IP 10.0.0.1, destination IP 192.168.1.1) arrives from the L3Out in VRF 2, the source IP 10.0.0.1 is classified into the normal pcTag 49000 and will be dropped by the ingress provider VRF (VRF 2) since there is no route-leaking configuration for the normal pcTag (L3Out EPG 1). This ensures that only 10.0.0.128/25 will be allowed to go between VRFs.

Advanced configuration 2 (Shared L3Out with multiple VRF/BDs)


Shared L3Out advanced configuration 2 (Shared L3Out with multiple VRF/BDs)

Figure 132 illustrates a configuration where one L3Out (in VRF 2) is sharing (leaking) a default route to EPGs in VRF 1 and VRF 3. This configuration is valid. EPGs in VRF 1 and 3 can communicate with devices behind L3Out in VRF 2 based on the contract. However, users need to be aware of a security hole with this configuration in first-generation leaf switches. This configuration with first-generation leaf switches results in that EPG 1 in VRF 1 and EPG 3 in VRF 3 can also communicate with each other through VRF 2. In shared service (VRF route leaking), a contract is applied in a consumer VRF with a global pcTag shared by a provider VRF. This means, in this example, that a contract is applied on either VRF 1 or VRF 3. When EPG 1 in VRF 1 tries to talk to 192.168.200.1 (EPG 3 in VRF 3), it falls under the default route leaked by VRF 2, and a contract for the default route is applied. Hence, the packet is allowed on the ingress consumer VRF 1. After that, it just follows a routing table on each VRF in order to reach VRF 3 without being applied more contracts. To avoid this issue on first-generation leaf switches, the L3Out in VRF 2 should leak only unique routes that do not overlap with subnets in other VRFs.

On second-generation (or later) leaf switches, traffic from VRF 1 gets dropped on VRF 2 instead of getting forwarded to VRF 3 through VRF 2. However, when the ingress EPG and the shared L3Out are on the same leaf, traffic is sent out to the external device which may send the traffic back to ACI VRF 2. In such a case, the traffic is sent to VRF 3 if 0.0.0.0/0 is used for “External Subnets for the External EPG/Shared Security Import Subnet” on top of “Shared Route Control Subnet”. If non-0.0.0.0/0 subnets such as 0.0.0.0/1 and 128.0.0.0/1 are used for “External Subnets for the External EPG/Shared Security Import Subnet” while 0.0.0.0/0 is used only for “Shared Route Control Subnet”, the traffic is dropped when it’s sent back to ACI VRF 2.

Advanced configuration 3 (Shared L3Out with multiple VRF/L3Outs)


Shared L3Out advanced configuration 3 (Shared L3Out with multiple VRF/L3Outs)

Figure 133 illustrates a configuration where one L3Out (in VRF 2) is sharing (leaking) a default route to VRF 1 and VRF 3. In exchange, VRF 2 is receiving external routes (10.0.0.0/8, 30.0.0.0/8) from L3Outs in VRF 1 and 3. This configuration may allow traffic from L3Out 1 (VRF 1) to L3Out 3 (VRF 3) through VRF 2 regardless of leaf generations. On second-generation leaf switches, if the source L3Out (VRF) is on the same border leaf as the intermediate VRF 2, the traffic is sent out to Router 2 via L3Out 2 in VRF 2 instead of being reforwarded to VRF 3. For example, if L3Out 1 (VRF 1) and L3Out 2 (VRF 2) are on the same border leaf, traffic from 10.0.0.0/8 (L3Out 1) to 30.0.0.0/8 (L3Out 3) is sent out to Router 2 in VRF 2 instead of being reforwarded to L3Out 3 in VRF 3. This means that if all three L3Outs are on the same border leaf, this security hole can be prevented. In the case of three L3Outs being deployed on different leaf switches, VRF 2 should leak only unique routes that do not overlap with other VRFs. This issue will be fixed through the following:

CSCvt06173 ACI: Shared L3Outs allow traffic through the intermediate VRF

Advanced configuration 4 (Shared L3Out with unintended leak)


Shared L3Out advanced configuration 4 (Shared L3Out with unintended leak)

Figure 134 illustrates a configuration where VRF 2 is receiving (shared/leaked) routes from multiple VRFs. The intention is L3Out 1 (VRF 1) to leak only 10.0.0.0/8 without 11.0.0.0/8, and L3Out 3 (VRF 3) to leak all routes to VRF 2. In this configuration, however, not only 10.0.0.0/8 but also all routes from L3Out 1 (VRF 1) are leaked to VRF 2. This is because VRF 2 only checks the prefix when it imports routes from other VRFs via MP-BGP VPNv4. It does not check its source VRF, which is identified by a Route-Target (RT) as mentioned in the “Infra MP-BGP” section. Hence, in this particular example, all routes from any VRFs are subject to be leaked (imported via MP-BGP) to VRF 2 due to Aggregate Shared Routes in L3Out 3. The same thing occurs if L3Out 3 is configured with Shared Route Control Subnet for 11.0.0.0/8 explicitly instead of the aggregate option. This means that the shared L3Out configuration in each VRF should specify only its own unique external routes without overlapping with other VRFs. This limitation was resolved in ACI 5.0(1k) as a result of the following enhancement:

CSCvi20535 ACI: Need VRF awareness in Shared Route Control scope for Shared L3Outs

L3Out BFD

Bidirectional forwarding detection (BFD) on L3Out interfaces was introduced in APIC Release 1.2(2). See the APIC Layer 3 Networking Configuration Guide for BFD on other components, such as ISIS between leaf and spine switches, OSPF, and static routes between spines and IPN devices, etc.

Limitations

● BFD on L3Out is supported only on routed interface, subinterface, and SVI. It is not supported on loopback interfaces since there is no multihop BFD in ACI yet.

● BFD for BGP prefix peers (dynamic neighbors) is not supported.

● BFD subinterface optimization can be enabled only on the Interface BFD Policy, but not on the Global BFD Policy. When BFD subinterface optimization is enabled on one subinterface, it will be activated for all of the subinterfaces on the same physical interface.

Use BFD on L3Out

There is just one checkbox in each L3Out to enable and establish a BFD session if no customization is required. As mentioned in the previous sections for each of the L3Out routing protocols, Figure 135 shows the checkbox to enable BFD, which is disabled by default.


Enable BFD on L3Out routing protocols in the GUI (APIC Release 3.2)

When BFD is enabled without any customization, the BFD parameters will be derived from a default BFD policy located under “Fabric > Access Policies > Policies > Switch > BFD > BFD IPv4/v6 > default”. See the next section for how to customize BFD parameters.

Customize BFD on L3Out


Global BFD parameters in the GUI (APIC Release 3.2)

A default global BFD policy located under “Fabric > Access Policies > Policies > Switch > BFD > BFD IPv4/v6 > default” contains the BFD parameters to be used on any switches in the ACI fabric. Users can also create a nondefault BFD policy and apply it to a specific switch via Switch Policy Group and Switch Profile under “Fabric > Access Policies > Switches”.


Interface BFD parameters in the GUI (APIC Release 3.2)

Users can override the BFD parameters from a switch-level global BFD policy via an interface-level BFD policy by creating BFD Interface Profile under Logical Interface Profile. The interface-level BFD policy is located under “Tenant > Policies > Protocol > BFD”.

Document history

New or Revised Content

Updated section

Date

Added a note to call out that routes learned via OSPF inside a BGPL3Out are not distributed to other switches via MP-BGP.

L3Out BGP > Limitations and guidelines

February 28, 2023

Added a corner case for 2nd generation switches when the ingress VRF and shared VRF are on the same leaf

L3Out Shared Service (VRF route leaking) > Advanced shared L3Out configuration options > Advanced configuration 2 (Shared L3Out with multiple VRF/BDs)

January 6, 2023


Contents

Introduction. 1

Cisco Nexus Dashboard Insights Configuration Prerequisites for Cisco ACI Fabrics. 1

In-band Management 1

Configuration Steps. 1

Connecting Cisco ACI In-band Management Network with the Cisco Nexus Dashboard Data Network - Option 1a: Directly Connected to an EPG via Phyiscal Domain and Static Path Binding. 1

Configuration Steps. 1

Connecting Cisco ACI In-band Management Network with the Cisco Nexus Dashboard Data Network - Option 1b: Directly Connected to an EPG Using a VMM Domain for virtual Cisco Nexus Dashboard. 1

Configuration Steps. 1

Connecting Cisco ACI In-band Management Network with the Cisco Nexus Dashboard Data Network - Option 2: Any Infra (Using an L3Out) 1

Configuration Steps. 1

Network Time Protocol 1

Configuration Steps. 1

Precision Time Protocol 1

Single Pod Grandmaster Configuration: 1

Monitoring Policy (Fabric Node Control Policy) 1

Telemetry Policy. 1

NetFlow Policy. 1

Cisco Nexus Dashboard Configuration. 1

Adding a Cisco ACI Site to Cisco Nexus Dashboard. 1

(Optional) Configuring External Service Pools – Required for NetFlow.. 1

Cisco Nexus Dashboard Insights Setup. 1

Configuration Steps for Cisco Nexus Dashboard Insights 6.x Site Group: 1

Configuration Steps for a Cisco Nexus Insights Release 5.x Site. 1

Cisco ACI NetFlow Configuration. 1

NetFlow Records Policy. 1

NetFlow Exporters Policy. 1

NetFlow Monitor Policy. 1

Tenant Level NetFlow.. 1

Access Policy NetFlow.. 1

Basic Verification. 1

In-band Verification. 1

Cisco APIC Verification. 1

Switch Verification. 1

Connectivity to Cisco Nexus Dashboard Data Interface Verification. 1

Network Time Protocol Verification. 1

Cisco APIC Network Time Protocol Verification. 1

Switch Network Time Protocol Verification. 1

Precision Time Protocol Verification. 1

Fabric Node Control Verification. 1

NetFlow Verification. 1


Introduction

Cisco Nexus Dashboard Insights is the Day-2-Operations tool for Cisco Data Center fabrics. Focusing on anomalies in the network, Cisco Nexus Dashboard Insights gives operators quick visibility into the network health through a modern and scalable architecture. For more information, see the Cisco Nexus Dashboard Insights White Paper.

This document is intended to serve as a checklist and guide for configuring Cisco ACI fabrics to support Cisco Nexus Dashboard Insights. In this white paper, prerequisites such as in-band management, how to connect Cisco Nexus Dashboard to Cisco ACI, Network Time Protocol (NTP), Precision Time Protocol (PTP), and Monitoring policies will be discussed. Further, the configuration of Cisco Nexus Dashboard and Cisco Nexus Dashboard Insights to onboard apps will be covered. Finally, there will be a verification and troubleshooting section.

We will use an example fabric with the following topology to configure the below settings. This fabric is running release 5.1(3e) and is cabled in the following manner:


Note: The documentation set for this product strives to use bias-free language. For the purposes of this documentation set, bias-free is defined as language that does not imply discrimination based on age, disability, gender, racial identity, ethnic identity, sexual orientation, socioeconomic status, and intersectionality. Exceptions may be present in the documentation due to language that is hardcoded in the user interfaces of the product software, language used based on RFP documentation, or language that is used by a referenced third-party product.

Cisco Nexus Dashboard Insights Configuration Prerequisites for Cisco ACI Fabrics

In-band Management

The Cisco Nexus Dashboard Insights service uses the Cisco ACI in-band management network to receive the network telemetry data from the Cisco APIC controllers and all the switches in the fabric. Therefore, you must configure in-band management for your Cisco ACI fabric. The in-band management configuration can be summarized with the following major pieces:

● Access Policies for Cisco APIC interfaces (access ports)

● MGMT tenant in-band bridge domain with a subnet

● MGMT tenant node management address (Cisco APIC, leaf switch, and spine switch)

● MGMT tenant node management EPG for in-band management

This section shows how to configure the in-band management EPG and allocate in-band IP addresses to the fabric devices. For more information, see the Cisco APIC Basic Configuration Guide, Release 5.1(x) - Management.

You must open specific ports to use in-band management. For information about which ports to open, see the various "Communication Ports" sections of the Cisco Nexus Dashboard User Guide for the release that you deployed.

Configuration Steps

The steps include:

1. Navigate to Fabric > Access Policies and in the Quick Start menu, choose Configure Interface, PC and vPC.


2. In the dialog, click the green plus + symbol twice to expand the wizard.

a. Select the two switches where the Cisco APIC ports are connected from the drop-down list.

b. Enter a name in the Switch Profile Name field.

c. Set the Interface Type to Individual.

d. In the Interfaces field, enter the Cisco APIC interfaces either as a comma-separated list or as a range.

e. Enter a name in the Interface Selector Name field.

f. Set the Interface Policy Group to Create One. You do not need to select an interface-level policy; the defaults are sufficient.

g. In the Attached Device Type drop-down list, choose Bare Metal.

h. The Domain and VLAN should both be set to Create One.

i. Enter a name in the Domain Name field to name the physical domain that is associated with in-band management.

ii. Enter a VLAN ID that will be used for in-band management in the fabric.


i. Click Save.

j. Click Save again.


k. Click Submit.


3. Navigate to Tenants > mgmt.

4. Expand Networking > Bridge Domains > inb.


5. Right-click the Subnets folder and choose Create Subnet.


6. In the dialog, enter the gateway IP address of the in-band management subnet.

a. Choose Advertised Externally if needed for L3Out advertisement.

7. Click Submit.

8. Still in the mgmt tenant, right-click the Node Management EPGs folder and choose Create In-Band Management EPG.


9. In the dialog:

a. Enter a name for the in-band management EPG.

b. Enter the VLAN defined in step 2.h.ii when configuring access policies. Use "VLAN-###" as the format.

c. In the Bridge Domain drop-down list, choose the in-band bridge domain.

d. Click Submit.


10. Still in the mgmt tenant, expand Node Management Addresses.

11. Right-click Static Node Management Addresses and choose Create Static Node Management Addresses.


12. In the dialog:

a. For Node Range, enter a range of 1 to 1 to configure Cisco APIC 1.

b. Put a check in the In-band Addresses check box.

c. In the In-Band Management EPG drop-down list, choose the EPG that you created in step 9.

d. In the In-Band IPv4 Address field, enter the IP address that should be used for the node with the CIDR mask.

e. In the In-Band IPv4 Gateway field, enter the IP address of the gateway assigned to the in-band bridge domain in step 6.


f. Click Submit.

13. Repeat step 12 for each Cisco APIC using node ID 1, 2, 3 and so on as needed. Likewise, repeat the step for each leaf and spine node ID in the fabric.

Connecting Cisco ACI In-band Management Network with the Cisco Nexus Dashboard Data Network - Option 1a: Directly Connected to an EPG via Phyiscal Domain and Static Path Binding

Cisco Nexus Dashboard data interfaces network should be reachable to the Cisco ACI in-band network. For simplicity, there are two major connectivity options available to accomplish this:

● Cisco Nexus Dashboard as an endpoint inside Cisco ACI, residing in a new and unique bridge domain and EPG combination

● Cisco Nexus Dashboard reachable using a L3Out in the MGMT tenant in-band VRF instance


With the first option, the Cisco Nexus Dashboard should be learned as a Layer 3 endpoint in Cisco ACI and the Cisco ACI fabric should act as the gateway for the Cisco Nexus Dashboard. To reach the in-band bridge domain subnet, either deploy the Cisco Nexus Dashboard bridge domain locally inside the MGMT tenant tied to the in-band VRF instance, otherwise route leaking would be necessary to leak the Cisco Nexus Dashboard subnet into the in-band VRF instance and likewise the in-band bridge domain subnet into the Cisco Nexus Dashboard VRF instance.

Configuration Steps

The steps Include:

1. Navigate to Fabric > Access Policies and in the Quick Start menu choose Configure Interface, PC and vPC.


2. In the dialog, click the green plus + symbol twice to expand the wizard.

a. Select the two switches where the Cisco Nexus Dashboard Data ports are connected from the drop-down list.

b. Enter a name in the Switch Profile Name field.

c. Set the Interface to Individual.

d. In the Interfaces field, enter the Cisco Nexus Dashboard interfaces as either a comma-separated list or as a range.

e. Enter a name in the Interface Selector Name field.

f. Set the Interface Policy Group to Create One. You do not need to select an interface-level policy; the defaults are sufficient.

g. In the Attached Device Type drop-down list, choose Bare Metal.

h. The Domain and VLAN should both be set to Create One.

i. Enter a name in the Domain Name field to name the physical domain that is associated with in-band management.

ii. Enter a VLAN ID that will be used for static path bindings in the fabric.


i. Click Save.

j. Click Save again.


k. Click Submit.


3. Navigate to Tenants > mgmt.

4. Expand Networking > Bridge Domains.

5. Right-click the Bridge Domains folder and choose Create Bridge Domain.


6. Enter a name for the bridge domain.

a. In the VRF drop-down list, choose inb.

b. Click Next.

c. Click the + symbol in the Subnets area to bring up a dialog.

i. Enter the Cisco Nexus Dashboard Data Network gateway IP address and CIDR mask.

ii. Choose Advertise Externally as needed.

iii. Click OK.

d. If necessary, under Associated L3Outs, click the + and choose the in-band VRF instance L3Out.

e. Click Next.

f. Click Finish.

7. Still under the mgmt tenant, navigate to Application Profiles and right-click and choose Create Application Profile.


8. In the dialog, enter a name for the application profile.

9. Under EPGs, click the + symbol.

a. Enter a name for the EPG where the Cisco Nexus Dashboard data interface will belong.

b. Choose the bridge domain created in step 5.

c. Choose the physical domain that you created earlier.

d. Click Update.


e. Click Submit.

10. Expand the newly created Application Profile > Application EPGs > EPG and click the Static Ports folder.

a. Right-click the folder and select Deploy Static EPG on PC, vPC, or Interface.


b. In the dialog, choose Port.

c. In the Node drop-down list, choose the first leaf node where the first Cisco Nexus Dashboard Data interface is connected.

d. In the Path drop-down list, choose the interface on the node where the Cisco Nexus Dashboard Data interface is connected.

e. In the Port Encap field, enter the VLAN number that you created earlier when defining the VLAN pool under the access policies.

f. For Deployment Immediacy, choose Immediate.

g. For Mode, choose the appropriate mode based on how the Cisco Nexus Dashboard appliance was configured.

i. If the Cisco Nexus Dashboard was configured with a VLAN TAG, choose Trunk.

ii. If the Cisco Nexus Dashboard was configured without a VLAN TAG, choose Access.

h. Click Next.

i. Click Finish.

j. Repeat this process for all Cisco Nexus Dashboard Data interfaces connected to the fabric.


11. Still under the mgmt tenant, navigate to Contracts and expand the folder.

12. Right-click Standard and choose Create Contract.


13. In the dialog, name the contract. Use a name that is clear in terms of the flow. For example: ND-to-inb.

a. Click the + to create a subject.

i. In the new dialog, name the subject.

ii. Click the + to create a new filter.

iii. Expand the drop-down list under Name and click the + to create a new filter.


iv. In the new dialog, name the filter.

v. Click the + under Entries.


vi. Name the entry.

vii. Choose the Ether Type from the drop-down list. For allowing all communications, leave the value as unspecified.

viii. Choose the IP protocol.

ix. Enter the destination port.

x. Click Update.

xi. Click Submit in the Create Filter dialog. The new filter should be selected under the Create Contract Subject.

b. Click Update.


c. Click OK to complete the subject.

d. The subject should show up under the Subjects section of the Create Contract dialog.


e. Click Submit.

14. Still in the mgmt tenant, navigate to Application Profiles > your-AP-name > Application EPGs > your-EPG-name, and right-click Contracts, and choose Add Consumed Contract.


15. In the dialog, in the Contract drop-down list, choose the contract that you created in step 13.


a. Click Submit.

16. Still in the mgmt tenant, navigate to Node Management EPGs and choose the in-band EPG.

17. Under Provided Contracts, click the + and in the Name drop-down list, choose the contract that you created in step 13.


18. Click Update.

This completes the connectivity section for Cisco Nexus Dashboard being directly connected to an EPG toward in-band management.

Connecting Cisco ACI In-band Management Network with the Cisco Nexus Dashboard Data Network - Option 1b: Directly Connected to an EPG Using a VMM Domain for virtual Cisco Nexus Dashboard

In Cisco ACI, VMM integration is a process that leverages external virtualization controller northbound APIs to manage network constructs in a programmable, automated, and highly scalable manner. Multiple hypervisor vendors are supported. See the Virtualization Compatability Matrix for more information. For more information on VMM integration, see the Cisco ACI Virtualization Guide.

With Cisco Nexus Dashboard release 2.1, the virtual form factor use case is expanded into Cisco Nexus Dashboard Insights. Currently, VMware vCenter (.ova) and KVM (.qcow2) virtual machines are supported. For detailed information on virtual Cisco Nexus Dashboard and its deployment, see the Cisco Nexus Dashboard 2.1 Deplyoment Guide. The main requirements around connectivity remain, whereby the data interface of the virtual Cisco Nexus Dashboard would need access to the inband management network of Cisco ACI. For the purpose of this document, we will assume VMM Integration is in place either to VMware vCenter or to Red Hat Virtualization.

Similar to option 1a, the assumption here is that the virtual Cisco Nexus Dashboard is directly connected using a supported hypervisor to a leaf switch or through a single intermediate switch. The virtual Cisco Nexus Dashboard should be learned as a Layer 3 endpoint in Cisco ACI and the Cisco ACI fabric should act as the gateway for the Cisco Nexus Dashboard. To reach the in-band bridge domain subnet, either deploy the Cisco Nexus Dashboard bridge domain locally inside the MGMT tenant tied to the in-band VRF instance, otherwise route leaking would be necessary to leak the Cisco Nexus Dashboard subnet into the in-band VRF instance and likewise the in-band bridge domain subnet into the Cisco Nexus Dashboard VRF instance.

By virtue of the VMM integration, there is no need to program the leaf switch interfaces manually for where the hypervisor that hosts the virtual Cisco Nexus Dashboard will be connected. VMM integration will dynamically program the VLAN on the port where the VM is detected. The only thing that is required is the correct access policies as well as associating the VMM domain to the EPG.

Configuration Steps

Prerequisites:

· An existing VMM domain

o Access policies for a new hypervisor tied to an existing VMM domain

o Existing hypervisor with configured access policies tied to the existing VMM domain

This section focuses on the tenant aspect, including:

· Tenant policies

o Bridge domain for Cisco Nexus Dashboard Data interface and matching EPG inside an application profile.

o Bridge domain subnet for Cisco Nexus Dashboard Data interface

o Contract allowing communication to the in-band (node control) EPG

The steps Include:

1. Navigate to Tenants > mgmt.

2. Expand Networking > Bridge Domains.

3. Right-click the Bridge Domains folder and choose Create Bridge Domain.


4. Enter a name for the bridge domain.

a. In the VRF drop-down list, choose inb.

b. Click Next.

c. Click the + symbol in the Subnets area to bring up a dialog.

i. Enter the Cisco Nexus Dashboard Data Network gateway IP address and CIDR mask.

ii. Choose Advertise Externally as needed.

iii. Click OK.

d. If necessary, under Associated L3Outs, click the + and choose the in-band VRF instance L3Out.

e. Click Next.

f. Click Finish.

5. Still under the mgmt tenant, navigate to Application Profiles and right-click and choose Create Application Profile.


6. In the dialog, enter a name for the application profile.

7. Under EPGs, click the + symbol.

a. Enter a name for the EPG where the Cisco Nexus Dashboard data interface will belong.

b. Choose the bridge domain created in step 5.

c. Choose the VMM domain.

d. Click Update.


e. Click Submit.

8. Still under the mgmt tenant, navigate to Contracts and expand the folder.

9. Right-click Standard and choose Create Contract.


10. In the dialog, name the contract. Use a name that is clear in terms of the flow. Example: ND-to-inb.

a. Click the + to create a subject.

i. In the new dialog, name the subject.

ii. Click the + to create a new filter.

iii. Expand the drop-down list under Name and click the + to create a new filter.


iv. In the new dialog, name the filter.

v. Click the + under Entries.


vi. Name the entry.

vii. Choose the Ether Type from the drop-down list. For allowing all communications, leave the value as unspecified.

viii. Choose the IP protocol.

ix. Enter the destination port.

x. Click Update.

xi. Click Submit in the Create Filter dialog. The new filter should be selected under the Create Contract Subject.

b. Click Update.


c. Click OK to complete the subject.

d. The subject should show up under the Subjects section of the Create Contract dialog.


e. Click Submit.

11. Still in the mgmt tenant, navigate to Application Profiles > your-AP-name > Application EPGs > your-EPG-name, and right-click Contracts, and choose Add Consumed Contract.


12. In the dialog, in the Contract drop-down list, choose the contract that you created in step 13.


a. Click Submit.

13. Still in the mgmt tenant, navigate to Node Management EPGs and choose the in-band EPG.

14. Under Provided Contracts, click the + and in the Name drop-down list, choose the contract that you created in step 13.


15. Click Update.

This completes the connectivity section for virtual Cisco Nexus Dashboard being directly connected to an EPG leverageing VMM Integration.

Connecting Cisco ACI In-band Management Network with the Cisco Nexus Dashboard Data Network - Option 2: Any Infra (Using an L3Out)

In this deployment model, the Cisco Nexus Dashboard data interface is located external to Cisco ACI on any infra. The data network must have reachability to the Cisco ACI in-band management network and likewise Cisco ACI in-band must have reachability to the Cisco Nexus Dashboard data interfaces. For Cisco ACI internal VRF instances to communicate with outside networks, an L3Out is required to establish peering with an external router.

This section will go over the high-level steps to configure an L3Out for the in-band management VRF instance "inb" and advertise the in-band bridge domain subnet out as well as learn and apply the policy to external subnets such as the Cisco Nexus Dashboard data interface.

The any infra configuration can be summarized with the following major pieces:

● Access policies for L3Out

● Configuring the L3Out

● Contracts between in-band EPG and L3Out External EPG


For more information, see the Cisco APIC Layer 3 Networking Configuration Guide, Release 5.2(x) and Cisco ACI Fabric L3Out Guide white paper.

Configuration Steps

The steps include:

1. Navigate to Fabric > Access Policies and in the Quick Start menu, choose Configure Interface, PC and vPC.


2. In the dialog, click the green plus + symbol twice to expand the wizard.

a. Choose from the drop-down list the switches where the external router is connected.

b. Enter a name in the Switch Profile Name field.

c. Set the Interface Type to Individual.

d. In the Interfaces field, enter the ports where the external router is connected either as a comma-separated list or as a range.

e. Enter a name in the Interface Selector Name field.

f. Set the Interface Policy Group to Create One. Choose the appropriate interface level properties needed for the external router.

g. In the Attached Device Type drop-down list, choose Bare Metal.

h. The Domain and VLAN should both be set to Create One.

i. Enter a name in the Domain Name field to name the physical domain that is associated with in-band management.

ii. Enter a VLAN ID that will be used for static path bindings in the fabric.


i. Click Save.

j. Click Save again.


k. Click Submit.


3. Navigate to Tenants > mgmt.

4. Expand Networking.

5. Right-click the L3Outs folder and choose Create L3Out.


6. In the new dialog:

a. Enter a name for the L3Out.

b. Choose the VRF instance.

c. Choose the Layer 3 domain that you created in the previous steps.

d. Choose the routing protocol or leave the field blank for static routing.

e. Click Next.


f. Choose the Layer 3 and Layer 2 interface type.

g. Choose the node, and enter a router ID and loopback if necessary.

h. Choose the interface and enter the appropriate parameters.


i. Click Next.

j. Use the drop-down list to choose the appropriate interface protocol policy.


k. Click Next.

l. Enter the name for the external EPG.

m. Click Finish.

7. Still under the mgmt tenant, navigate to Networking > Bridge Domains > inb > subnets and click on the subnet that you defined.


8. In the work pane, ensure that there is a check in the Advertised Externally check box.


9. Click the parent bridge domain object called "inb," then in the work pane click on the Policy tab > Layer 3 Configurations tab and click the + symbol next to Associated L3Outs.


a. Choose the L3Out created in the previous step from the drop-down list.

b. Click Update.


10. Still under the mgmt tenant, navigate to Contracts and expand the folder.

11. Right-click Standard to create a new standard contract.


12. In the dialog, name the contract.

a. Use a name that is clear in terms of the flow. For example: ND-to-inb.

b. Click the + to create a subject.

i. In the new dialog, name the subject.

ii. Click the + to create a new filter.

iii. Expand the drop-down list under Name and click the + to create a new filter.


iv. In the new dialog, name the filter.

v. Click the + under Entries.


vi. Name the entry.

vii. In the EtherType drop-down list, choose a type. To allow all communications, leave the value as Unspecified.

viii. Choose the IP protocol.

ix. Enter the destination port.

x. Click Update.

xi. Click Submit in the Create Filter dialog. The new filter should be selected under the Create Contract Subject.

c. Click Update.


d. Click OK to complete the subject.

e. The subject should show up under the Subjects section of the Create Contract dialog.


f. Click Submit.

13. Still in the mgmt tenant, navigate to Networking > L3Outs > your-L3Out > External EPGs and select the external EPG.


14. In the work pane, click on the Contracts tab, click the Action button, and choose Add Consumed Contract.


a. Choose Submit.

15. Still in the mgmt tenant, navigate to Node Management EPGs and choose the in-band EPG.

16. Under Provided Contracts, click the + and in the Name drop-down list, choose the contract that you created.


17. Click Update.

This completes the connectivity section for Cisco Nexus Dashboard on any infra toward in-band management.

Network Time Protocol

Network time protocol (NTP) is a core Cisco ACI service that should be enabled regardless of using Cisco Nexus Dashboard Insights or not. Having NTP enabled on the Cisco APIC and switches ensures consistency among log messages, faults, events, and internal atomic counters for debugging. This is required for Cisco Nexus Dashboard Insights to correlate information correctly and show meaningful anomalies and their relationships.

See the Cisco APIC Basic Configuration Guide, Release 5.2(x) - Provisioning Core Cisco ACI Fabric Services for the traditional steps to configure NTP by configuring the date/time policy under Fabric > Fabric Policies. The following procedure uses a new wizard to configure the same policy.

Configuration Steps

1. In the main menu, navigate to System > Quick Start and choose First time setup of the ACI Fabric.


2. In the dialog, under NTP, choose Edit Configuration or Review and Configure if it has not been set up previously.

3. In the dialog:

a. Choose the display format preference for the Cisco APIC.

b. Choose the time zone for the Cisco APIC.

c. Under NTP Servers, click the + to add in the IP address or hostname of the NTP server to be used by this site.


d. Click Save and Continue.

4. Click Proceed to Summary and click Close.

Precision Time Protocol

If the flow analytics in Cisco Nexus Dashboard Insights is enabled for a data center network site, and flow monitoring rules are provisioned, every Cisco Nexus 9000 series switch in the site will stream out the flow records for the monitored flows on a per second basis. A flow record has a rich set of meta data about the flow and a precision time protocol (PTP) time stamp. Upon receiving the streamed flow records from switches in the network, Cisco Nexus Dashboard Insights runs flow analytics and correlation functions to stitch the flow data from individual switches together to form end-to-end flows. For each of the flows, Cisco Nexus Dashboard Insights uses the PTP time stamps to calculate end-to-end flow latency.

For the flow latency calculation to function correctly, the network switches need to have PTP enabled and configured correctly. They need to use the same PTP grandmaster.

For more information, see the Cisco APIC System Management Configuration Guide, Release 5.1(x) - Precision Time Protocol.

For a Cisco ACI fabric with only a single pod, PTP can be enabled without the need for an external grandmaster. The fabric will elect a single spine switch to act as a grandmaster and all other switches will synchronize to this grandmaster. Cisco ACI Multi-Pod fabrics require an external grandmaster, and we recommend that you have them connected to the external IPN device. This ensures an equal number of hops to reach the active grandmaster. You can connect the grandmaster on a leaf switch port as well using an EPG or L3Out, which then can be used as grandmaster candidates in case the active grandmaster goes down.

Single Pod Grandmaster Configuration:

1. In the main menu, navigate to System > System Settings > PTP and Latency (previously known as the Precision Time Protocol setting).


2. In the work pane, for Precision Time Protocol, choose Enabled.


3. At the bottom, click Submit.

Monitoring Policy (Fabric Node Control Policy)

Telemetry Policy

For information about the monitoring policy, see the Cisco APIC Troubleshooting Guide, Release 4.2(x) - DOM.

The fabric node control policy is used to enable digital optical monitoring (DOM) and concurrently to select a flow collection feature such as Analytics (Cisco Secure Workload [Tetration]), NetFlow, and Telemetry (Cisco Nexus Dashboard Insights). This is the same policy used for enabling DOM. To apply this policy, fabric-level switch selectors for leaf and spine switches should be configured and a policy group to reference this fabric node control policy should be selected.

1. Navigate to Fabric > Fabric Policies > Policies > Monitoring > Fabric Node Controls > default.


2. In the work pane, put a check in the Enable DOM check box.

3. For Feature Selection, choose Telemetry Priority.


After you complete the above steps, you must apply the policy through the familiar profile, selectors, and policy group associations, except this time, apply the policy for leaf switches and spines switches instead of for interfaces. Begin by creating a leaf policy group:

1. Navigate to Fabric > Fabric Policies > Switches > Leaf Switches > Policy Groups.

2. Right-click Policy Groups and choose Create Leaf Switch Policy Group.


3. In the dialog, name the policy group.

a. For node control policy, if you created a custom policy earlier, choose that policy here and click Submit.

b. Otherwise, the default will be used when nothing is selected. You can click Submit with a blank policy group.


4. Next, create a profile under Fabric > Fabric Policies > Switches > Leaf Switches > Profiles.


5. In the dialog, give the profile a name.

a. For Switch Associations, click the + symbol to add a row.


i. Give the switch association a name.

ii. Choose the switches in the Blocks section using the drop-down list and put a check in the check box to choose all leaf switches.

iii. In the Policy Group drop-down list, choose the policy group that has DOM and Telemetry enabled.

iv. Click Submit.

The policy group has now been applied to all leaf switches. The steps should be repeated for all spine switches. This includes creating the policy group and referencing the node control policy, creating a spine switch profile, and associating the policy group to the block of spine switches.

NetFlow Policy

For information about the monitoring policy, see the Cisco APIC Troubleshooting Guide, Release 4.2(x) - DOM.

The fabric node control policy is used to enable digital optical monitoring (DOM) and concurrently to select a flow collection feature such as Analytics (Cisco Secure Workload [Tetration]), NetFlow, and Telemetry (Cisco Nexus Dashboard Insights). This is the same policy used for enabling DOM. To apply this policy, fabric-level switch selectors for leaf and spine switches should be configured and a policy group to reference this fabric node control policy should be selected.

For more information on Cisco Nexus Dashboard Insights and NetFlow support, see Cisco Nexus Dashboard Insights User Guide

1. Navigate to Fabric > Fabric Policies > Policies > Monitoring > Fabric Node Controls > default.


2. In the work pane, put a check in the Enable DOM check box.

3. For Feature Selection, choose Telemetry Priority.


After you complete the above steps, must apply the policy through the familiar profile, selectors, and policy group associations, except this time, apply the policy for leaf switches and spines switches instead of for interfaces. Begin by creating a leaf policy group:

4. Navigate to Fabric > Fabric Policies > Switches > Leaf Switches > Policy Groups.

5. Right-click Policy Groups and choose Create Leaf Switch Policy Group.


6. In the dialog, name the policy group.

a. For node control policy, if you created a custom policy earlier, choose that policy here and click Submit.

b. Otherwise, the default will be used when nothing is selected. You can click Submit with a blank policy group.


7. Next, create a profile under Fabric > Fabric Policies > Switches > Leaf Switches > Profiles.


8. In the dialog, give the profile a name.

a. For Switch Associations, click the + symbol to add a row.


i. Give the switch association a name.

ii. Choose the switches in the Blocks section using the drop-down list and put a check in the check box to choose all leaf switches.

iii. In the Policy Group drop-down list, choose the policy group that has DOM and Telemetry enabled.

iv. Click Submit.

The policy group has now been applied to all leaf switches. The steps should be repeated for all spine switches. This includes creating the policy group and referencing the node control policy, creating a spine switch profile, and associating the policy group to the block of spine switches.


Cisco Nexus Dashboard Configuration

Adding a Cisco ACI Site to Cisco Nexus Dashboard

This operation is conducted in Cisco Nexus Dashboard by selecting the Sites option in the menu bar. You must enter the name of the node management in-band EPG that you configured previously when adding the site into Cisco Nexus Dashboard.

For more information about Cisco Nexus Dashboard and Cisco Nexus Dashboard Insights, see the Cisco Nexus Dashboard Deployment Guide - Fabric Connectivity and Cisco Nexus Dashboard Insights User Guide - Installation and Setup.

1. Using a browser, open a session to the Cisco Nexus Dashboard GUI.

2. In the left-hand menu, choose Sites.


3. In the work pane, choose Actions > Add Site.


4. In the new screen, ensure Cisco ACI is selected as the site type.

a. Fill in the name for this site. This name will carry over to all other Services, such as Cisco Nexus Dashboard Orchestrator or Cisco Nexus Insights.

b. Enter the in-band IP address of the Cisco APIC.

c. Enter a username for authentication to the Cisco APIC.

d. Enter the password for the specified username. This password is only used once for the initial connection. Afterward, a certificate-based authentication is used between Cisco APIC and Cisco Nexus Dashboard for all subsequent operations.

e. Specify the login domain for username.

f. Enter the node management in-band EPG name.

g. Drop the pin on the map.

h. Click Add at the bottom right corner.


(Optional) Configuring External Service Pools – Required for NetFlow

The external service pools are used to configure persistent IP addresses to be used for certain services. These persistent IP addresses are retained even if the backend service is relocated to a different Cisco Nexus Dashboard Node. For more information, see the Cisco Nexus Dashboard User Guide.

The external service pools are required for NetFlow and are used to when programing the flow exporter under the NetFlow monitoring policy.

Note: If Cisco Nexus Dashboard Insights is already running before the external service pools are created, Cisco Nexus Dashboard Insights must be disabled and re-enabled for the changes to take effect.


This section provides an overview of the steps to configure the external service pools in Cisco Nexus Dashboard. Detailed steps are provided later in this document.

1. Using a browser, open a session to the Cisco Nexus Dashboard GUI.

2. In the left-hand menu, Choose Infrastructure > Cluster Configuration.

3. Under the External Service Pool tile, click the pencil icon to edit the external serivce pools.


4. In the pop-up, under Data Serivce IP’s, click Add IP Address.


5. In the text box, input the IP address and click the green checkmark to save the entry.

6. Click Add IP Address once again and repeat the process until you have configured six data service IP addresses. These IP addresses will be randomly claimed by services.


Cisco Nexus Dashboard Insights Setup

Cisco Nexus Dashboard Insights setup is used to enable sites that are registered on Cisco Nexus Dashboard.

Enable the following key features to receive the greatest benefit from Cisco Nexus Dashboard Insights:

● Software Analytics: Used to stream switch and Cisco APIC software analytics to Cisco Nexus Dashboard Insights for further processing, correlation, and anomaly detection. Set this to Enabled.

● Flow Analytics: Used to configure rules and have switches export flow metadata to Cisco Nexus Dashboard Insights. Set this to Enabled.

● Microburst Sensitivity: Based on a threshold percentage, this setting can be set to low, medium, or high.

With the Cisco Nexus Dashboard Insights 6.0 release, the following new features have been added:

● Multiple sites can now be grouped into a site group for a holistic view of related sites.

● Bug Scan can be enabled to run peridocially to check the fabric for known defects downloaded from the Cisco Cloud.

● Assurance Analysis: Used to take a detailed snapshot of the network fabric periodically including intent, policy, and hardware states. These snapshots can then be used for delta analysis, pre-change analysis, and to query the model using natural language queries in the Explore function.

● Alert rules can be configured to provide more granular control of anomalies, setting the initial state to acknowledge or to customize the recommendation.

● Compliance requirements can be enabled to provide communication or configuration checks on the snapshots to ensure business requirements and operational requirements are in compliance with known standards.


Configuration Steps for Cisco Nexus Dashboard Insights 6.x Site Group:

This section discusses the high level steps to enable a site in Cisco Nexus Dashboard Insights release 6.0. Detailed steps on creating a site group or adding a site into an existing site group are out of the scope of this document. For more information about setting up Cisco Nexus Dashboard Insights, see the Cisco Nexus Dashboard Insights 6.x ACI User Guide - Installation and Setup and if necessary, the Cisco Nexus Dashboard Insights 6.x Deployment Guide.

1. Using a browser, open a session to the Cisco Nexus Dashboard GUI.

2. In the left-hand menu, choose Services.

3. Choose Insights from the menu.


After the Cisco Nexus Dashboard Insights service launches, proceed to either create a new site group or configure a site group already in existence by editing the existing site group and adding a member. The following steps are equivalent for both new and existing site groups.

1. Select Member opens a pop-up that shows the available sites. Select an available site and click Select.


2. Set the Status to Enabled.


3. In the Configuration column, click Configure. This opens a new pop-up.


a. In this pop-up input the username and password that will be used for assurance analysis. These credentials should have admin-level privilege.


b. Click Save.

4. Click the checkmark to save.


5. Click Save at the bottom.

a. After the site is added successfully, the Collection Status column value changes from "Adding" to "Enabled."

6. Click the Microbusrt tab along the top menu, then click the sensitivity drop-down list and choose the desired microburst sensitivity for the site.


7. Click on the Flows tab in the top menu, then click on the pencil icon to edit the new site.


a. In the pop-up, choose the toggle for the desired flow collection mode.

b. Create flow telemetry rules, if needed.


c. Click Save at the bottom.

8. Click on the Assurance Analysis tab in the top menu, then click on the pencil icon to edit the new site.


a. In the pop-up, set the state to Enabled.

b. If necessary, choose a start time in the future in case another site is currently running an assurance analysis.

c. For repeat time, ensure enough time is allocated for a large fabric. See the User Guide for more information.

d. Click Save at the bottom.


9. Click the Bug Scan tab in the top menu, then click on the pencil icon to edit the new site.


a. In the pop-up, set the state to Enabled.

b. Click Save at the bottom.


10. After you have enabled all desired settings, click on the X in the blue title bar to return to the Site Group Overview page.


Configuration Steps for a Cisco Nexus Insights Release 5.x Site

For more information about setting up Cisco Nexus Dashboard Insights, see the Cisco Nexus Insights 5.x ACI User Guide - Installation and Setup.

1. Using a browser, open a session to the Cisco Nexus Dashboard GUI.

2. In the left-hand menu, choose Service Catalog.

3. Click Open for Cisco Nexus Insights.


This will open a new tab in your browser. If this is your first time setting up Cisco Nexus Insights, a setup wizard will appear.

4. In the Multi-Site and Flow Configuration section, click Edit configuration.


5. Using the drop-down lists under the SW Analytics and Flow Analytics columns, choose Enabled where appropriate. Under the Microburst Sensitivity column, choose the desired sensitivity.


6. Click Done at the bottom right corner to exit the Enable Multiple Sites screen.

7. Click Done once more to exit the Insights Setup screen.

At this point, it will take about 5-15 minutes for data to start populating into the Cisco Nexus Dashboard Insights service before any information can be displayed. You can add any future sites into Cisco Nexus Dashboard as described in the previous chapter. In Cisco Nexus Insights, in the top right toolbar, you can also click the Gear/Settings icon and choose Nexus Insights Setup.


Cisco ACI NetFlow Configuration

If NetFlow is required, ensure the Cisco Nexus Dashboard External Service Pools persistent IP addresses are assigned on the data network and ensure the site in the site group flow collection mode is set to NetFlow. If it is, the NetFlow Exporter IP addresses will be shown under the Collector List column as shown below:


The high level workflow for NetFlow is consistent across tenant NetFlow and access policy NetFlow. The workflow consists of a NetFlow record policy that defines what to collect and matching what fields in the headers and a NetFlow Exporter that defines the source and destination IP address, NetFlow version, and EPG where the destination can be reached. Finally, the NetFlow record policy and NetFlow exporter is referenced by the NetFlow monitor policy, which is then applied to a bridge domain or interface policy group.

Specifically for Cisco Nexus Dashboard Insights, the destination port to be used in the NetFlow Exporter is 5641.

Specific configuration steps are out of the scope of this document. For more information on NetFlow and Cisco ACI, see theCisco APIC and NetFlow technote.

Below are examples of each of the required policies:

NetFlow Records Policy

Use the drop-down list to choose the necessary options.

NetFlow Exporters Policy

We recommend that you use Source Type = Inband Management IP. With this option, there is no need to input any IP address manually in the Source IP Address field. Ensure that destination port 5641 and Version 9 is used. Also, NetFlow in Cisco ACI requires that the exporter IP address be in a user VRF instance or common/default VRF instance. The L3out can be in the mgmt tenant.


NetFlow Monitor Policy

The NetFlow monitor policy simply ties the record policy and monitoring policy together to be used by the desired object, such as a bridge domain or interface policy group.


Tenant Level NetFlow


This configuration can be found under Tenant > Policies > NetFlow.

As shown, the NetFlow monitor is then attached to the bridge domain under Policy > Advanced/Troubleshooting.

The following screenshot shows how to apply a NetFlow monitor policy on an existing bridge domain in Cisco APIC release 5.2:


Access Policy NetFlow


This configuration can be found under Fabric > Access Policies > Policies > Interface > NetFlow.

As shown, the NetFlow monitor is then attached to the interface policy group.

The following screenshot shows how to apply a NetFlow monitor policy on an existing vPC interface policy group in Cisco APIC release 5.2:


Basic Verification

In-band Verification

As with any configuration in Cisco ACI, the first thing to do is check for faults. In this case, check the mgmt tenant or the system level for faults.

Cisco APIC Verification

From the Cisco APIC GUI, navigate to System > Controllers > Controllers > Interfaces and ensure there is a new entry under L3 Management Interfaces. There should be a new bond0 with the VLAN configured in the VLAN pool and in the node management EPG.


From the Cisco APIC CLI, run ifconfig bond0.98 and verify the IP address on the in-band interface:

Switch Verification

From the Cisco APIC, you can run the show switch command to check in-band IP configuration quickly:


For connectivity, connect to a leaf switch either through the Cisco APIC or out-of-band management, run the show ip int brief vrf mgmt:inb command:


With that output, we can determine that VLAN14 on this leaf switch is the SVI for the in-band bridge domain. Running the show ip int VLAN14 command shows the gateway as secondary, and the primary is the static node address for the switch itself:


Finally, test connectivity with iping. Send a ping to the Cisco APIC in-band address:

Connectivity to Cisco Nexus Dashboard Data Interface Verification

This test can be performed from the Cisco APIC or from the leaf switch. To start, looking at the EPG where Cisco Nexus Dashboard is configured is an easy way to confirm whether Cisco ACI has learned the Cisco Nexus Dashboard at all.

Navigate to Tenants > mgmt > Application Profiles > [name] > Application EPGs > [name] > Operational to view the endpoints.


From the Cisco APIC CLI, issue a ping to each of the IP addresses listed.


If the ping fails, ensure there is a contract in place between the in-band EPG and the Cisco Nexus Dashboard EPG.


Network Time Protocol Verification

Cisco APIC Network Time Protocol Verification

From the NX-OS-style CLI, run the show ntp command to display the Cisco APICs' configuration and status:


Switch Network Time Protocol Verification

The standard NX-OS commands apply, as well as some Linux commands:

● show clock

● show ntp peers

● show ntp peer-status

● show ntp statistics peer ipaddr <ip>

● date

● cat /etc/timezone


Precision Time Protocol Verification

The standard NX-OS commands apply:

● show ptp parent

● show ptp counters all

● show ptp clock

With single-pod Cisco ACI, all switches should have the same parent clock:


Fabric Node Control Verification

The basic verification on the switch to ensure the node control policy was correctly applied is show analytics hw-profile, and it should output "Telemetry" as the feature priority:

After Cisco Nexus Dashboard Insights has been configured and enabled for the site, running show analytics exporter will show the data interface IP addresses of the Cisco Nexus Dashboard as export destinations:


NetFlow Verification

The basic verification on the switch to ensure the flow exporters are configured correctly, flows are being collected in the cache, and if NetFlow packets is being generated and exported by the CPU:

· show flow exporter

· show flow monitor

· show flow cache

· tcpdump -i kpm_inb port 5641

F1-P1-Leaf-104# show flow exporter

Flow exporter dpita:dpita-flow-exp:

Destination: 192.168.100.104

VRF: common:default (1)

Destination UDP Port 5641

Source: 192.168.99.104

DSCP 44

Export Version 9

Sequence number 262

Data template timeout 0 seconds

Exporter Statistics

Number of Flow Records Exported 974

Number of Templates Exported 171

Number of Export Packets Sent 262

Number of Export Bytes Sent 56740

Number of Destination Unreachable Events 0

Number of No Buffer Events 0

Number of Packets Dropped (No Route to Host) 0

Number of Packets Dropped (other) 0

Number of Packets Dropped (Output Drops) 0

Time statistics were last cleared: Never

Flow exporter dpita:dpita-test-exp2:

Destination: 192.168.100.105

VRF: common:default (1)

Destination UDP Port 5641

Source: 192.168.99.104

DSCP 44

Export Version 9

Sequence number 262

Data template timeout 0 seconds

Exporter Statistics

Number of Flow Records Exported 974

Number of Templates Exported 171

Number of Export Packets Sent 262

Number of Export Bytes Sent 56740

Number of Destination Unreachable Events 0

Number of No Buffer Events 0

Number of Packets Dropped (No Route to Host) 0

Number of Packets Dropped (other) 0

Number of Packets Dropped (Output Drops) 0

Time statistics were last cleared: Never


Feature Prio: NetFlow


F1-P1-Leaf-104# show flow monitor

Flow Monitor default:

Use count: 0

Flow Record: default

Flow Monitor dpita:dpita-test-mon:

Use count: 1

Flow Record: dpita:dpita-test-record

Bucket Id: 1

Flow Exporter: dpita:dpita-flow-exp

Flow Monitor dpita:dpita-test-105:

Use count: 1

Flow Record: dpita:dpita-test-record

Bucket Id: 1

Flow Exporter: dpita:dpita-test-exp2


Feature Prio: NetFlow


F1-P1-Leaf-104# show flow cache

IPV4 Entries

SIP DIP BD ID S-Port D-Port Protocol Byte Count Packet Count TCP FLAGS if_id flowStart flowEnd

192.168.1.100 192.168.4.100 537 0 0 1 86814 63 0x0 0x16000000 1217618386 1217638714


F1-P1-Leaf-104# tcpdump -i kpm_inb port 5641

tcpdump: verbose output suppressed, use -v or -vv for full protocol decode

listening on kpm_inb, link-type EN10MB (Ethernet), capture size 262144 bytes

11:47:40.116456 IP 192.168.99.104.52255 > 192.168.100.104.5641: UDP, length 220

11:47:40.116588 IP 192.168.99.104.39779 > 192.168.100.106.5641: UDP, length 220


FabricPath to ACI Migration Cisco Validated Design Guide

First Published: September 1, 2014 Last Updated: May 5, 2017

Contents

■ Introduction 3

¯ Preface 3

¯ Audience 3

¯ Scope 3

¯ Advantages to Adopting ACI 4

■ Initial Design Considerations 6

¯ Extension Considerations 6

¯ Layer 3 Considerations 13

¯ Management Out-of-Band and In-Band 14

■ Migration Strategy 15

¯ Connectivity With VLAN to EPG Static Mappings 16

¯ Scenario 1: Mapping a VLAN to Multiple EPGs 19

¯ Scenario 2: Mapping VLANs to EPGs (1:1) 22

■ Infrastructure Deployment Considerations 26

¯ FabricPath Enabled Datacenter 26

¯ Management 30

¯ Virtual Environment 30

¯ ACI Infrastructure Deployment 35

■ Migration Scenario 1 58

¯ Fabric Access Policy Configuration 59

¯ Tenant Configuration 63

¯ Integration Phase – Scenario 1 80

¯ Migration Phase – Scenario 1 88

¯ Fabric Optimization 109

■ Migration Scenario 2 110

¯ Fabric Access Policy Configuration 112

¯ Tenant Configuration 114

¯ Integration Phase – Scenario 2 123

¯ Migration Phase – Scenario 2 128

¯ Fabric Optimization 149

■ Lessons Learned 151

¯ UCS B-series and ACI integration considerations 151

¯ Infra Address Pool 151

¯ ACI Object Naming Conventions 152

■ Obtaining Documentation and Submitting a Service Request 154

■ Legal Information 155


Introduction

Preface

This document describes the migration procedures that can be adopted to move workloads and applications between a Brownfield environment and a new Greenfield ACI fabric. Different use cases are discussed, including the migration of network services and of the connectivity to the external Layer 3 network.

Audience

This document is intended for use by network architects and engineers to aid in developing operational-based solutions for Cisco ACI.

Scope

The scope of this document is to specifically cover Cisco ACI concepts for implementing an operational model for the ACI fabric. Limited background information is included on other related components whose understanding is required for the solution implementation. For more background information on ACI please refer to the following link: http://www.cisco.com/go/aci.

The following documents discuss Cisco ACI design and deployment considerations, which are useful prerequisites:

· Cisco Application Centric Infrastructure Design Guide: http://www.cisco.com/c/en/us/solutions/collateral/data-center-virtualization/application-centric-infrastructure/white-paper-c11-731960.html

· Cisco Application Centric Infrastructure (ACI) - Endpoint Groups (EPG) Usage and Design: http://www.cisco.com/c/en/us/solutions/collateral/data-center-virtualization/application-centric-infrastructure/white-paper-c11-731630.html


Advantages to Adopting Cisco ACI

Cisco Application Centric Infrastructure (ACI) is an innovative secure architecture that delivers centralized application-driven policy automation, management, and visibility of physical and virtual networks. ACI is built upon a fabric foundation that delivers the best-in-class infrastructure by combining hardware, software, and ASIC innovations into an integrated system.

ACI provides significant advantages over a FabricPath-based network, and some of those are listed as follows:

Areas

FabricPath

ACI Fabric

ACI Technical Advantages

ACI Business Advantages

Fabric Technology

routing

VXLAN, bridging, gateway, vPC

· Enables a reliable, yet flexible placement of multitenant segments throughout the data center.

· Enables better utilization of available network paths in the underlying infrastructure.

· Reduces time to market for new services in a reliable manner

· Enables an easy, modular, and scalable approach to deploy and place application workloads anywhere in the data center

Fabric Technology Enhancements

N/A

Enhanced VXLAN

· Enhanced VXLAN provides advanced capabilities such as atomic counters and fabric-wide security.

· ACI-vCenter integration means VNIDs are where you need them and when, instead of needing to configure them on all switches, which helps scale effectively.

· Complete overlay visibility and troubleshooting insight.

· Increase in application performance reflects directly on improved customer experience and ease of use of business and consumer services

· Increased reliability due to proactive problem resolution

Endpoint (MAC/IP) Discovery

Multicast, broadcast flooding

Routing control plane

· With routing control plane, ACI inherently increases stability, reliability, and security due to the use of routing and eliminating broadcast, multicast-based endpoint (MAC) discovery

· Combines the efficiency of Layer 3 routing and VXLAN to provide a highly flexible, secure, and scalable solution

· A more stable, reliable, and secure fabric directly contributes to customer success

· Increases operational efficiency of customer staff

Management

Per device

A single system

· Industry leading Cisco Nexus operating system

· Even with hundreds of switches, ACI provides a single point of managing the fabric via Application Policy Infrastructure Controller (APIC).

· Freedom from VNID/VLANID management on a per-switch basis

· Unified firmware/software management control with rolling updgrade schedules

· Substantial operational savings from eliminating hours of time and effort spent in managing hundreds of switches (including configuration, status checks, or upgrades)

Operations

Per device

Single system

· As a single system, ACI provides complete application, network, and virtual compute visibility

· Visibility: application health scores, fabric health scores, device health scores

· Ability to perform impact analysis with reflection on what applications are impacted by network configuration changes (such as if a switch goes down, which EPGs/VLANs/VXLANs are impacted)

· Subnet <> BD <> EPG independence (much simpler to implement than VXLANs/VXLANs on a large scale as would be needed in any legacy networking solution, including standalone)

· End-to-end visibility reduces troubleshooting time of not only network infrastructure, but also for the virtual, compute, and application infrastructure

· Increase in application performance reflects directly on improved customer experience and ease of use of business services

Security

Per device

Integrated

· Automates security policy while allowing security teams to retain control over policies for compliance

· Automated insertion of security services simplifies application deployments

· Effectively addresses the ever-increasing concern around security

· Improved security with faster provisioning

· Simplified operations

Programmability

XML, Python per device

Fabric-wide APIs available on APIC

· There is a single-point API for entire fabric

· Support for OpFlex and device packages to extend fabric policy outside of the fabric

· Consistency and agility across infrastructure

· Flexible deployment, easier scaling, and lower TCO

Virtual Integration

Not built in

Readily available

· Central deployment model accelerates network and security infrastructure configuration

· Helps enable an any-workload, anywhere deployment model

· Operational efficiency

· Rapid deployment

· Higher availability and increased customer satisfaction

L4 – L7 Integration

No Automation

Automated and tightly coupled

Tightly coupled L4-L7 service automation enables automation of application lifecycle

· Increase in application performance reflects directly on improved customer experience and ease of use of business services

· End-to-end visibility reduces troubleshooting time of not only network infrastructure, but also for the virtual, compute, and application infrastructure

Application Intelligence

Traditional VLAN-based

Application Profiles, EPG-based grouping, Application Policy

· Ability to define network policy by application definition

· Contracts allow granular, simple control of interaction endpoint groups

· Improves time to market, as application provision can be automated end-to-end and with ease

· Operational improvement enabled by self-documenting data center through the APIC policy model

· Provides real-time visibility into detailed information about application, compute, VMs, and associated policies

· Improved security and management


In a nutshell, the motivation for customers to adopt ACI is due to:

■ Having an out-of-box automated fabric

■ Deploying a solution versus independent devices

Some of the operational advantages offered by ACI are:

■ End-to-end visibility reduces troubleshooting time of not only network infrastructure, but also for the virtual, compute, and application infrastructure

■ Increase in application performance reflects directly on improved customer experience and ease of use of business services

■ Flexibility, control, and customization

■ Highly secure and scalable multitenancy

■ Proactive problem resolution and faster troubleshooting

■ Simplified operations

■ Applications delivered in business time

Initial Design Considerations

Extension Considerations

There are several options for extending from the ACI fabric to traditional environments (that is, spanning tree protocol (STP), vPC, and FabricPath). An in-depth explanation for each is not provided, but rather a pros and cons list, and an overview about which option most customers choose to deploy in their networks today.

If you would like to review extension options in detail, review the CCO white paper “Connecting Application Centric Infrastructure (ACI) to Outside and 3 Networks” using the following link: http://www.cisco.com/c/en/us/solutions/collateral/data-center-virtualization/application-centric-infrastructure/white-paper-c07-732033.html.

This is the most popular option for extension to an ACI environment because via EPG is simple and straightforward. Users can extend an EPG beyond an ACI leaf by statically assigning a leaf port (along with a VLAN ID) to an EPG. After doing so, all the traffic received with the configured VLAN ID on this leaf port is mapped to the EPG and the configured policy for this EPG is enforced. The endpoints need not be directly connected to the ACI leaf port. They can be behind a network as long as the VLAN associated with the EPG is enabled within the network that connects the remote endpoint to the ACI fabric.

Consider the example in Figure 1 in which the following are shown:

1. Virtual workloads connected to a port group defined on an ACI-managed DVS and using VLAN tag 502 to send the traffic toward the ACI fabric.

2. Virtual workloads connected to a port group defined on a standard DVS not ACI-managed. From an ACI perspective, such endpoints are considered as belonging to a physical domain, hence static path bindings to a corresponding EPG is performed using VLAN tag 202.

3. Extension connectivity to an external network performed by creating a static path binding to a pair of Cisco Nexus 7000 switches using VLAN tag 202.

Note: The VLAN tags used for the static path bindings are only locally significant (on a per-interface basis), therefore there is no technical requirement to use the same 202 value shown in the following example.

Figure 1: L2Out via Static VLAN-EPG Mapping

· Easiest of the solutions

· Straightforward

· No contract enforcement between devices outside of the fabric (on the same broadcast domain) and devices inside the fabric, since they are all part of the same EPG202.

· STP TCNs, which are flooded from the Cisco Nexus 7000s, can result in traffic disruption as they cause the ACI fabric to flush the MAC address tables on the leaf nodes. There are mitigation steps to limit impact from this.

o Use vPC or double-sided vPC to connect ACI with STP environments.

o Enable the peer-switch feature with vPC (in the STP environment) that will eliminate root-bridge changes.

o Under the ACI BD configuration, enable flood mode for Layer 2 Unknown unicast packets. This will reduce the traffic disruption during an STP topology change.

This section explains connectivity via a bridge domain (BD). Instead of extending from an EPG on the fabric, it is taken up a level and performs the extension from the BD. This enables the insertion of policy (that is, whitelist/contract functionality) between devices outside of the fabric (on the same broadcast domain) and devices inside of the fabric. Additionally, the STP TCN issue is marginally improved, as the STP TCNs are not flooded in EPGs attached to the BDs (only to the L2Out EPG).


Figure 2: L2Out via BD

Pros: Contract enforcement is enabled between devices outside of the fabric (on the same broadcast domain) and devices inside the fabric.

Cons: More complex to deploy.

If you need policy enforcement (contract functionality) enabled between devices outside of the fabric (on the same broadcast domain) and devices inside the fabric, but you don’t want the added complexity of the L2Out via BD, then Connectivity via EPG – With Policy is the recommended solution.

In the following figure, you can see the blending of the L2Out via EPG and L2Out via BD. There is an “Outside” EPG, which has connectivity via static path bindings to Nexus 7000s on the outside. Additionally, the “Outside” EPG also has static path bindings to a standard VMware DVS. The definition of an Outside EPG allows you to provide a contract to control the policy (communication) between “internal” EPGs (AppOneWeb, AppTwoWeb, and AppThreeWeb) and endpoints connected to the Outside EPG.

Note: This is the approach adopted for the migration scenarios discussed in this document.

Figure 3: L2Out via EPG (with Policy Enforcement)


Pros: Allows the same Contract enforcement between devices outside of the fabric (on the same broadcast domain) and devices inside the fabric as the L2Out via BD, without the associated complexity.

Cons: Same STP TCN concerns as for the Layer 2 via EPG scenario.

While ACI does not run STP, nor participate in STP environments, it does pass STP bridge protocol data units (BPDUs) it receives to other devices in the same EPG. For this reason, it is very important that the design takes this principle into account.

The following figure shows the three mechanisms currently used in ACI for loop detection.

Figure 4: ACI Loop Detetction Mechanisms


· LLDP Loop Detection: if an ACI leaf node receives on an edge port an LLDP frame generated by another leaf node part of the same fabric, the edge port is disabled.

· Mis-Cabling Protocol (MCP) Loop Detection: new link-level protocol sending MCP frames on all VLANs on all edge ports. If any ACI leaf receives on an edge port an MCP frame generated by another leaf part of the same ACI fabric, the edge port gets error-disabled.

· STP Loop Detection: when connecting to an outside network, the ACI fabric floods the received STP BPDU frames within the boundary of the EPG (by using the VXLAN network identifier (VNID) assigned for the EPG when it encapsulates the BPDU in VXLAN format). External switches are expected to break any potential loop upon receiving the flooded BPDU from the ACI fabric.

Regarding the STP loop detection mechanism, in order for the external network to be able to detect a Layer 2 loop, it is important that the VLAN ID mapped to an EPG is kept consistent across different interfaces. This requires attention when extending connectivity outside the fabric by leveraging static EPG to VLAN mapping, as discussed in the “Layer-2 Extension Options” section.

Figure 5: ACI and STP Interaction


Figure 5 highlights what happens when that is not the case: the Layer 2 loop cannot be detected by the external Layer 2 switches, since a different VLAN tag is used on the two interfaces connecting them to the ACI fabric. Notice that this would not create an end-to-end Layer 2 loop in the data plane, but it may cause the Layer 2 switches to error-disable the interface that receives the BPDU for VLAN 10 on an interface configured as part of VLAN 20.

A couple of additional best practices when connecting an external Layer 2 network to the ACI fabric are captured as follows (and in the following diagrams):

· Never connect the same STP domain (Layer 2 network) to ACI fabric edge interfaces part of two different EPGs. Since the STP BPDUs are flooded inside the EPG, a Layer 2 loop created via the external Layer 2 domain cannot be detected in this case.

· Always ensure there is a single logical vPC connection between the ACI fabric and the external Layer 2 network domain.


Figure 6: STP and ACI Designs – STP Loop Free


In the example below, STP BPDUs are flooded inside of the EPG, not at the BD level. The Cisco Nexus 7000s will see BPDUs through the ACI fabric. As long as the devices in the STP environment see the appropriate BPDUs, they will forward and block appropriately.


Figure 7: STP and ACI Designs – STP Loop Free with vPC

In the example below, STP BPDUs are flooded inside of the EPG, not at the BD level. However, because there are no physical loops connecting up to the ACI fabric, there is no chance of an STP loop.


Although the ACI fabric control plan doesn’t run STP, it does intercept the STP TCN frame. Why would ACI care about STP TCN frames if it doesn’t run STP? ACI uses the TCNs to flush out MAC address entries, which helps avoiding the “black-holing” of traffic after an STP topology change on the outside network. Upon receiving an STP BPDU TCN frame, the APIC flushes the MAC addresses for the corresponding EPG that experienced the STP topology change. This does have an impact on the choice of how the ACI fabric forwards unknown unicast. By default, the ACI leaf forwards the unknown unicast traffic to a spine proxy for further lookup. The spine node will drop the packet if the MAC address doesn’t exist in the proxy database. This option is called “Hardware Proxy,” and it is the default option. The other unknown unicast configuration option is “flood mode”, which causes the bridge domain (BD) to operate like a standard switch. When the “Hardware Proxy” option is selected and the fabric detects an STP topology change, the MAC addresses for the EPG are flushed in the fabric. Communication is impacted until the endpoints MAC addresses are learned again.

There are several ways to limit the impact of STP TCNs with ACI. When connecting ACI to external STP domains, use the following best practices:

1. Use vPC or double-sided vPC to connect ACI with STP environments to ensure no Layer 2-looped topology can be created between those networks.

2. Recommend to enable the peer-switch feature with vPC (in the Brownfield STP environment) that will eliminate root-bridge changes.

3. Under the ACI bridge domain (BD) configuration, enable flood mode for Layer 2 unknown unicast packets. This will reduce the traffic disruption during an STP topology change.

BPDU frames for Per-VLAN Spanning Tree (PVST) and Rapid Per-VLAN Spanning Tree (RPVST) carry a VLAN tag. The ACI leaf can identify which EPG the BPDU should be flooded in based on the VLAN tag in the frame. In MST (802.1s) deployments, however, BPDU frames are sent untagged over the native VLAN. Because of these factors, additional configuration is required in the ACI fabric in order for Multiple Spanning Tree (MST) BPDUs to be properly flooded.

By default, there is no native VLAN configured for ACI. Additionally, the native VLAN is not generally used to carry data traffic. To accept traffic for any VLAN, the VLAN must be provisioned by a statically assigned port and a VLAN to an EPG. As a result, to ensure MST BPDUs are flooded to the desired ports, the user must create a specific EPG to carry those BPDUs. As shown in the following diagram, the mode must be “native” given that the BPDU frame is untagged.

Figure 8: Assign Port to an EPG Using Native Mode


In addition to the configuration tasks previously described, the user must also create a physical domain and associated VLAN pool (which includes VLAN 1 in this example), and the attachable access entity profile to allow VLAN 1 to be used for these ports.

Layer 3 Considerations

When using routed interfaces, routed subinterfaces, or SVI interfaces for ACI, be aware that the ACI fabric defaults to a system MTU of 9000 for all interfaces (Layer 2 and Layer 3). This means that it is critical to ensure that the MTU configuration on the corresponding external router interfaces matches this value. Failure to do so will lead to suboptimal fragmentation or routing protocol adjacency failures or could lead to packet loss. For example, if devices inside the fabric, that is, VMs, are configured to use jumbo MTUs, and the L3Out is configured for an MTU of 1500, the fabric will drop the packets on egress.

As an example, on external Cisco Nexus 7000 devices, this means you have to configure the following:

· Set the system jumboMTU to 9216 globally: DCCORE01(config)# system jumbo 9216

· Configure the Layer 2 interfaces (Layer 2 port-channels, Layer 2 trunks, and Layer 2 access ports) to the system jumboMTU value: DCCORE01(config-if)# mtu 9216

· Configure the Layer 3 interfaces (SVIs, routed sub-interfaces, and Layer 3 routed interfaces) to match the ACI MTU of 9000: DCCORE01(config-if)# mtu 9000

Note: Enable Jumbo MTUs throughout the datacenter, and use a routing platform (that is, ASR 1000, ASR 9000, and so on) to step down your MTU from jumbo to a standard MTU of 1500. Do not use a switching platform to perform MTU translation.

When using SVI-based interfaces for external connectivity, it is mandatory that you define and associate an external routed domain to the L3Out connection. This is because incoming and outgoing traffic must be using the specified VLAN tag so that the corresponding SVI interfaces can be enabled. The available VLAN tags are configured in the VLAN pool associated to the External Routed Domain, which will be discussed in more detail in “External Routed Domain” section.

Figure 9: Associate an External Layer 3 Domain to an L3Out Connection


Management Out-of-Band and In-Band

· Contracts are always needed to permit traffic to the out-of-band (OOB) interfaces of the leaf and spine switches. Don’t forget to configure your contract in the “mgmt” tenant.

· Tenant  à Tenant mgmt à Node Management EPGs, click Out-Of-Band EPG, and select “default”.

· The default contract acts as a permit ip any.

Figure 10: Create a Contract to Access the OOB Mgmt Network


Note: If you install both a default route via OOB and via inband, the inband path is preferred over OOB.

Figure 11: Preferred Inband Default Route


Migration Strategy

The process described in this guide is referred to as “network-centric migration”, and consists of interconnecting the existing Brownfield datacenter network (usually built on STP, vPC or in this case, FabricPath) to a new deployment of ACI, with the goal of migrating workloads from the old environment to the new environment.

In order to accomplish the migration, you must map traditional network concepts (VLANs, IP subnets, VRFs, etc.) to new ACI constructs, like endpoint groups (EPGs), bridge domains (BDs), and private networks (ACI constructs will be discussed later in this document).

The following diagram shows the ACI network-centric migration methodology, which highlights the major steps required for performing the migration of applications from a Brownfield datacenter network to an ACI fabric.

Figure 12: ACI Network Centric Migration Methodology


The steps of the ACI network-centric migration are described as follows:

1. Deployment –The first step is the design and deployment of the new ACI POD; it is likely that the size of such a deployment is initially small, with plans to grow it over time. A typical ACI POD consists of at least two spines and two leaf switches, which are managed by a cluster of at least three APIC controllers.

2. Integration – The second step is the integration between the existing DC network infrastructure (usually called the Brownfield network) and the new ACI POD. Layer 2 and Layer 3 connectivity between the two networks is required to allow successful workload migration from the Brownfield network to the new ACI infrastructure.

3. Migration – The final step consists of migrating workloads between the Brownfield network and the ACI POD. It is likely that this application migration process may take several months to complete. During this time, a Layer 2 and Layer 3 interconnect between the Brownfield environment and ACI infrastructure remains in place to allow communication to occur until the migration is complete.

The following sections discuss in great detail those required steps, focusing on specific migration use cases.

· Note: This design guide is the companion document for the “Migrating Existing Networks to Cisco ACI” document.

Connectivity With VLAN to EPG Static Mappings

In ACI, VLANs do not exist inside the fabric; they are only defined on the edge ports connecting the virtual or physical endpoints. This means that the VLAN tags are localized on a per-interface basis. This method allows the possibility of establishing intra-IP subnet communication between devices, which are a part of segments identified by different 802.1q encapsulation tags (different VLAN numbers), or even another type of tag altogether, such as VXLAN or NVGRE encapsulation.

The following diagram shows the ACI normalization of ingress encapsulation, which demonstrates the fabric normalization of the port encapsulation.


Figure 13: Fabric Normalization of Port Encapsulation


The traditional concept of a VLAN as a Layer 2 broadcast domain is replaced in the ACI fabric with a BD. The BD represents the Layer 2 broadcast domain where endpoints (either virtual or physical) are connected.

To better demonstrate this concept, consider the following diagram, which illustrates that it is possible to associate different VLAN tags (VLAN 20 and 30, in this example), which are configured on different edge ports to the same IP broadcast domain. The result is that endpoint 10.10.10.10 will still be able to communicate with endpoint 10.10.10.11, even though they are attached to different local VLANs.

Figure 14: ACI Local VLAN Significance


To connect the Brownfield network and the ACI fabric via Layer 2, perform the workload migration:

1. Establish a double-sided vPC connection between a pair of ACI border leaf nodes and the two devices representing the boundary between Layer 2/Layer 3 in the Brownfield data center network. Depending on the Layer 2 technology used in the Brownfield network (i.e., STP, vPC or FabricPath), this Layer 2/Layer 3 boundary may be found at the aggregation layer or on a dedicated pair of devices called border leaf nodes. The following diagram shows the Brownfield network connected to the ACI fabric. The use of dedicated border leaf swithes for Layer 2/Layer 3 connectivity is recommended, but not required. It is worth noting that at the time of writing this document, up to 12K endpoints can be supported on the Brownfield network if they need to communicate at Layer 2 with the ACI fabric. This is because of the size of the hardware table available on a given pair of border leaf nodes to learn the MAC and IP addresses of those endpoints (on a Layer 2 interface). In scenarios where it is required to support a higher number of external endpoints, it is possible to deploy different pairs of border leaf nodes and spread among them the Layer 2 VLANs connecting to the Brownfield network domain. Always ensure that you are within the verified scalability numbers for endpoints, especially when attaching Brownfield environments to ACI. For more information, refer to the following document: http://www.cisco.com/c/en/us/td/docs/switches/datacenter/aci/apic/sw/1-x/verified-scalability/b_Verified_Scalability_Release_1_1_2h.html

Figure 15: Layer 2 Interconnection between the FP and ACI Networks


In this example, the Brownfield network is represented by a FabricPath implementation. The FabricPath spine layer not only serves as the Layer 2/Layer 3 boundary for the environment, but will also serve as the connection point to the ACI environment. A double-sided vPC+ connection to a pair of ACI border leaf nodes allows the extension of Layer 2 connectivity between the two network infrastructures without introducing any Layer 2 loop in the topology. This allows all vPC links to actively forward traffic on all paths.

Note: This design would look identical if the Brownfield network was built with STP or vPC as opposed to FabricPath.

2. Associate endpoints connected to VLANs in the Brownfield network to Endpoint Groups (EPGs), which are defined inside of the ACI fabric. As previously mentioned, the recommended approach discussed in this document consists of statically mapping VLAN tags to EPGs on the ACI leaf nodes. When doing so, there are a couple of scenarios to explore, which are discussed in detail in the following two sections.

Scenario 1: Mapping a VLAN to Multiple EPGs

In this scenario, the customer has a single VLAN deployed in the FabricPath network, which supports multiple applications. Due to compliance regulations, the customer intends to segregate the application workloads on VLAN 100 based on application type, but cannot change IP addresses on any of their application servers.

The goal is to migrate the application workloads from the FabricPath environment to the new ACI fabric, where you can take advantage of the security functionalities offered by ACI to logically isolate the application workloads into groups of endpoints, based on application type.

Figure 16: FabricPath to ACI Migration (Scenario 1)


The figure above shows the current application deployment for VLAN 100. All application servers reside in a single FaricPath VLAN.

As shown in the following diagram, the current environment is a FabricPath datacenter, with Cisco Nexus 7000 devices serving as the aggregation/spine devices and providing Layer 2/Layer 3 services. The access layer devices are Cisco Nexus 5648 switches (Layer 2 only). For Scenario 1, there is one routing table configured in the FabricPath environment (which is the default routing table) and one VLAN configured (VLAN 100).

Figure 17: Current DC Based on FabricPath


From the FabricPath Core switches, OSPF is running to the Data center core routers (DCCORE01/02).

For the Compute layer, there are two ESXi hosts being managed by vSphere 5.5, with a traditional DVS. For Scenario 1, all three of the VM hosts reside in portgroup VLAN100 on the VMware-managed DVS (Distributed Virtual Switch). Because all three VM hosts reside in the same VLAN, they have full communication to each other, even though they are supporting different applications.

1. Servers in VLAN 100 are divided into three logical groupings, based upon the application they support. The application grouping is as follows:

a. AppOneWeb

b. AppTwoWeb

c. AppThreeWeb

2. Workloads belonging to the same application can communicate with other servers that support the same application.

3. Servers in one application group cannot communicate with other servers supporting a different application.

4. Communication must be maintained between servers in the legacy FabricPath DC infrastructure and the new ACI DC infrastructure during migration.

5. IP addressing must be maintained unaltered for all servers.

The following diagram shows the desired end state of the ACI fabric. VLAN 100 is subdivided into three endpoint groups: AppOneWeb, AppTwoWeb, and AppThreeWeb, respectively. While these three “groups” will share a common IP subnet, inter-EPG communication is restricted because of the default “white listing” type of policy implemented inside the ACI fabric.

Additionally, you will create a fourth EPG (not shown in the following diagram), called “Outside”, which will be used during the migration between the FabricPath environment and the ACI environment. From an ACI fabric perspective, all the endpoints still connected to the Brownfield network are grouped as part of this outside EPG. Finally, an L3Out connection is utilized to interconnect the ACI fabric to the external Layer 3 network domain.

Figure 18: ACI Fabric at the End of the Implementation


Scenario 2: Mapping VLANs to EPGs (1:1)

In this scenario, the customer has a classic, multitiered application (Web/App/DB), in which each tier of the application is located in its own dedicated VLAN. The customer is performing a network-centric migration of their VLAN into ACI; this implies that each Brownfield VLAN is related to an EPG and a BD in the ACI fabric (VLAN = EPG = BD). By performing static mappings of the VLANs to EPGs, the customer ensures that their workloads, which are connected to the Brownfield FabricPath network, remain a part of the same Layer 2 broadcast domain with the workloads inside of the ACI fabric.


Figure 19: FabricPath to ACI Migration (Scenario 2)


As shown in Figure 17, servers are already deployed into three logical groupings in the FabricPath network, based upon the application (or application tier) they support. The application grouping is as follows:

a. AppOneWeb workloads are part of FP VLAN 210.

b. AppTwoWeb workloads are part of FP VLAN 211.

c. AppThreeWeb workloads are part of FP VLAN 212.


Figure 20: Current DC Based on FabricPath (with FWs)


Those three VLANs are then mapped to different VRF instances (VRF210, VRF211, and VRF212, respectively) to maintain logical isolation between tenants also across the Layer 3 domain.


In this specific scenario, a pair of active/standby firewalls has also been added at the perimeter of the FabricPath network. Each tenant (VRF) connects to a separate FW interface, so that security policies can be enforced to control inter-tenant communication and north-to-south traffic flows between each tenant and the DC Layer 3 core.

For each VRF, a static default route is configured on the FabricPath core switches, pointing to the ASA firewall. The ASA firewall has specific static routes configured to route between FabricPath VRFs, as well as routes to get to devices outside of the data center by routing to the data center core routers (DCCORE01/02). This means that all communication between different VMs has to flow up through the FabricPath environment and out to the firewall before it can talk with another VM of a different VRF.

For the compute layer, there are two ESXi hosts being managed by vSphere 5.5, with a traditional DVS. For Scenario 2, all three of the VM hosts reside in different port groups on the VMware-managed DVS (Distributed Virtual Switch).

1. Use network-centric deployment mode for ACI (VLAN = EPG = BD).

2. Servers of the same security zone (i.e., VRF) can communicate freely.

3. Server communication between different security zones (i.e., different VRFs) must pass through a stateful firewall for inspection.

4. Communication must be maintained for servers in the legacy FabricPath DC infrastructure and the new ACI DC infrastructure during migration.

5. IP addressing must be maintained for all servers.


Figure 21: Current DC Based on FabricPath


As shown in the following diagram, the end state for the ACI deployment will closely mimic the FabricPath environment. The ACI fabric will have one tenant, three VRFs, and three BD/EPGs that map in a 1:1 fashion to VLANs defined in the FabricPath environment.

Figure 22: ACI Fabric at the End of the Implementation


From a compute perspective, the end goal will be to move all of the VMs from the vCenter-managed VDS to an ACI-managed DVS. The VMM integration with the ACI-managed DVS allows for the automatic configuration of EPG-based port groups. This allows VMware administrators to then manage VMNIC setting to control endpoint placement within the ACI fabric.

Infrastructure Deployment Considerations

FabricPath-Enabled Data Center

The FabricPath-enabled data center represents the Brownfield network infrastructure and consists of a traditional FabricPath deployment with a pair of Cisco Nexus 7000 and a pair of Cisco Nexus 5000. The pair of Cisco Nexus 7000 are deployed with a VDC infrastructure to allow them to provide a dual FabricPath core as well as a dual Layer 3 DC core.

Note: For more information about FabricPath and relative deployment best practices, refer to the following documents:

· http://www.cisco.com/c/en/us/products/collateral/switches/nexus-5000-series-switches/guide_c07-690079.html

· http://www.cisco.com/c/dam/en/us/products/collateral/switches/nexus-7000-series-switches/white_paper_c07-728188.pdf

VDC (1) and VDC (2) provide DC core functionality and connect to VDC (3) and VDC (4) representing the FabricPath core.

Figure 23: FabricPath Data Center


The Data Center core consists of a VDC within each of the Cisco Nexus 7000.


Figure 24: Cisco Nexus 7000 VDCs

Nexus7K-01


N7K1# show vdc


Switchwide mode is m1 f1 m1xl f2 m2xl f2e f3


vdc_id vdc_name state mac

type lc

------ -------- ----- ----------

--------- ------

1 N7K1 active 38:ed:18:a2:f1:41

Admin None

2 FP_Core01 active 38:ed:18:a2:f1:42

Ethernet f2e

3 DC_CORE1 active 38:ed:18:a2:f1:43

Ethernet f2e


N7K1#


Nexus7K-02


N7K2# show vdc


Switchwide mode is m1 f1 m1xl f2 m2xl f2e f3


vdc_id vdc_name state mac

type lc

------ -------- ----- ----------

--------- ------

1 N7K2 active 38:ed:18:a2:f3:c1

Admin None

2 FP_Core02 active 38:ed:18:a2:f3:c2

Ethernet f2e

3 DC_CORE2 active 38:ed:18:a2:f3:c3

Ethernet f2e


N7K2#


The DC core is attached to the FabricPath core via a full mesh of 10-G point-to-point connections. Routes are exchanged between the DC core and the FabricPath core using OSPF as routing protocol.

The DC core devices are then interconnected via two 10-G connections and provide the vPC peer-link functionality. This is required since a vPC connection is leveraged to connect to the ACI fabric and establish Layer 3 connectivity.

The FabricPath core consists of a VDC within each of the Cisco Nexus 7000.

The FP core devices are connected to the DC core routers via a full mesh of 10-G point-to-point connections. Routes are exchanged between the FP core and the DC core using OSPF.

The FP spines are then interconnected via two 10-G connections that provide the vPC peerlink functionality. The vPCs are then created to connect to the ACI fabric for Layer 2 connectivity.

The FabricPath core VDCs provide the spine functionality and are connected to a pair of Cisco Nexus 5000 leaf switches. The Cisco Nexus 5000 leaf switches are then connected via vPC to the UCS fabric Interconnects (FIs).

Figure 25: Cisco Nexus 7000 FabricPath Configuration


FP_Core01# sh run fabricpath


!Command: show running-config fabricpath

!Time: Mon Oct 5 07:01:44 2015


version 6.2(12)

feature-set fabricpath


vlan 5,10-212,600,610-612

mode fabricpath

fabricpath switch-id 201

vpc domain 10

fabricpath switch-id 200


interface port-channel1

switchport mode fabricpath


interface Ethernet3/1

switchport mode fabricpath


interface Ethernet3/2

switchport mode fabricpath


interface Ethernet3/3

switchport mode fabricpath


interface Ethernet3/4

switchport mode fabricpath


interface Ethernet4/1

switchport mode fabricpath


interface Ethernet4/2

switchport mode fabricpath


interface Ethernet4/3

switchport mode fabricpath


interface Ethernet4/4

switchport mode fabricpath

fabricpath domain default

spf-interval 50 50 50

lsp-gen-interval 50 50 50

root-priority 101


FP_Core01#


FP_Core02# show running-config fabricpath


!Command: show running-config fabricpath

!Time: Mon Oct 5 07:03:27 2015


version 6.2(12)

feature-set fabricpath


vlan 5,10-12,100,210-212,600,610-612

mode fabricpath

fabricpath switch-id 202

vpc domain 10

fabricpath switch-id 200


interface port-channel1

switchport mode fabricpath


interface Ethernet3/1

switchport mode fabricpath


interface Ethernet3/2

switchport mode fabricpath


interface Ethernet3/3

switchport mode fabricpath


interface Ethernet3/4

switchport mode fabricpath


interface Ethernet4/1

switchport mode fabricpath


interface Ethernet4/2

switchport mode fabricpath


interface Ethernet4/3

switchport mode fabricpath


interface Ethernet4/4

switchport mode fabricpath

fabricpath domain default

spf-interval 50 50 50

lsp-gen-interval 50 50 50

root-priority 100


FP_Core02#


The FabricPath core includes a pair of ASA firewalls for inter-VRF routing within Scenario 2. The FWs are connected to the FP core and to the DC core and are participating in static routing for reachability.

Figure 26: FabricPath Data Center Services


Management

Out-of-band (OOB) management access is used for all devices within the validated topology. This includes the ACI Spine/Leaf switches, the APIC controller cluster, the Cisco Nexus 7000 switches and Cisco Nexus 5600 switches used for FabricPath, the ASA firewalls and the UCS chassis.

Note: Although OOB management was used for the purpose of this migration, in-band management is also a valid design option.

To configure the node management address, log in to the APIC GUI with administrator privileges and follow the path below:

Tenant à [mgmt] à Node Management Addresses

Figure 27: Management - ACI Fabric OOB


Virtual Environment

The virtual environment represents the topology and configuration required to support the distribution of workload in the FabricPath and ACI environments. For the migration, the virtual environment within the FabricPath domain remains the constant while a second virtual environment is deployed to eventually support the workload within the ACI fabric.

Note: Depending on the use case and the requirements, there may not be a requirement for a second virtual environment in the ACI fabric. The use of the second virtual environment within the ACI migration presented in this document is based on the overall assumption that the new data center fabric will also deploy new network and compute devices that would require to be connected to an ACI managed virtual environment.

The configuration discussed as follows pertains to both the existing FabricPath and the new ACI virtual environments. For each network domain the connected virtual environment consists of a UCSB-Mini with integrated fabric interconnects. Each of the fabric interconnects has a 10-G connection to a pair of FabricPath or ACI leaf nodes. A port channel on each of the fabric interconnects is connected to a vPC on the FabricPath/ACI domains.

Note: Depending on the use case and requirements, there is no ACI requirement for using a Cisco UCSB compute node. Based on the current environment, the migration discussed throughout the document is capable of supporting other compute nodes.

vNIC DATA_A and vNIC DATA_B have been defined and extended to the ESXi host deployed on the FabricPath-attached compute node. The allowed VLANs include 100, 210, 211 and 212. VLAN 100 as defined in the FabricPath domain supports the VMs associated with Scenario 1 and VLANs 210-212 support the VMs associated with Scenario 2. The VLAN range will be used to create the DVS portgroup mapping for the VM NIC connectivity.

Refer to the following diagram for the vNIC and VLAN usage within the FabricPath-attached UCS chassis for Scenario 1 and Scenario 2.

Figure 28: FabricPath Compute Node Data vNIC


vNIC MGMT_A and vNIC MGMT_B have been defined and extended to the ESXi hosts deployed on the FabricPath-attached compute node. The allowed VLAN includes VLAN 144 and is defined in the out-of-band (OOB) management infrastructure located within the test environment.

Refer to the following diagram for the vNIC and VLAN usage within the FabricPath-attached UCS chassis associated with the OOB management access.

Figure 29: FabricPath Compute Node Management vNIC


In support of the connectivity between the FabricPath domain and the FI, the UCS LAN network control policy is set for CDP Enabled. This allows the FI and the FabricPath network to exchange CDP neighborship messages.

The compute resources that are connected to the ACI fabric leverage the same vNICs (data and management) previously presented for the compute nodes connected to the FabricPath network. Another pair of Data vNICs must be defined, since the goal is to connect those ESXi hosts to both a vCenter-managed DVS and an ACI-managed DVS. This is discussed in more detail in the “VMware” section.

As with the FabricPath-attached compute nodes, vNIC DATA_A and vNIC DATA_B have been defined and extended to the ESXi hosts deployed on the ACI-fabric-attached compute nodes. The allowed VLANs include 100, 210, 211 and 212. VLAN 100 supports the VMs associated with Scenario 1 and VLANs 210-212 support the VMs associated with Scenario 2.

As with the FabricPath-attached compute nodes, vNIC MGMT_A and vNIC MGMT_B have been defined and extended to the ESXi hosts deployed on the ACI-attached compute nodes. The allowed VLAN includes VLAN 144 and is defined in the out-of-band management infrastructure located within the test bed.

vNIC DATA_A_ACI and vNIC DATA_B_ACI have been defined and extended to the ESXi host deployed on the ACI-fabric-attached compute node. The allowed VLANs include 1001-1011. VLANs 1001-1010 as defined in the ACI fabric support the VMs associated with Scenario 1 and Scenario 2. The APIC will use a VLAN from the range and associate it to each DVS port group dynamically created as a result of the association of an EPG to the VMM domain. The ACI fabric will hence receive traffic sourced by VMs connected to those port groups tagged with those specific VLAN values.

Figure 30: ACI Fabric Compute Node Data vNIC


In support of the connectivity between the ACI domain and the FI, the UCS LAN network control policy is set for CDP enabled. This allows both the FI and the ACI leaf nodes to exchange CDP neighborship messages.

VMWare ESXi is the validated hypervisor deployed on the compute nodes to facilitate the virtual environments.

To support the initial step of the migration strategy, each ESXi host (on both the FP and ACI sides) is connected to a vCenter-managed DVS switch. The DVS switch leverages port groups associated to the FabricPath data VLANs used for Scenario 1 and Scenario 2 respectively: 100, 210, 211 and 212 (see the following diagram).

Figure 31: vCenter Managed DVS


The final step of the application migration consists of connecting the virtual machines to the ACI-managed DVS that is dynamically created in vCenter after the creation of the VMM Domain (this will be discussed in the “ACI VM Networking” section). The DVS switch will leverage dynamically created port groups associated to ACI internal EPGs and leveraging a set of VLAN tags in the range 1001-1010 (see the following diagram).

Figure 32: ACI-Managed DVS


The data vNICs exposed via the UCS configuration are configured as part of the vCenter-managed DVS uplinks, as shown in the following figure.

Figure 33: VMware Uplinks for vCenter-Managed DVS


ESXi hosts connected to both the FabricPath and ACI networks are attached to the vSphere-managed DVS, since the first step of the application migration consists of performing a live vMotion of virtual machines across these two ESXi hosts.

The ACI Data vNICs exposed via the UCS configuration are configured as part of the ACI-managed DVS uplinks, as shown in the following diagram.

Figure 34: VMware Uplinks for ACI-Managed DVS


Only the ESXi hosts connected to the ACI fabric have uplinks connected to the ACI-managed DVS.

Cisco Discovery Protocol (CDP) is enabled within the vCenter to accommodate the neighborship message exchange from the ESXi host to the ACI fabric. The CDP messages from the VM to the FI and from the FI to the ACI leaf switches are required to properly exchange host details for dynamically created port groups.

Note: ACI has the capability to support user-defined port groups and avoid the requirement for CDP support. VM integration with UCS-B Series and ACI requires specific configurations. Refer to the following for more details: http://www.cisco.com/c/en/us/support/docs/cloud-systems-management/application-policy-infrastructure-controller-apic/118965-config-vmm-aci-ucs-00.html

ACI Infrastructure Deployment

Within this section, you will accomplish the following:

1. Bring the APIC controllers online.

2. Initialize the fabric.

3. Configure the fabric resources for network connectivity.

Each step previously listed is executed once along with the Fabric Configuration section being used in both Scenario (1) and Scenario (2).

The Infrastructure Deployment section is the foundation to the ACI fabric configuration. The APIC Controller section provides the details required to bring the APIC cluster online while the Fabric Initialization section initiates the process for bringing the Spine/Leaf topology online and to configure the infrastructure. Once the APIC and the Spine/Leaf topology is accessible, the Fabric Configuration section provides the necessary details for connecting the ACI fabric to the virtual environment, the FabricPath domain, the data center core and the firewall services layer.

At the Deployment Phase, you are going to configure the ACI fabric and stage it in preparation for integration with the existing FabricPath environment.

Figure 35: Deployment Phase


APIC controllers are a set of three servers connected at different points in the ACI fabric. Each APIC server connects to a pair of leaf nodes, with 10-G ports, in active/standby configuration. The 1-G management connections are wired to an out-of-band (OOB) switched network for device access.


In the design discussed herein, the three APICs are each physically connected to the same pair of leaf nodes. With KVM access to each controller, the following table provides the configuration parameters required to initialize the controller nodes. APIC 1 through 3 should be configured sequentially with the appropriate controller details.


Note: In production environments, the recommendation is to connect each APIC node to a different pair of ACI leaf nodes.

Table 1: APIC Controller Initialization

FabricPathCore

ACI Fabric

Notes

Fabric Name

ACI_Fabric

The fabric name specified must be the same for each controller

Number of Controllers

3

Specifies the total number of controllers within the cluster

Controller ID

1

Specifies the instance of the controller

Controller Name

apic1/apic2/apic3

Specifies the individual controller name

TEP Address Pool

10.0.0.0/16

Specifies the address range for VXLAN TEP address pool

NOTE: Minimum TEP Address pool is /23. This address range should not overlap with any allocated address space.

Infra VLAN ID

3967

Specifies the infrastructure VLAN ID that will be extended to the controllers for infrastructure communications

NOTE: 3967 is the recommended Infra VLAN ID because this VLAN is not reserved on any Cisco platform, should it need to be extended outside of the ACI fabric (that is, for extending connectivity to the Cisco AVS virtual switch)

Multicast Address Pool

225.0.0.0/15

Specifies the multicast address pool for use within the fabric

APIC IP Address/Mask

10.10.10.10/.11/.12

Specifies the management IP address and mask for the specified controller

Default Gateway

10.10.10.1

Specifies the default gateway for the controller

APIC Password

********

Specifies the administrator password for the fabric

The initial KVM screen output for the controller is noted in the following diagram. After each input, hitting the enter key takes you to the next parameter. Following the last configuration parameter, the output gives you the opportunity to accept the configuration or start the configuration script over.

Note: The Infra VLAN cannot be changed once the startup script has been completed. Changing the Infra VLAN requires initiating the startup script.


Using the parameters in the table above, complete the APIC startup screen for the first APIC then move to the second and third APIC node respectively.

Figure 36: APIC Controller Initialization


The Fabric Initialization process is the ability to register the given spine/leaf topology. Each node part of the desired infrastructure must be accepted into the topology for connectivity to the overall environment, which makes up the ACI fabric. Once the administrator accepts the node into the fabric and assigns a node ID and name, the APIC then provisions the logical connectivity. Each node is identified within the controller by the platform-specific serial number and will appear in the APIC-generated topology once the configuration is complete.

To initiate fabric Initialization, log in to the APIC GUI with administrator privileges and follow the path below:

Fabric à Inventory à Fabric Membership

Figure 37: Fabric Initialization


This section contains the fabric configuration parameters for resources used throughout the fabric.

Route Reflector Policy

The MP-BGP route reflector policy is used to specify two parameters:

· The AS number assigned to the ACI fabric (65000 in the following example).

· The devices in the ACI fabric deployed as MP-BGP route-reflectors (Spine1 and Spine 2 in the example).

It is worth recalling that MP-BGP is the control plane used inside the ACI fabric to communicate to the ACI leaf nodes external IP prefixes information learned on the border leaf nodes via the L3Out connection.

Fabric à Fabric Policies à Pod Policies à Policies à [BGP Route Reflector default]

Figure 38: Route Reflector Policy


Aside from the GUI representation of the required parameters, the ACI API can also be used to provide access to the configuration and management of the infrastructure. The XML format of the above configuration can be applied utilizing multiple techniques. See the following Cisco APIC REST API User Guide for additional details:

http://www.cisco.com/c/en/us/td/docs/switches/datacenter/aci/apic/sw/1-x/api/rest/b_APIC_RESTful_API_User_Guide.html

Note: The XML formats required to perform the same configuration steps shown on the APIC GUI will be shown throughout the document, starting with the following BGP RR Policy.

XML 1: Route Reflector Policy


<!—Route Reflector -->

<bgpInstPol descr="" name="default" >


<!—Route Reflector Nodes-->

<bgpRRP descr="" name="">

<bgpRRNodePEp descr="" id="202"/>

<bgpRRNodePEp descr="" id="201"/>

</bgpRRP>


<!—Route Reflector ASN -->

<bgpAsP asn="65000" descr="" name=""/>

</bgpInstPol>


The Access Policy section pertains to the resources required to provide the physical connectivity within the desired topology. The parameters within this section are configured once and referenced in the vPC Scenario (1) and Scenario (2) sections to provide network connectivity.

The following policies are used to create each vPC within the topology:

1. Configure an Attachable Access Entity Profile.

2. Configure an Interface Profile.

3. Configure Interface Policies

a. Configure a Link Level Policy

b. Configure a CDP Policy

c. Configure an LLDP Policy

d. Configure a Port Channel Policy

4. Configure a Switch Profile

5. vPC Policy

The figure below is a representation of the overall fabric resources and how they relate to each other.

Figure 39: Access Policy Model


The vPC domain allows for the creation of a user-specified vPC domain to identify the leaf switches participating in the specified vPC while the interface and the switch profiles specify the desired switch and interface polices the vPC domain will be created within.

Each vPC configuration (Interface Policy Group) references physical attributes such as link/speed and port channel behavior policies. The individual interface policies are defined and referenced for each subsequent vPC configuration.

Note: The use of the Quick Start guide is not used in order to demonstrate the object relationship for the configuration parameters. Additionally, while Quick Start menus can change from version to version, the method of configuration displayed in this whitepaper will remain valid.

The Attachable Access Entity Profile provides a template for attachment point between the switch and interface profiles and the fabric resources such as the VLAN pool. The AEP can be considered the ‘glue’ between the defined physical, virtual or Layer 2 / Layer 3 domains and the fabric interfaces (logical or physical), essentially allowing to specify what VLAN tags can be used on those interfaces.


Note: Although a single AEP is used to support both scenarios, multiple AEPs can be deployed providing further granularity in defining connectivity.

To configure an attachable access entity profile, login to the APIC GUI with administrator privileges and follow the path below:

Fabric à Access Policies à Global Policies à Attachable Access Entity Profiles à [AAEP]

Figure 40: Attachable Access Entity Profile


XML 2: Attachable Access Entity Profile


<!—Attachable Access Entity Profile -->

<infraAttEntityP descr="" name="AAEP" >

</infraAttEntityP>


Interface Profile

The Interface Profile enables you to define a set of “interface selectors” each specifying a block of physical interfaces. Each selector uses the properties identified under the corresponding Interface Policy Group, as shown in following diagram. The policy names used in the following example reflect the policy type name and the node(s). A single interface profile will be used for each switch and switch combination used in the topology.


Note: Depending on the use case, multiple combinations of interface profiles can be deployed.


To configure an interface profile, log in to the APIC GUI with administrator privileges and follow the path below:

Fabric à Access Policies à Interface Policies à Profiles à [intProfile_Leaf1_2]

Figure 41: Interface Profile


XML 3: Interface Profile


<!—Interface Profile -->

<infraAccPortP descr="" name="intProfile_Leaf1_2"/>

</infraAccPortP>


Repeat the process for the remaining interface profiles:

Fabric à Access Policies à Interface Policies à Profiles à [intProfile_Leaf1]

Fabric à Access Policies à Interface Policies à Profiles à [intProfile_Leaf2]

Interface Policies

The ACI Interface Policies specify interface properties for fabric connectivity. The interface policies are used for all physical connections and are reflective of the physical device connected.

Link Level Policy

The link level policy specifies the Layer 1 parameters of host-facing ports. The policy contains the interface-specific details such as auto-negotiation, speed, and debounce interval. The policy names used as follows reflect the speed and negotiation abilities.

To configure a link level policy, log in to the APIC GUI with administrator privileges and follow the path below:

Fabric à Access Policies à Interface Policies à Policies à Link Level

Figure 42: Interface Policy – Link Level Policy


XML 4: Interface Policy - Link Level Policy


<!—1 GIG Auto Policy -->

<fabricHIfPol autoNeg="on" descr="" linkDebounce="100" name="1GigAuto" speed="1G"/>


<!—10 GIG Auto Policy -->

<fabricHIfPol autoNeg="on" descr="" linkDebounce="100" name="10GigAuto" speed="10G"/>


The CDP interface policy is primarily used to obtain protocol addresses of neighboring devices and discover the platform of those devices. The policy names used as follows reflect the policy type and admin state.


To configure a CDP policy, log in to the APIC GUI with administrator privileges and follow the path below:

Fabric à Access Policies à Interface Policies à Policies à CDP Interface


Figure 43: Interface Policy – CDP Policy


XML 5: Interface Policy - CDP Policy


<!—CDP Off Policy -->

<cdpIfPol adminSt="disabled" descr="" name="CDP_OFF" />


<!—CDP On Policy -->

<cdpIfPol adminSt="enabled" descr="" name="CDP_ON" />


The LLDP interface policy defines a common configuration that applies to one or more LLDP interfaces. LLDP uses the logical link control (LLC) services to transmit and receive information to and from other LLDP agents. The policy names used as follows reflect the policy type and admin state.

To configure an LLDP policy, login to the APIC GUI with administrator privileges and follow the path below:

Fabric à Access Policies à Interface Policies à Policies à LLDP Interface

Figure 44: Interface Policy – LLDP Policy


XML 6: Interface Policy - LLDP Policy


<!—LLDP Off Policy -->

<lldpIfPol adminRxSt="disabled" adminTxSt="disabled" descr="" name="LLDP_OFF" />


<!—LLDP On Policy -->

<lldpIfPol adminRxSt="enabled" adminTxSt="enabled" descr="" name="LLDP_ON" />


The port channel policy enables you to bundle several physical ports together to form a single port channel. LACP enables a node to negotiate an automatic bundling of links by sending LACP packets to the peer node. The policy names used as follows reflect the policy type name and mode.

To configure a port channel policy, log in to the APIC GUI with administrator privileges and follow the path below:

Fabric à Access Policies à Interface Policies à Policies à Port Channel Policies

Figure 45: Interface Policy – Port Channel Policy


XML 7: Interface Policy - Channel Policy


<!—Port Channel LACP Active Policy -->

<lacpLagPol ctrl="fast-sel-hot-stdby,graceful-conv,susp-individual" descr="" dn="uni/infra/lacplagp-LACP_ACTIVE" maxLinks="16" minLinks="1" mode="active" name="LACP_ACTIVE" />


<!—Port Channel LACP MacPinning Policy -->

<lacpLagPol ctrl="fast-sel-hot-stdby,graceful-conv,susp-individual" descr="" dn="uni/infra/lacplagp-LACP_MacPinning" maxLinks="16" minLinks="1" mode="mac-pin" name="LACP_MacPinning" />


<!—Port Channel LACP Off Policy -->

<lacpLagPol ctrl="fast-sel-hot-stdby,graceful-conv,susp-individual" descr="" dn="uni/infra/lacplagp-LACP_OFF" maxLinks="16" minLinks="1" mode="off" name="LACP_OFF" />


The switch profile identifies the leaf nodes that are provisioned to support the environment. The switch profile name swProfile_Leaf1_2 specifies a switch selector designator Leaf1_2 identifying nodes 101 and 102. Also, the previously defined interface profiles are also associated to the switch profiles, as shown in the following diagram.


Note: The switch profile configuration must be updated following the completion of each interface profile.

To configure a switch profile, log in to the APIC GUI with administrator privileges and follow the path below:

Fabric à Access Policies à Switch Policies à Switch Profile à [swProfile_Leaf1_2]

Figure 46: Switch Profile


XML 8: Switch Profile


<infraNodeP descr="" name="swProfile_Leaf1_2" >


<!—Switch Profile Leaf Selector -->

<infraLeafS descr="" name="Leaf1_2" type="range">

<infraNodeBlk descr="" from_="101" name="29a1f174834b2ea7" to_="102"/>

</infraLeafS>


<!—Associated Interface Profile -->

<infraRsAccPortP tDn="uni/infra/accportprof-intProfile_Leaf1_2"/>

</infraNodeP>


Repeat the process for the remaining switch profiles:

Fabric à Access Policies à Switch Policies à Switch Profile à [swProfile_Leaf1]

Fabric à Access Policies à Switch Policies à Switch Profile à [swProfile_Leaf2]

The vPC domain identifies the leaf nodes that define a virtual port channel. Within the vPC configuration, a VPC explicit protection group represents a vPC domain (a protection group). You can explicitly configure member nodes of the group using a fabric policy node endpoint. The explicit vPC protection group name used in Scenario 1 is ‘VPC_Domain_Leaf1_2’ along with the logical pair ID number ‘12’.

Note: Depending on the use case, multiple vPC domains may need to be created to support the topology.

To configure a vPC domain, login to the APIC GUI with administrator privileges and follow the path below:

Fabric à Access Policies à Switch Policies à Switch Profile à [swProfile_Leaf1_2]

Figure 47: vPC Domain


XML 9: vPC Domain


<!—vPC Domain -->

<fabricProtPol descr="" name="default" pairT="explicit">


<!—vPC ID and Name-->

<fabricExplicitGEp id="12" name="VPC_Domain_Leaf1_2">

<fabricRsVpcInstPol tnVpcInstPolName="default"/>


<!—Node Selection -->

<fabricNodePEp descr="" id="101" name=""/>

<fabricNodePEp descr="" id="102" name=""/>

</fabricExplicitGEp>

</fabricProtPol>


This section provides the steps required to support the vPC configuration on the pair of ACI leaf switches used in the validated topology. Multiple vPCs are used to support the physical connectivity of external devices to the ACI fabric, as for example the Cisco Nexus 7000 switches (FabricPath and DC core devices) for Layer 2 and Layer 3 connectivity, the ASAs for firewall services and the UCS for virtual hosts.

The following network diagram highlights all the vPC connections used for the different migration scenarios:

1. The vPC identified with the number 1 is used for the Layer 2 connectivity between the ACI fabric and the FabricPath core and will be named VPC_FPCORE.

2. vPC 2 is used for layer 3 connectivity between the ACI fabric and the DC core (VPC_DCCORE).

3. vPC 3 is used for the connectivity between the ACI fabric and the ASA inside interface. Notice how two separate vPC connections are used from the ACI fabric, one to connect to the Active FW node and the other to connect to the Standby node (VPC_ASA_IN_1 and VPC_ASA_IN_2). This has some important implications for the configuration of the L3Out connections between the ACI VRFs and the ASA FWs, as discussed in more detail in the “External Routed Networks” section part of the migration Scenario 2.

4. vPC 4 is used for the connectivity between the ACI fabric and the ASA outside interface. As discussed above, two vPC logical connections are used also in this case, one to each ASA FW node (VPC_ASA_OUT_1 and VPC_ASA_OUT_2).

5. vPC 5 is used for the UCS connectivity between the ACI fabric and Fabric Interconnect A (VPC_FI_A).

6. vPC 6 is used for the UCS connectivity between the ACI fabric and Fabric Interconnect B (VPC_FI_B).


Figure 48: vPC Connections


Some important considerations that relate to the creation of those vPC logical connections:

· Depending on the topology and requirements, other valid configurations may be used to establish Layer 3 connectivity between the Brownfield and ACI environments (routed interfaces or routed sub-interfaces in lieu of SVI connectivity over vPC). The design suggested in this document consists in creating a logical vPC connection and static routing. This provides the advantage that a link failure scenario would be recovered at the Layer 2 level (that is, re-hashing of traffic flows across the remaining physical links of the vPC).

· The reason you didn’t just use the same Layer 2 vPC1 also for establishing Layer 3 connectivity between the FP and the ACI networks (instead of the separate vPC2) is because traditionally, during migrations, the idea is to migrate off the equipment that is Layer 2 attached, and then retire it, shut it down, or repurpose it. It was important to demonstrate that the Layer 3 DC routers (DCCORE01/02) would be used even after migration from the FabricPath FP_CORE01/02 devices was completed.

· Although multiple vPCs on the ASA were used (a vPC for the inside interface and a separate vPC for the outside interface), a single vPC could have also been deployed to support the same topology. The use case and connectivity requirements should define the overall connectivity strategy.

The following sections highlight the different configuration steps required for the creation of a vPC.

Note: The vPC configuration discussed in the following sections is relative to the creation of VPC_FPCORE (for Layer 2 connectivity between the FP and the ACI networks). A pretty much identical procedure can be followed for the creation of the other vPC connections (not covered in this document).

The following section describes the procedure for creating in APIC the vPC logical connection used for Layer 2 communication between the FabricPath and the ACI networks.

The Cisco Nexus 7000 FP spines will be connected to the ACI leaf switches using full-meshed 10-G interfaces for redundancy and to carry data traffic for different VLANs used for the migration scenarios. The 2x10G interfaces will be directly connected between two Cisco Nexus 7000 switches for high availability.

The following table provides the physical interface designation for the Cisco Nexus 7000 and the ACI leaf connectivity.

Table 2: Layer 2 Physical Connectivity

Fabric Path Core

ACI Fabric

Speed

FP_CORE1_3/10

Leaf1_1/4

10GIG

FP_CORE1_3/11

Leaf2_1/4

10GIG

FP_CORE2_3/11

Leaf1_1/5

10GIG

FP_CORE2_3/11

Leaf2_1/5

10GIG

The Interface Policy Group enables you to specify the interface characteristics and define the behavior for selected ports. Within the Interface Policy Group, interface parameters such as the link properties (Link Level Policy) and Port Channel capabilities are defined. The policy names used as follows reflect the policy type name along with the interface type and location within the topology.

To configure an interface policy group, log in to the APIC GUI with administrator privileges and follow the path below:

Fabric à Access Policies à Interface Policies à Policy Groups à [policyGrpVPC_FPCORE]

Table 3: Layer 2 vPC Interface Policy Group

Interface Poicy Group

Configuration

Name

policyGrpVPC_FPCORE

Link Aggregation Type

VPC

Link Level Policy

10GIGAuto

CDP_Policy

CDP_ON

LLDP_Policy

LLDP_ON

Port Channel Policy

LACP_ACTIVE

Attachable Entity Profile

AAEP


Figure 49: Layer 2 vPC Interface Policy Group


XML 10: vPC Interface Policy Group


<infraAccBndlGrp descr="" dn="uni/infra/funcprof/accbundle-policyGrpVPC_FPCORE" lagT="node" name="policyGrpVPC_FPCORE" >

<infraRsMonIfInfraPol tnMonInfraPolName=""/>


<!—LLDP Policy Selection -->

<infraRsLldpIfPol tnLldpIfPolName="LLDP_ON"/>

<infraRsStpIfPol tnStpIfPolName=""/>


<!—LLDP Policy Selection -->

<infraRsCdpIfPol tnCdpIfPolName="CDP_ON"/>

<infraRsL2IfPol tnL2IfPolName=""/>


<!— Attachable Entity Profile Selection -->

<infraRsAttEntP tDn="uni/infra/attentp-AAEP"/>

<infraRsMcpIfPol tnMcpIfPolName=""/>


<!—Port Channel Policy Selection -->

<infraRsLacpPol tnLacpLagPolName="LACP_ACTIVE"/>

<infraRsStormctrlIfPol tnStormctrlIfPolName=""/>


<!—Link Level Policy Selection -->

<infraRsHIfPol tnFabricHIfPolName="10GigAuto"/>

</infraAccBndlGrp>


Note: The interface policies selected are the ones previously created in the “Interface Policies” section.

The Interface Profile enables you to define the specific interfaces that use the properties identified under the Interface Policy Group. Within the Interface Profile, each physical interface is identified and added. The policy names used as follows reflect the policy type name and the node(s).

To configure an Interface Profile, log in to the APIC GUI with administrator privileges and follow the path below:

Fabric à Access Policies à Interface Policies à Profiles à [intProfile_Leaf1_2]

Table 4: Layer 2 vPC Interface Profile

Interface Profile

Configuration

Interface Selector

intSelector_1_4

Interface Selector

intSelector_1_5

Note: Object names, such as the Interface Selector name, cannot be modified once implemented. This may present an operational challenge if the specific interfaces wanted to be re-used later on in the future. Because of this fact, separate interface selectors were created for each and every interface in use. This ensures that the interface selectors can be reused in the future, if needed, with no impact. Careful consideration should be exercised when planning object naming.

Figure 50: Layer 2 vPC Interface Profile


XML 11: Layer 2 vPC Interface Profile


<infraAccPortP descr="" name="intProfile_Leaf1_2" >


<!—Interface Selector -->

<infraHPortS descr="" name="intSelector_1_5" type="range">

<infraRsAccBaseGrp fexId="101" tDn="uni/infra/funcprof/accbundle-policyGrpVPC_FPCORE"/>


<!—Port Selector -->

<infraPortBlk descr="" fromCard="1" fromPort="5" name="block2" toCard="1" toPort="5"/>


<!—Interface Selector -->

</infraHPortS>

<infraHPortS descr="" name="intSelector_1_4" type="range">

<infraRsAccBaseGrp fexId="101" tDn="uni/infra/funcprof/accbundle-policyGrpVPC_FPCORE"/>


<!—Port Selector -->

<infraPortBlk descr="" fromCard="1" fromPort="4" name="block2" toCard="1" toPort="4"/>

</infraHPortS>

</infraAccPortP>


VMM integration allows a manager such as VMware vCenter to be linked to ACI so that policies can be made available for virtual machines hosted within the VMM domain. Once the APIC and a vCenter servers are linked together (via communication over an OOB network) with the creation of a VMM domain, the following actions take place:

· A Distributed Virtual Switch (DVS) managed by APIC is created and made available to all the ESXi hosts managed by the vCenter server.

· Every time an EPG is created in APIC and bound to the VMM domain, a corresponding port group is dynamically created in vCenter for the previously described DVS. Virtual machines (VMs) can then be connected to that port group and this allows the ACI fabric to classify them as part of the defined EPG.

The following sections described the various steps required for the creation of the VMM Domain.

A Dynamic VLAN pool is managed internally by the APIC to allocate VLANs to the dynamically created port groups (associated to the EPGs) that are made available on the APIC-managed DVS. In the following example, a Dynamic Pool 1000-1010 is created and will be used for the migration scenarios discussed in later sections.

Note: A VMM domain can be associated with only one dynamic VLAN pool.

Table 5: VM Networking – Dynamic VLAN Pool

VM Networking

VLAN Pool

Description

Name

ACI_VMM_DynamicPool

-

Allocation Mode

Dynamic Allocation

-

Encap Block

1000-1010

VM Portgroup VLAN Pool


To configure a dynamic VLAN pool, login to the APIC GUI with administrator privileges and follow the path below:

Fabric à Access Policies à Pools à VLAN à [ACI_VMM_DynamicPool]

Figure 51: VM Networking – Dynamic VLAN Pool

XML 12: VM Networking – Dynamic VLAN Pool


<fvnsVlanInstP allocMode="dynamic" descr="Dynamic Vlan Pool" name="ACI_VMM_DynamicPool">


<!—VLAN Pool -->

<fvnsEncapBlk allocMode="inherit" from="vlan-1001" name="" to="vlan-1010"/>

</fvnsVlanInstP>


VMM domains contain VM controllers such as VMware vCenter or Microsoft System Center Virtual Machine Manager (SCVMM) and the credential(s) required for the ACI API to interact with the VM controller. A VMM domain enables VM mobility within the domain but not across domains.


To configure a VMM domain, the first step is defining the VM Provider (Microsoft or VMware options are available at the time of writing of this document). In order to do that, log in to the APIC GUI with administrator privileges and follow the path below:

Fabric à VM Networking à Policies à VM Provider VMware à [ACI_VMM] à Controller


Figure 52: VM Networking – Provider VMware


XML 13: VM Networking – Provider VMware


<vmmDomP enfPref="hw" mcastAddr="0.0.0.0" mode="default" name="ACI_VMM">

<vmmRsDefaultStpIfPol tnStpIfPolName="default"/>

<vmmRsDefaultFwPol tnNwsFwPolName="default"/>

<vmmRsDefaultLldpIfPol tnLldpIfPolName="default"/>


<!—vCenter IP -->

<vmmCtrlrP dvsVersion="unmanaged" hostOrIp="10.201.144.160" inventoryTrigSt="untriggered" name="controller" port="0" rootContName="aci" scope="vm" statsMode="disabled">


<!—vCenter Credentials -->

<vmmRsAcc tDn="uni/vmmp-VMware/dom-ACI_VMM/usracc-Creds"/>

</vmmCtrlrP>


<!—Dynamic VLAN Pool -->

<infraRsVlanNs tDn="uni/infra/vlanns-[ACI_VMM_DynamicPool]-dynamic"/>

<vmmRsDefaultCdpIfPol tnCdpIfPolName="default"/>

<vmmRsDefaultLacpLagPol tnLacpLagPolName="default"/>

<vmmRsDefaultL2InstPol tnL2InstPolName="default"/>

<vmmUsrAccP descr="" name="Creds" usr="root"/>

</vmmDomP>


As shown in following figure, the following parameters must be specified to successfully create a VMM domain:

· Host name (or IP Address): this is the DNS name or IP address of the vCenter server the APIC should be paired with.

· DVS version: specifies the version of the APIC-managed DVS that is created as a result of the pairing between APIC and vCenter.

· Datacenter: specifies the set of compute resources that will be part of the VMM Domain. The same vCenter server may define different logical data centers and a separate VMM domain can be created associating the APIC with each data center object defined in vCenter. It is also important to ensure that the data center name used in vCenter exactly matches the “Datacenter” configuration section in the VMM Controller policy window.


Figure 53: VMM Integration


· Associated Credentials: those are the credentials required to connect to the vCenter server.

As previously mentioned, the Attachable Access Entity Profile provides a template for attachment point between the switch and interface profiles and the fabric resources such as the VLAN pool. In order to be able to use the VLANs part of the Dynamic Pool (1000-1010) for connectivity to the various port groups defined in the APIC-managed DVS, it is first required to associate the just created VMM domain to the AEP previously created in the “Access Policies” section.


Note: Although a single Attachable Access Entity Profile was deployed to support the fabric path migration (for both physical and virtual domains for both Scenarios 1 and 2), individual use cases and connectivity requirements may dictate the use of multiple AAEPs. One such use case is an overlap of VLAN resources amongst tenants.

To associate the VMM Domain with the Attachable Entity Profile, log in to the APIC GUI with administrator privileges and follow the path below:

Fabric à Access Policies à Global Policies à Attachable Access Entity Profiles à [AAEP]


Figure 54: VM Networking – Attachable Access Entity Profile


XML 14: VM Networking – Attachable Access Entity Profile


<infraAttEntityP name="AAEP" >


<!—VMM Domain Association -->

<infraRsDomP tDn="uni/vmmp-VMware/dom-ACI_VMM"/>

</infraAttEntityP>


To support connectivity from the ACI fabric to vCenter VMware a combination of CDP/LLDP are used to exchange host connectivity details. To enable the APIC-created DVS with the supported communication protocol, a vSwitch with CDP enabled and LLDP disabled is created and attached to the AEP, Attachable Access Entity Profile.

Note: VM integration with UCS-B Series and ACI requires specific configurations. Refer to the following document for more information:

http://www.cisco.com/c/en/us/support/docs/cloud-systems-management/application-policy-infrastructure-controller-apic/118965-config-vmm-aci-ucs-00.html

To configure a VMM vSwitch Policy, log in to the APIC GUI with administrator privileges and follow the path below:

Fabric à Access Policies à Global Policies à Attachable Access Entity Profile à [AAEP]

Figure 55: VM Networking – vSwitch Policy


XML 15: VM Networking – vSwitch Policy


<infraAttEntityP name="AAEP" >


<!—vSwitch Policy -->

<infraAttPolicyGroup descr="" name="">

<infraRsOverrideCdpIfPol tnCdpIfPolName="CDP_ON"/>

<infraRsOverrideLacpPol tnLacpLagPolName="LACP_MacPinning"/>

<infraRsOverrideLldpIfPol tnLldpIfPolName="LLDP_OFF"/>

</infraAttPolicyGroup>

</infraAttEntityP>


Migration Scenario 1

Now that the work has been done in the “Infrastructure Deployment Considerations” section (that is all the physical interfaces, vPC domain, BGP RR, VM Networking integration common to all the migration scenarios have been configured), the specific migration scenario configurations can start. These include fabric access policy configuration (specific for Migration Scenario 1) and tenant configuration.

Note: A single tenant configuration is shown as part of the Migration Scenario 1, but the same considerations remain valid and can be replicated in a multitenant environment.

1. Fabric Access Policy Configuration

a. Static VLAN Pool

b. Physical Domain

c. External Routed Domain that is used to create the L3Out connection between the ACI fabric and the DC core routers (DCCORE01/02 routers)

2. Tenant Configuration

a. Tenant creation (named “Scenario 1”)

b. Private Network (that is, VRF100; this maps to the default VRF in the FabricPath domain)

c. Bridge Domain (that is, BD100; this will map to VLAN 100 in the FabricPath domain)

d. Security Contracts, where applicable

i. L3OUT contract

ii. Outside contract

e. Connectivity (via an L3Out to DCCORE01/02)

f. Application Profile and EPGs (i.e., Application Profile AP_IN and AP_OUT)

i. EPG Outside

ii. EPG AppOneWeb

iii. EPG AppTwoWeb

iv. EPG AppThreeWeb

Figure 56: Scenario 1 ACI Design


Fabric Access Policy Configuration

A static VLAN pool (vlanPool_Scenario1) needs to be created for Scenario 1with the following encap blocks:

· “L3OUT” Encap Block: defines the VLAN tag that is going to be used on the L3Out to establish Layer 3 connectivity between the FP and the ACI networks. As already discussed, the design choice was to create a vPC logical connection with static routing for this purpose, so VLAN 49 is used to establish Layer 3 communication between SVIs defined on the ACI Border Leaf nodes and corresponding SVIs defined on the DC Core devices.

· “Layer2” Encap Block: defines the set of VLANs used to establish Layer 2 connectivity between the FP and the ACI networks. Only VLAN 100 is required for migration Scenario 1.

Table 6: Scenario 1 Static VLAN Pools

VLAN Pool

Configuration

Description

Name

vlanPool_Scenario1

-

Allocation Mode

Static Allocation

-

Encap Block

49

L3OUT

-

100

Layer 2

To configure the static VLAN pool, log in to the APIC GUI with administrator privileges and follow the path below:

Fabric à Access Policies à Pools à VLAN à [vlanPool_Scenario1]

Figure 57: Scenario 1 Static VLAN Pool


XML 16: Scenario 1 Static VLAN Pool


<fvnsVlanInstP allocMode="static" name="vlanPool_Scenario1" >


<!—VLAN Pools -->

<fvnsEncapBlk allocMode="inherit" descr="" from="vlan-49" name="" to="vlan-49"/>

<fvnsEncapBlk allocMode="inherit" descr="" from="vlan-100" name="" to="vlan-112"/>

</fvnsVlanInstP>


Differently from the VMM domain previously discussed, physical domains are usually specifically defined for each given tenant because in most deployments, a physical server belongs to one tenant. In the specific case of migration Scenario 1, the physical domain is defined not to connect physical servers but to allow connectivity from the ACI fabric to the VMs that have not yet been migrated and are still connected to the FP network, as well to VMs that have been migrated to the ACI fabric but are initially connected to the vCenter-managed VDS. Since a static EPG-VLAN mapping is performed to allow Layer 2 connectivity to those VMs, they can be considered as “physical resources” from an ACI fabric perspective, hence it is required to define a physical domain to be able to specify the VLANs to be used to connect to them.

Note: In multitenant deployments, a separate physical domain will likely be created for each tenant. This allows to granularly manage resources associated with each tenant.

See the following table for the configuration details:

Table 7: Scenario 1 Physical Domain

Physical Domain

Configuration

Name

phyDomain_Scenario1

VLAN Pool

vlanPool_Scenario1


To configure the physical domain, log in to the APIC GUI with administrator privileges and follow the path below:

Fabric à Access Policies à Physical and External Domains à Physical Domains à [phyDomain_Scenario1]

Figure 58: Scenario 1 Physical Domain


XML 17: Scenario 1 Physical Domain


<physDomP name="phyDomain_Scenario1" >


<!—VLAN Pool association -->

<infraRsVlanNs tDn="uni/infra/vlanns-[vlanPool_Scenario1]-static"/>

</physDomP>


As shown above, the physical domain is both associated with the AEP and with the static VLAN pool previously defined.

As previously mentioned, a VLAN (or in general a set of VLANs) is required for establishing Layer 3 connectivity to the external Layer 3 network. This is specifically true when SVI interfaces are defined on the ACI border leaf nodes to route traffic to the external world (as it is the case for the migration scenarios discussed in this paper).

The definition of an external routed domain is then required to associate a VLAN pool with the L3Out configuration (see the following table):

Table 8: Scenario 1 External Routed Domain

External Routed Domain

Configuration

Name

extRoutedDomain_Scenario1

VLAN Pool

vlanPool_Scenario1

To configure the external routed domain, log in to the APIC GUI with administrator privileges and follow the path below:

Fabric à Access Policies à Physical and External Domains à External Routed Domain à [extRoutedDomain_Scenario1]

Figure 59: Scenario 1 External Routed Domain


XML 18: Scenario 1 External Routed Domain


<l3extDomP name="extRoutedDomain_Scenario1" >


<!—VLAN Pool Association -->

<infraRsVlanNs tDn="uni/infra/vlanns-[vlanPool_Scenario1]-static"/>

</l3extDomP>


As it was the case for the physical domain, the external routed domain must be associated with an AEP and with a VLAN pool.

Tenant Configuration

Note: The use of the Quick Start guide is not used in order to demonstrate the object relationship for the configuration parameters. Additionally, while Quick Start menus can change from version to version, the method of configuration displayed in this document will not change.

A tenant is a logical container for application policies that enable an administrator to exercise domain-based access control. For the sake of the migration study, you will create separate tenants for each scenario. See the following table or the configuration details:

Table 9: Scenario 1 Tenant

Tenant

Configuration

Name

Scenario1


Note: A tenant represents a unit of isolation from a policy perspective and can represent a customer, an organization or domain in an enterprise setting, or just a convenient grouping of policies.

To configure the tenant, log in to the APIC GUI with administrator privileges and follow the path below:

Tenant à ADD TENANT à [Create Tenant Scenario1]

Figure 60: Scenario 1 Tenant Definition


XML 19: Scenario 1 Tenant Definition


<!—Tenant Scenario1 -->

<fvTenant name="Scenario1"/>


In this section, you create a private network (VRF100) representing the VRF you are going to associate to the previously created Tenant Scenario1. See the following table or the configuration details:

Table 10: Scenario 1 Private Network

Private Network

Configuration

Name

VRF100

Policy Control Enforcement Preference

Enforced

For the private network, you will be selecting “enforced” policy control. This ensures that the ACI fabric will use whitelisting model, which means that no Endpoint Group (EPG) will be able to communicate with another endpoint group, unless explicitly permitted with a contract. See the following table or the configuration details:

Note: By enforcing policy control, this means that contracts are required for communication to occur between EPGs.

To configure the private network, log in to the APIC GUI with administrator privileges and follow the path below:

Tenants à [Scenario1] à Networking à Private Networks à [Create VRF100]

Figure 61: Scenario 1 Private Network


XML 20: Scenario 1 Private Network


<!—Private Network VRF100 -->

<fvCtx descr="" knwMcastAct="permit" name="VRF100" pcEnfPref="enforced">

</fvCtx>


In this section, use the following details to create a bridge domain (BD100) representing the Layer 2 broadcast domain that is extended between the FabricPath and the ACI domains. The bridge domain will be associated with the private network VRF100. See the following table for the configuration details:

Table 11: Scenario 1 Bridge Domain

Bridge Domain

Configuration

Name

BD100

Private Network

Scenario1/VRF100

Layer 2 Unknown Unicast

Flood

Layer 2 Unknown Multicast Flooding

Flood

Multi Destination Flooding

Flood within encapsulation

Unicast Routing

Enabled

ARP Flooding

Enabled

Enforce subnet check for IP learning

Enabled


This bridge domain will eventually house the pervasive Anycast GW for VLAN 100. All of the EPGs that are defined and used for Scenario 1 will be associated to the same bridge domain BD100.

Table 11 and Figure 62 highlight the specific BD configuration parameters, it is important to point out how the BD must be configured to flood Layer 2 unknown unicast and ARP traffic. This is because you need to ensure that Layer 2 communication can be successfully established between workloads connected to the FP and ACI leaf nodes. The endpoints in the FP domain may not have been discovered yet on the ACI fabric, so flooding will be needed to ensure communication.

To configure the bridge domain, log in to the APIC GUI with administrator privileges and follow the path below:

Tenants à[Scenario1] à Networking à Bridge Domains à [Create BD100]

Figure 62: Scenario 1 Bridge Domain


XML 21: Scenario 1 Bridge Domain


<!—Bridge Domain BD100 -->

<fvBD arpFlood="yes" descr="" epMoveDetectMode="" limitIpLearnToSubnets="yes" llAddr="::" mac="00:00:0C:07:AC:64" multiDstPktAct="encap-flood" name="BD100" unicastRoute="yes" unkMacUcastAct="flood" unkMcastAct="flood"/>


One or more IP subnets can be associated to a given bridge domain. In this case, the IP subnet 100.1.1.0/24 is defined for BD100; this is required because one of the steps of the migration procedure consists in moving the default gateway for that IP subnet away from the FP spine devices and into the ACI fabric (the ACI fabric offers a distributed gateway functionality on all the leaf nodes).

As a result of the following configuration, an SVI interface will be created as part of the private network VRF100 to be able to route traffic in and out of IP subnet 100.1.1.0/24.

Note: In order to be able to perform routing functions, the BD must be enabled for unicast routing by setting the corresponding flag shown in the previous screenshot.

Table 12: Scenario 1 Bridge Domain Subnet

Bridge Domain Subnet

Configuration

Description

Subnet

100.1.1.254/24

Pervasive Gateway

Scope

Public


This bridge domain will eventually house the pervasive Anycast GW for VLAN 100; a temporary IP address 100.1.1.254 is assigned until then to be able to verify connectivity.

To configure bridge domain subnet, log in to the APIC GUI with administrator privileges and follow the path below:

Tenant à[Scenario1] à Networking à Bridge Domains à [BD100] à Subnets

Figure 63: Scenario 1 Bridge Domain Subnet


XML 22: Scenario 1 Bridge Domain Subnet


<!—Bridge Domain Subnet -->

<fvBD name="BD100">

<fvSubnet ctrl="" ip="100.1.1.1/24" name="" preferred="no" scope="public"/>

</fvBD>


In this section, use the information in the following table to create a filter. Based on the requirements for Scenario 1, an “any to any” filter is created to allow communication for all devices inside of the ACI fabric with devices outside of the fabric (connected to the FP network). See the following table for the configuration details:

Table 13: Scenario 1 Contract Filter

Contract Filter

Configuration

Description

Filter

any-any

-


To configure the contract filter, log in to the APIC GUI with administrator privileges and follow the path below:

Tenants à [Scenario1] à Security Policies à Filters à [Create a filter]

Figure 64: Scenario 1 Contract Filter


XML 23: Scenario 1 Contract Filter


<!—Contact Filter -->

<vzFilter name="Any-any">

<vzEntry applyToFrag="no" arpOpc="unspecified" dFromPort="unspecified" dToPort="unspecified" descr="" etherT="ip" icmpv4T="unspecified" icmpv6T="unspecified" name="Any" prot="unspecified" sFromPort="unspecified" sToPort="unspecified" stateful="no" tcpRules=""/>

</vzFilter>


In this section two contracts will be created: “L3Out_Permit_Any” for establishing Layer 3 communications with the routed domain outside of the fabric and “FP_Out_Permit_Any” for allowing Layer 2 communication between endpoints inside of the fabric on VLAN 100, and devices which remain outside of the fabric in the FabricPath environment.

14: Scenario 1 Contract

Contract

Configuration

Description

Contract1

L3Out_Permit_Any

Layer 3 communication outside of the fabric

Contract2

FP_OUT_Permit_Any

Layer 2 communication between endpoints inside of the fabric on VLAN 100, and devices which remain outside of the fabric in the FabricPath environment

Filter

any-any


To configure the contracts, log in to the APIC GUI with administrator privileges and follow the path below:

Tenants à [Scenario1] à Security Policies à Contracts à [L3Out_Permit_Any]

Figure 65: Scenario 1 Contract Definition


XML 24: Scenario 1 Contract Definition


<!—Contact -->

<vzBrCP name="L3OUT_Permit_Any" prio="unspecified" scope="context">

<vzSubj consMatchT="AtleastOne" descr="" name="any" prio="unspecified" provMatchT="AtleastOne" revFltPorts="yes">

<vzRsSubjFiltAtt tnVzFilterName="Any-any"/>

</vzSubj>

</vzBrCP>


Repeat the process for the remaining Contracts:

Tenants à [Scenario1] à Security Policies à Contracts à [FP_OUT_Permit_Any]

As previously discussed, static routing is configured between the DC core devices and the ACI border leaf nodes to establish Layer 3 communications in and out of the ACI fabric. HSRP is run between the Cisco Nexus 7000 DC core devices to provide the ACI fabric with a single virtual IP address as next-hop toward the external Layer 3 domain. At the same time, the ACI fabric will define a single floating IP address (used by both border leaf switches) to be used by the DC core devices as next-hop toward the IP subnets defined inside the ACI fabric.

· The ACI fabric defines a 0.0.0.0/0 static routes on each border leaf pointing as next-hop to the HSRP VIP provided by the Cisco Nexus 7000 pair of devices in the DC core on VLAN 49.

· The pair of Cisco Nexus 7000 DC core devices (DCCORE01/02) use a static route to reach the IP subnet associated to the bridge domain BD100 (100.1.1.0/24). The next-hop for the static route is the secondary IP address assigned on both ACI border leaf nodes to the SVI 49. Also, the static route is configured with an admin distance of 254. This will ensure that DCCORE01/02 prefer the OSPF learned route from the FabricPath switches (FP_CORE01/02), until the default gateway for the 100.1.1.0/24 IP subnet is migrated to the ACI fabric (and removed from the FP spines). At that point, the static route to 100.1.1.0/24 will be used to funnel traffic back towards the ACI fabric. Note: Static routing over the vPC was used for this scenario as there are known issues with dynamic routing over vPCs to other Cisco Nexus platforms (this is not an ACI limitation). You could have also used routed sub-interfaces or routed interfaces to the DCCORE01/02 routers in conjunction with either EIGRP, eBGP, iBGP, or OSPF routing.

In order to complete the configuration of an L3OUT, you will need complete the following tasks:

1. Configure L3Out Properties

2. Configure Logical Node Profiles

3. Configure Logical Interface Profiles

4. Configure L3Out EPG parameters

5. Configure Contracts for the L3Out EPG

In this section the External Routed Network, L3Out_Scenario1, will be created. The L3Out will define the network details for reaching Layer 3 networks outside of the ACI fabric domain.

Table 15: Scenario 1 L3Out Properties

L3OUT

Configuration

Description

Name

L3Out_Scenario1

-

Private Network

Scenario1/VRF100

Associate the L3Out with the proper private network

External Routed Domain

extRoutedDomain_Scenario1

Associate the L3Out with the proper external routed domain (this is the previously created domain that contains VLAN 49, which can be used by the L3Out for SVI-based connectivity).

To configure the external routed network, log in to the APIC GUI with administrator privileges and follow the path below:

Tenants à [Scenario1] à Networking à External Routed Networks à [Create L3OUT_Scenario1]

Figure 66: Scenario 1 L3Out Properties


XML 25: Scenario 1 L3Out Properties


<!—External Routed Network – L3OUT -->

<l3extOut enforceRtctrl="export" name="L3Out_Scenario1" targetDscp="unspecified">


<!—Association w/VRF100 -->

<l3extRsEctx tnFvCtxName="VRF100"/>


<!—Association w/External Routed Domain -->

<l3extRsL3DomAtt tDn="uni/l3dom-extRoutedDomain_Scenario1"/>

</l3extOut>


In this section, the external routed network node profile will be created. The node profile defines the fabric nodes that participate in the L3Out connectivity and provides the static route. One logical node profile will be created, specifying the two physical border leaf nodes. A static route is configured on each physical border leaf node pointing to the HSRP VIP address on the DC core devices on VLAN 49.

Table 16: Scenario 1 L3Out Node Profiles

Node Profile

Configuration

Description

Name

Leaf1_2_Node_Profile

Logical node profile specifying both physical border leaf nodes

Node ID

topology/pod-1/node-101

-

Router ID

150.1.1.1

(This must be a unique IP address which is NOT in use). The ACI fabric will automatically create a loopback on the associated border leaf with this IP.

Static Route

0.0.0.0/0

49.1.1.1

Node ID

topology/pod-1/node-102

-

Router ID

150.1.1.2

(This must be a unique IP address which is NOT in use). The ACI Fabric will automatically create a loopback on the associated border leaf with this IP.

Static Route

0.0.0.0/0

49.1.1.1


To configure the node profile, log in to the APIC GUI with administrator privileges and follow the path below:

Tenants à [Scenario1] à Networking à External Routed Networks à [L3OUT_Scenario1] à Logical Node Profiles à [Create Node_Profile]

Figure 67: Scenario 1 L3Out Node Profiles


XML 26: Scenario 1 L3Out Node Profile


<l3extOut descr="" enforceRtctrl="export" name="L3Out_Scenario1" targetDscp="unspecified">


<!—Node Profile -->

<l3extLNodeP name="Leaf1_Node_Profile" targetDscp="unspecified">


<!—Node 101 -->

<l3extRsNodeL3OutAtt rtrId="150.1.1.1" rtrIdLoopBack="yes" tDn="topology/pod-1/node-101">

<ipRouteP aggregate="no" ip="0.0.0.0/0" name="" pref="1">

<ipNexthopP descr="" name="" nhAddr="49.1.1.1"/>

</ipRouteP>

</l3extRsNodeL3OutAtt>


<!—Node P102 -->

<l3extRsNodeL3OutAtt rtrId="150.1.1.2" rtrIdLoopBack="yes" tDn="topology/pod-1/node-102">

<ipRouteP aggregate="no" descr="" ip="0.0.0.0/0" name="" pref="1">

<ipNexthopP descr="" name="" nhAddr="49.1.1.1"/>

</ipRouteP>

</l3extRsNodeL3OutAtt>

</l3extOut>


In this section, the interface profile is created to define on both border leaf nodes the SVI interfaces on VLAN 49 to be used for Layer 3 communication with the DC core devices.


Table 17: Scenario 1 L3Out Interface Profile

Interface Profile

Description

Name

Leaf_Int_Profile

-

Interface Type

SVI

-

Path Type

Virtual Port Channel

-

Path

Node101-102/policyGrpVPC_DCCORE

Refer to the vPC Interface Policy Group previously created in the “Creation of Virtual Port Channels (vPCs)” section.

Encap

Vlan-49

-

Site A IP Address

49.1.1.252/24

-

Site A Secondary IP Address

49.1.1.254/24

The secondary IP address for Site A MUST match Site B

Site B IP Address

49.1.1.253/24

-

Site B Secondary IP Address

49.1.1.252/24

The secondary IP address for Site B MUST match Site A

MTU

9000

By default the fabric will “inherit” the system MTU, which is 9000. It is considered best practice to manually set the fabric MTU on your interface profile to match the router on the other side.


To configure the interface profiles, log in to the APIC GUI with administrator privileges and follow the path below:

Tenants à [Scenario1] à Networking à External Routed Networks à [L3OUT_Scenario1] à Logical Node Profiles à [Leaf1_Node_Profiles] à Logical Interface Profiles à [Create Interface_Profiles]

Figure 68: Scenario 1 L3Out Interface Profile


XML 27: Scenario 1 L3Out Interface Profile


<l3extOut descr="" enforceRtctrl="export" name="L3Out_Scenario1" targetDscp="unspecified">


<l3extLNodeP name="Leaf1_Node_Profile" targetDscp="unspecified">


<!—Interface Proile -->

<l3extLIfP descr="" name="Leaf_Int_Profile" tag="yellow-green">

<l3extRsNdIfPol tnNdIfPolName=""/>


<!—SVI -->

<l3extRsPathL3OutAtt addr="0.0.0.0" descr="" encap="vlan-49" ifInstT="ext-svi" llAddr="::" mac="00:22:BD:F8:19:FF" mode="regular" mtu="9000" tDn="topology/pod-1/protpaths-101-102/pathep-[policyGrpVPC_DCCORE]" targetDscp="unspecified">


<!—Interface Proile node 102 -->

<l3extMember addr="49.1.1.253/24" descr="" llAddr="::" name="" side="B">

<l3extIp addr="49.1.1.254/24" descr="" name=""/>

</l3extMember>


<!—Interface Proile node 101 -->

<l3extMember addr="49.1.1.252/24" descr="" llAddr="::" name="" side="A">

<l3extIp addr="49.1.1.254/24" descr="" name=""/>

</l3extMember>

</l3extRsPathL3OutAtt>

</l3extLIfP>

</l3extLNodeP>


</l3extOut>


The L3Out External Network is defined to represent the external Layer 3 world to the ACI fabric. Multiple external networks can be configured (using IP prefix and mask) to define the external IP prefixes capable of accessing fabric resources within the tenant. A unique external EPG is associated to each defined external network, and this allows you to then apply different security policies (contracts) between each external EPG and EPGs defined internally to the ACI fabric. Without those contracts, all connectivity from outside is blocked and external routes are not learned when using a dynamic routing protocol.

Table 18: Scenario 1 L3Out EPG

EPG

Configuration

Description

Name

L3EPG

-

Subnet

0.0.0.0/0

Defines the external subnets/network, which will be allowed to communicate to the ACI fabric from outside. In this case all the external Layer 3 prefixes can have access to the internal resources (assuming a contract is properly configured).

Scope

Security Import Subnet

The “Security Import Subnet” flag is set by default and ensures that external traffic matching the configured IP subnet (all the traffic in this example) is properly classified as part of this External EPG (EPG classification on a L3Out is IP subnet based and not VLAN based as on regular Layer 2 interfaces). The field has other functions that are not required to support the use case discussed (used to control route import/export for transit routing scenarios).


To configure the L3Out external network, log in to the APIC GUI with administrator privileges and follow the path below:

Tenants à [Scenario1] à Networking à External Routed Networks à [L3OUT_Scenario1] à Networks à [L3EPG]

Figure 69: Scenario 1 L3Out External Network


XML 28: Scenario 1 L3Out External Network


<l3extOut descr="" enforceRtctrl="export" name="L3Out_Scenario1" targetDscp="unspecified">


<!—Layer3 EPG -->

<l3extInstP name="L3EPG">


<!—Route Control -->

<l3extSubnet aggregate="" descr="" ip="0.0.0.0/0" scope="import-security"/>

</l3extInstP>

</l3extOut>


In this section, under the Contracts tab for the L3EPG, provide the previously defined L3Out contract (“L3OUT_Permit_Any”). This contract will then be consumed by the internal EPGs to allow successful communication with the external Layer 3 domain.

19: Scenario 1 L3OUT EPG Provider Contract

Provider Contracts

Configuration

Description

Name

L3OUT_Permit_Any

-

To configure the provider contract for the external routed network, log in to the APIC GUI with administrator privileges and follow the path below:

Tenants à [Scenario1] à Networking à External Routed Networks à [L3OUT_Scenario1] à Networks à [L3EPG]

Figure 70: Scenario 1 L3Out EPG Provider Contract


XML 29: Scenario 1 L3Out EPG Provider Contract


<l3extOut descr="" enforceRtctrl="export" name="L3Out_Scenario1" targetDscp="unspecified">


<!—Layer3 EPG -->

<l3extInstP name="L3EPG">


<!—Provider Contract -->

<fvRsProv tnVzBrCPName="L3OUT_Permit_Any"/>

</l3extInstP>

</l3extOut>


Integration Phase – Scenario 1

The next phase is the integration phase. Now that the ACI fabric has been staged, you are going to begin the configuration sections in ACI where you will be establishing connectivity to the FabricPath environment via the vPCs.

Figure 71: Scenario 1 Integration Phase


Application profiles define the policies, services, and relationships between endpoint groups (EPGs). Each application profile contains one or more EPG that can communicate with the other EPGs in the same application profile and with EPGs in other application profiles according to the contract rules.

Create two application profiles: one called AP_IN, which will house the new EPGs and will provide the logical separation between different application endpoints. The second one called AP_OUT, which will house an EPG, mapped to VLAN 100 in the FabricPath domain and that will represent to the ACI Fabric all the endpoints that are still connected to the Brownfield network and the ones that are migrated to the ACI fabric (but not yet relocated to the final Internal EPGs).

Figure 72: Scenario 1 Application Profiles


In this section, use the information in the following table to create the application profile. Application profiles define the policies, services, and relationships between endpoint groups (EPGs). For Scenario 1 the application profile AP_IN will contain the EPGs for the newly managed application migrated from the FabricPath domain.

Table 20: Scenario 1 Application Profile AP_IN

Application Profile

Configuration

Description

Name

AP_IN

-

Note: Each application profile contains one or more EPG that can communicate with the other EPGs in the same application profile and with EPGs in other application profiles according to the contract rules.

To configure the application profile, log in to the APIC GUI with administrator privileges and follow the path below:

Tenants à [Scenario1] à Application Profileà [AP_IN]

Figure 73: Scenario 1 Application Profile AP_IN


XML 30: Scenario 1 Application Profile AP_IN


<!—Layer3 EPG -->

<fvAp name="AP_IN" />


In this section, use the information in the following table to create the internal EPGs (AppOneWeb, AppTwoWeb and AppThreeWeb) part of the previously created application profile.

Table 21: Scenario 1 Internal EPGs

EPG

Bridge Domain

Domain

AppOneWeb, AppTwoWeb, AppThreeWeb

BD100

VMware/ACI_VMM

Note: The EPGs will not consume contracts by configuring this under the EPG. You will consume contracts via a VZANY contract under Tenant à [Scenario1] à Private Network à [VRF100] EPG Collection for Context. This choice allows you to consume the contract for all EPGs associated with that VRF, as opposed to consuming the same contract for each EPG, resulting in a saving of HW resources.

The following screen highlights how to configure AppOneWeb via the APIC GUI. A similar procedure can be followed to configure the other internal EPGs:

Tenants à [Scenario1] à Application Profileà [AP_IN] à Application EPGs à [Create AppOneWeb]

Figure 74: Scenario 1 EPG AppOneWeb


As shown in the previous Figure 73, the internal EPGs part of the AP_IN application profile will only be used to connect virtual machines attached to the ACI-managed DVS (ACI_VMM). As a consequence, the EPGs should only be associated to the corresponding VMM domain, as highlighted in the following screen.

Figure 75: Scenario 1 EPG AppOneWeb Virtual Domain Association


XML 31: Scenario 1 EPG AppOneWeb


<fvAEPg name="AppOneWeb">


<!—EPG Domain association -->

<fvRsDomAtt encap="unknown" instrImedcy="immediate" resImedcy="immediate" tDn="uni/vmmp-VMware/dom-ACI_VMM">

</fvRsDomAtt>


<!—EPG Bridge Domain association -->

<fvRsBd tnFvBDName="BD100"/>

</fvAEPg>


For Scenario 1, the application profile AP_OUT will contain the EPG representing the endpoints still connected to the FP network and the ones already migrated to the ACI fabric domain but not yet connected to the final internal EPG destination.

Table 22: Scenario 1 Application Profile AP_OUT

Application Profile

Configuration

Description

Name

AP_OUT

-


To configure the Application Profile AP_OUT, log in to the APIC GUI with administrator privileges and follow the path below:

Tenants à [Scenario1] à Application Profileà [AP_OUT]

In this section, use the information in the following table to create the endpoint group Outside within the previously created Application Profile AP_OUT. Differently from the internal EPGs previously created, the EPG Outside is associated to the Physical Domain phyDomain_Scenario1 and not to the VMM domain. This is because the EPG will host the VMs that are connected to a vCenter-managed DVS, hence seen as physical servers from the perspective of the ACI fabric.

Table 23: Scenario 1 EPG Outside

EPG

Bridge Domain

Domain

Outside

BD100

phyDomain_Scenario1

To configure the external routed network EPG, log in to the APIC GUI with administrator privileges and follow the path below:

Tenants à [Scenario1] à Application Profileà [AP_OUT] à Application EPGs à [Outside]

In this section, use the information in the following table to create the static bindings for the EPG Outside. The static bindings will allow you to connect endpoints to the previously created EPG Outside. Those endpoints are VMs still connected to the FP network (and that will communicate with the ACI fabric via the Layer 2 vPC connecting the border leaf nodes to the FP spine devices) and VMs newly migrated to the ACI fabric deployed on the ESXi hosts part of the UCSB-Mini chassis connected to the ACI leaf nodes via FIs.

Table 24: Scenario 1 EPG Outside Static Bindings

Static Binding

Configuration

Node-101-102/policyGrpVPC_FI_A

vlan-100

Node-101-102/policyGrpVPC_FI_B

vlan-100

Node-101-102/policyGrpVPC_FPCORE

vlan-100


To configure static bindings, log in to the APIC GUI with administrator privileges and follow the path below:

Tenants à [Scenario1] à Application Profileà [AP_OUT] à Application EPGs à [Outside] à Static Bindings

Figure 76: Scenario 1 EPG Outside Static Bindings


XML 32: Scenario 1 EPG Outside Static Bindings


<fvAEPg name="Outside">


<!—EPG Static Binding FP_CORE-->

<fvRsPathAtt descr="" encap="vlan-100" instrImedcy="immediate" mode="regular" tDn="topology/pod-1/protpaths-101-102/pathep-[policyGrpVPC_FPCORE]"/>


<!—EPG Static Binding FI_A-->

<fvRsPathAtt descr="" encap="vlan-100" instrImedcy="immediate" mode="regular" tDn="topology/pod-1/protpaths-101-102/pathep-[policyGrpVPC_FI_A]"/>


<!—EPG Static Binding FI_B-->

<fvRsPathAtt descr="" encap="vlan-100" instrImedcy="immediate" mode="regular" tDn="topology/pod-1/protpaths-101-102/pathep-[policyGrpVPC_FI_B]"/>


<fvRsDomAtt encap="unknown" instrImedcy="immediate" resImedcy="immediate" tDn="uni/phys-phyDomain_Scenario1"/>


<fvRsBd tnFvBDName="BD100"/>

<fvRsProv tnVzBrCPName="FP_Out_Permit_Any"/>

</fvAEPg>


In this section, you will define the contract provided by the Outside EPG (FP_Out_Permit_Any) to allow communications with workloads connected to the internal EPGs.

Table 25: Scenario 1 EPG Outside Provider Contract

Provider Contract

Configuration

Description

Name

FP_Out_Permit_Any

Contract to allow connectivity outside of the fabric


To configure a contract association, log in to the APIC GUI with administrator privileges and follow the path below:

Tenants à [Scenario1] à Application Profileà [AP_OUT] à Application EPGs à [Outside] à Contracts à [Create Add Provided Contract]

Figure 77: Scenario 1 EPG Outside Provider Contract


XML 33: Scenario 1 EPG Outside Provider Contract


<fvAEPg name="Outside">


<!—EPG Provider Contract association-->

<fvRsProv tnVzBrCPName="FP_Out_Permit_Any"/>

</fvAEPg>


The final step needed in order to allow traffic to flow between the Outside EPG and the previously defined internal EPGs (WebAppOne, WebAppTwo and WebAppThree) in and out of the EPGs is to consume a VZANY contract.

Consuming the contract under the private network [VRF100] allows the configuration to consume the contract on behalf of all EPGs associated with that VRF, as opposed to consuming the same contract for each EPG.

Figure 78: Scenario 1 Consuming vzANY


To configure a vzANY contract association, log in to the APIC GUI with administrator privileges and follow the path below:

Tenant à [Scenario1] à Private Network à [VRF100] EPG Collection for Context.

Figure 79: Scenario 1 vzANY


XML 34: Scenario 1 vzANY


<fvCtx name="VRF100">


<!—vzANY Contract-->

<vzAny descr="" matchT="AtleastOne" name="">


<!—vzANY Provide-->

<vzRsAnyToCons prio="unspecified" tnVzBrCPName="FP_Out_Permit_Any"/>


<!—vzANY Consume-->

<vzRsAnyToCons prio="unspecified" tnVzBrCPName="L3OUT_Permit_Any"/>

</vzAny>

</fvCtx>


Migration Phase – Scenario 1

Now that Layer 2 connectivity is established between the ACI fabric and the FabricPath environment for VLAN 100, as well as Layer 3 connectivity from ACI to the DC core routers (DCCORE01/02), VM integration to vCenter is complete, and the contracts are in place. It is time to start migrating application VMs from the FabricPath environment into the ACI fabric.

Figure 80: Scenario 1 Migration Phase


Applications within this scenario are currently deployed in a single FabricPath VLAN (VLAN 100), that is, the Web Server (1), Web Server (2), Web Server (3), and so on, currently reside in the same address space within the single VLAN. The intent of the migration is to provide separation of services, that is, provide logical separation of applications, between each of the web server environments. Note: Although the FabricPath topology is used as part of the migration efforts described herein, STP, vPC, or other topologies could leverage the overall strategy and process.

The migration plan includes the following steps that are detailed in the upcoming sections:

1. Premigration Validation: the intent of this step is to ensure that the current environment including applications is behaving as intended and will include confirmation of various connectivity checks.

2. Application Migration: this step will be accomplished by migrating the application within vCenter from the ESXi host connected to the FabricPath network to the ESXi host connected to the ACI fabric using vMotion.

3. Port Group Migration: this step involves migrating the host VM VMNIC from the standard DVS port group to the ACI-managed DVS port group.

4. Gateway Migration: this step includes migrating the gateway and Layer 3 functionalities from the FabricPath domain to the ACI fabric.

5. Continue Server Migration: within this step of the migration, additional server migration efforts continue until the point where all applications/servers have been migrated from the FabricPath domain to the ACI fabric.

The following are some important premigration assumptions:

· All virtual hosts attached to the vCenter-managed DVS are using the FabricPath spines as their Layer 3 gateway. Layer 2 communication between the VMs and the default gateway is achieved by stretching the Layer 2 broadcast domain across the FP network and by trunking VLAN 100 from the FP leaf devices down to the UCS chassis where the ESXi host resides.

· All virtual hosts attached to the vCenter-managed DVS exit the data center via Layer 3 connectivity through the DCCORE switches by learning external IP prefixes via OSPF control plane.

· The ESXi host connected to the FabricPath domain has uplinks connected to the vCenter-managed DVS (dvSwitchFabricPath).

· The ESXi host connected to the ACI fabric has uplinks connected to the vCenter-managed DVS (dvSwitchFabricPath) and to the ACI-managed DVS (ACI_VMM).

· All VMs are using shared storage (iSCSI), which is available for both the ESXi hosts in the FabricPath and ACI environments. This is what allows live vMotions to occur.

· Layer 2 connectivity from the FabricPath domain to the ACI fabric is successfully established via the Layer 2 vPC logical connection.

· The same Layer 2 broadcast domain is extended from the FabricPath network to the ACI fabric and allows Layer 2 connectivity between VMs deployed on the ESXi hosts connected to the FabricPath and ACI domains.

The following diagram shows the application host VM, AppOneWeb, which will be the initial host VM migrated from the FabricPath domain to the ACI fabric.

Figure 81: Scenario 1 AppOneWeb VM


The following diagram depicts the entire topology including the Layer 2 connectivity. AppOneWeb is currently located on the ESXi host in the FabricPath environment and connected to the VLAN100 port group on the vCenter-managed DVS.

Figure 82: Scenario 1 Premigration Topology


The initial validation step includes the following connectivity test from the host VM, AppOneWeb. The first validation test ensures that the correct interface on the VM has the required IP address and ARP entries. Connectivity confirmation via ping and traceroute allow for gateway and core reachability path and response test.

Figure 83: Scenario 1 Premigration Validation – AppOneWeb IP Address

IFCONFIG::


cisco@AppOneWeb:~$ ifconfig eth0

eth0 Link encap:Ethernet HWaddr 00:50:56:82:bd:bf

inet addr:100.1.1.101 Bcast:100.1.1.255 Mask:255.255.255.0

inet6 addr: fe80::250:56ff:fe82:bdbf/64 Scope:Link

UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1

RX packets:25 errors:0 dropped:3 overruns:0 frame:0

TX packets:75 errors:0 dropped:0 overruns:0 carrier:0

collisions:0 txqueuelen:1000

RX bytes:2204 (2.2 KB) TX bytes:10778 (10.7 KB)


cisco@AppOneWeb:~$


Figure 84: Scenario 1 Premigration Validation – AppOneWeb ARP Cache

ARP –A:


cisco@AppOneWeb:~$ arp -a

? (100.1.1.1) at 00:00:0c:07:ac:64 [ether] on eth0

? (100.1.1.102) at 00:50:56:82:d4:69 [ether] on eth0

? (100.1.1.103) at 00:50:56:82:d5:05 [ether] on eth0

cisco@AppOneWeb:~$

cisco@AppOneWeb:~$

In the following example, note that AppOneWeb can ping its GW (100.1.1.1) and the two other AppServers in VLAN 100 (100.1.1.102 and 100.1.1.103, respectively).

Figure 85: Scenario 1 Premigration Validation – Ping tests

PING::


cisco@AppOneWeb:~$ ping 100.1.1.1 -c 1

PING 100.1.1.1 (100.1.1.1) 56(84) bytes of data.

64 bytes from 100.1.1.1: icmp_seq=1 ttl=63 time=0.218 ms


--- 100.1.1.1 ping statistics ---

1 packets transmitted, 1 received, 0% packet loss, time 0ms

rtt min/avg/max/mdev = 0.218/0.218/0.218/0.000 ms

cisco@AppOneWeb:~$


cisco@AppOneWeb:~$ ping 100.1.1.102 -c 1

PING 100.1.1.102 (100.1.1.102) 56(84) bytes of data.

64 bytes from 100.1.1.102: icmp_seq=1 ttl=64 time=0.285 ms


--- 100.1.1.102 ping statistics ---

1 packets transmitted, 1 received, 0% packet loss, time 0ms

rtt min/avg/max/mdev = 0.285/0.285/0.285/0.000 ms

cisco@AppOneWeb:~$


cisco@AppOneWeb:~$ ping 100.1.1.103 -c 1

PING 100.1.1.103 (100.1.1.103) 56(84) bytes of data.

64 bytes from 100.1.1.103: icmp_seq=1 ttl=64 time=0.163 ms


--- 100.1.1.103 ping statistics ---

1 packets transmitted, 1 received, 0% packet loss, time 0ms

rtt min/avg/max/mdev = 0.163/0.163/0.163/0.000 ms

cisco@AppOneWeb:~$


cisco@AppOneWeb:~$ ping 199.199.199.1 -c 1

PING 199.199.199.1 (199.199.199.1) 56(84) bytes of data.

64 bytes from 199.199.199.1: icmp_seq=1 ttl=253 time=0.577 ms


--- 199.199.199.1 ping statistics ---

1 packets transmitted, 1 received, 0% packet loss, time 0ms

rtt min/avg/max/mdev = 0.577/0.577/0.577/0.000 ms

cisco@AppOneWeb:~$


Note as follows that the traceroute to 199.199.199.1 (a loopback on DCCORE01) shows that the path goes through the FabricPath environment.

Figure 86: Scenario 1 Premigration Validation – Traceroute tests

TRACEROUTE::


cisco@AppOneWeb:~$ traceroute 100.1.1.1

traceroute to 100.1.1.1 (100.1.1.1), 30 hops max, 60 byte packets

1 100.1.1.1 (100.1.1.1) 0.774 ms 0.845 ms 0.955 ms

cisco@AppOneWeb:~$


cisco@AppOneWeb:~$ traceroute 199.199.199.1

traceroute to 199.199.199.1 (199.199.199.1), 30 hops max, 60 byte packets

1 100.1.1.1 (100.1.1.1) 0.381 ms 0.528 ms 0.683 ms

2 100.1.1.1 (100.1.1.1) 0.839 ms 1.038 ms 1.145 ms

3 199.199.199.1 (199.199.199.1) 1.183 ms 1.257 ms 1.357 ms

cisco@AppOneWeb:~$


The second validation step includes the following connectivity test from the FabricPath Cisco Nexus 7000 Switches. The MAC address and ARP entries are queried to ensure that the Cisco Nexus 7000 switches can see the AppOneWeb VM.

In the following example, note that the MAC address of AppOneWeb is 0050.5682.bdbf and 1122 is the FabricPath SwitchID of the access-layer Cisco Nexus 5600 where the ESXi hosts is connected.

Figure 87: Scenario 1 Validation – FabricPath domain

SHOW MAC ADDRESS-TABLE::


FP_Core01# show mac address-table address 0050.5682.bdbf

Note: MAC table entries displayed are getting read from software.

Use the 'hardware-age' keyword to get information related to 'Age'


Legend:

* - primary entry, G - Gateway MAC, (R) - Routed MAC, O - Overlay MAC

age - seconds since last seen,+ - primary entry using vPC Peer-Link,

(T) - True, (F) - False , ~~~ - use 'hardware-age' keyword to retrieve age info

VLAN MAC Address Type age Secure NTFY Ports/SWID.SSID.LID

---------+-----------------+--------+---------+------+----+------------------

100 0050.5682.bdbf dynamic ~~~ F F 1122.0.0


FP_Core01#


Figure 87: Scenario 1 Validation – ARP validation

SHOW IP ARP::


FP_Core01# show ip arp 100.1.1.101


Flags: * - Adjacencies learnt on non-active FHRP router

+ - Adjacencies synced via CFSoE

# - Adjacencies Throttled for Glean

D - Static Adjacencies attached to down interface


IP ARP Table

Total number of entries: 1

Address Age MAC Address Interface

100.1.1.101 00:00:04 0050.5682.bdbf Vlan100

FP_Core01#


Figure 88: Scenario 1 Validation – Routing Table validation

SHOW IP ROUTE::


FP_Core01# show ip route 100.1.1.0/24

IP Route Table for VRF "default"

'*' denotes best ucast next-hop

'**' denotes best mcast next-hop

'[x/y]' denotes [preference/metric]

'%<string>' in via output denotes VRF <string>


100.1.1.0/24, ubest/mbest: 1/0, attached

*via 100.1.1.2, Vlan100, [0/0], 00:01:16, direct

FP_Core01#


The third validation step includes the following connectivity test from the ACI fabric. The endpoint connectivity and the route validation are used to confirm the endpoint discovery process and correct routing entry. Note that you can expect to see nothing from the ACI perspective during this test. The VM is on the FabricPath-attached ESXi host, and the gateway for VLAN 100 still resides in the FabricPath environment.


Figure 89: Scenario 1 Validation – ACI Fabric

SHOW ENDPOINT::


Leaf1# show endpoint ip 100.1.1.101

Legend:

O - peer-attached H - vtep a - locally-aged S - static

V - vpc-attached p - peer-aged L - local M - span

s - static-arp B - bounce

+-------------------+---------------+-----------------+--------------+-------------+

VLAN/ Encap MAC Address MAC Info/ Interface

Domain VLAN IP Address IP Info

+-------------------+---------------+-----------------+--------------+-------------+


Leaf1#


Figure 90: Scenario 1 Validation – Routing table Validation

SHOW IP ROUTE::


Leaf1# show ip route vrf Scenario1:VRF100 100.1.1.0/24

IP Route Table for VRF "Scenario1:VRF100"

'*' denotes best ucast next-hop

'**' denotes best mcast next-hop

'[x/y]' denotes [preference/metric]

'%<string>' in via output denotes VRF <string>


100.1.1.0/24, ubest/mbest: 1/0, attached, direct, pervasive

*via 10.0.40.65%overlay-1, [1/0], 6d04h, static

recursive next hop: 10.0.40.65/32%overlay-1

Leaf1#


Following the premigration steps, the next step consists of performing a live migration (vMotion) of the AppOneWeb VM from the ESXi server in the FabricPath environment to the ESXi server in the ACI environment. This step requires a vCenter administrator to initiate a ‘migrate’ virtual machine event.

Figure 91: Scenario 1 Host Migration


Following the live migration event, the virtual machine will leverage the Layer 2 connectivity established via the Layer 2 vPC logical connection to communicate to the other VMs still connected to the FP network and to its default gateway still deployed to the FP spine devices.

To validate the vMotion of the virtual machine and the continue communication with the infrastructure, start a continuous ping from AppOneWeb to the gateway. During the host migration, document any pack lost or abnormality. As shown below, only two ICMP packets have been lost during the live migration process.

Figure 92: Scenario 1 Host Migration Ping


cisco@AppOneWeb:~$ ping 100.1.1.1

PING 100.1.1.1 (100.1.1.1) 56(84) bytes of data.

64 bytes from 100.1.1.1: icmp_seq=1 ttl=255 time=0.480 ms

64 bytes from 100.1.1.1: icmp_seq=2 ttl=255 time=0.494 ms

64 bytes from 100.1.1.1: icmp_seq=3 ttl=255 time=0.507 ms

64 bytes from 100.1.1.1: icmp_seq=4 ttl=255 time=0.500 ms

64 bytes from 100.1.1.1: icmp_seq=5 ttl=255 time=0.505 ms

64 bytes from 100.1.1.1: icmp_seq=6 ttl=255 time=0.571 ms

64 bytes from 100.1.1.1: icmp_seq=7 ttl=255 time=0.495 ms

64 bytes from 100.1.1.1: icmp_seq=8 ttl=255 time=0.557 ms

64 bytes from 100.1.1.1: icmp_seq=11 ttl=255 time=1.16 ms

64 bytes from 100.1.1.1: icmp_seq=12 ttl=255 time=1.08 ms

64 bytes from 100.1.1.1: icmp_seq=13 ttl=255 time=1.10 ms

^C

--- 100.1.1.1 ping statistics ---

17 packets transmitted, 15 received, 11% packet loss, time 16023ms

rtt min/avg/max/mdev = 0.480/0.800/1.261/0.310 ms

cisco@AppOneWeb:~$


AppOneWeb VM is now on the standard DVS on the ESXi host in the ACI fabric environment with gateway reachability remaining on the FabricPath Spine.

Figure 93: Scenario 1 Migrate Host from FabricPath Domain to ACI Fabric


The first validation step includes the following connectivity test from the virtual host, AppOneWeb, to ensure network access.

In the following figure, note that the interface on the VM has the required IP address and ARP entries. Connectivity confirmation via ping and traceroute allows for gateway and core reachability path and response test.

Figure 94: Scenario 1 Migrate Host to ACI – AppOneWeb

IFCONFIG::


cisco@AppOneWeb:~$ ifconfig eth0

eth0 Link encap:Ethernet HWaddr 00:50:56:82:bd:bf

inet addr:100.1.1.101 Bcast:100.1.1.255 Mask:255.255.255.0

inet6 addr: fe80::250:56ff:fe82:bdbf/64 Scope:Link

UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1

RX packets:25 errors:0 dropped:3 overruns:0 frame:0

TX packets:75 errors:0 dropped:0 overruns:0 carrier:0

collisions:0 txqueuelen:1000

RX bytes:2204 (2.2 KB) TX bytes:10778 (10.7 KB)


cisco@AppOneWeb:~$


Figure 95: Scenario 1 Migrate Host to ACI – AppOneWeb ARP Validation

ARP –A:


cisco@AppOneWeb:~$ arp –a

? (100.1.1.102) at 00:50:56:82:d4:69 [ether] on eth0

? (100.1.1.1) at 00:00:0c:07:ac:64 [ether] on eth0

? (100.1.1.103) at 00:50:56:82:d5:05 [ether] on eth0

cisco@AppOneWeb:~$


In the following example, note that you still have ICMP reachability to the gateway and all other application VMs in VLAN 100.

Figure 96: Scenario 1 Migrate Host to ACI – AppOneWeb Ping Validation

PING::


cisco@AppOneWeb:~$ ping 100.1.1.1 -c 1

PING 100.1.1.1 (100.1.1.1) 56(84) bytes of data.

64 bytes from 100.1.1.1: icmp_seq=1 ttl=255 time=1.31 ms


--- 100.1.1.1 ping statistics ---

1 packets transmitted, 1 received, 0% packet loss, time 0ms

rtt min/avg/max/mdev = 1.318/1.318/1.318/0.000 ms

cisco@AppOneWeb:~$


cisco@AppOneWeb:~$ ping 100.1.1.102 -c 1

PING 100.1.1.102 (100.1.1.102) 56(84) bytes of data.

64 bytes from 100.1.1.102: icmp_seq=1 ttl=64 time=0.346 ms


--- 100.1.1.102 ping statistics ---

1 packets transmitted, 1 received, 0% packet loss, time 0ms

rtt min/avg/max/mdev = 0.346/0.346/0.346/0.000 ms

cisco@AppOneWeb:~$


cisco@AppOneWeb:~$ ping 100.1.1.103 -c 1

PING 100.1.1.103 (100.1.1.103) 56(84) bytes of data.

64 bytes from 100.1.1.103: icmp_seq=1 ttl=64 time=0.379 ms


--- 100.1.1.103 ping statistics ---

1 packets transmitted, 1 received, 0% packet loss, time 0ms

rtt min/avg/max/mdev = 0.379/0.379/0.379/0.000 ms

cisco@AppOneWeb:~$


cisco@AppOneWeb:~$ ping 199.199.199.1 -c 1

PING 199.199.199.1 (199.199.199.1) 56(84) bytes of data.

64 bytes from 199.199.199.1: icmp_seq=1 ttl=254 time=0.467 ms


--- 199.199.199.1 ping statistics ---

1 packets transmitted, 1 received, 0% packet loss, time 0ms

rtt min/avg/max/mdev = 0.467/0.467/0.467/0.000 ms

cisco@AppOneWeb:~$


In the following example, note that the traceroute still shows that you are going through the FabricPath environment to reach the loopback (199.199.199.1) on DCCORE01.

Figure 97: Scenario 1 Migrate Host to ACI – AppOneWeb Traceroute Validation

TRACEROUTE::


cisco@AppOneWeb:~$ traceroute 100.1.1.1

traceroute to 100.1.1.1 (100.1.1.1), 30 hops max, 60 byte packets

1 100.1.1.1 (100.1.1.1) 2.315 ms 0.906 ms 3.375 ms

cisco@AppOneWeb:~$


cisco@AppOneWeb:~$ traceroute 199.199.199.1

traceroute to 199.199.199.1 (199.199.199.1), 30 hops max, 60 byte packets

1 * * 100.1.1.2 (100.1.1.2) 1.216 ms

2 * * 199.199.199.1 (199.199.199.1) 1.354 ms

cisco@AppOneWeb:~$


The second validation step includes verifying the MAC address and ARP entries to ensure the virtual server connectivity is as intended.

In the following example, note that the MAC address for AppOneWeb has moved to the Port-channel 11 that connects to the ACI environment.

Figure 98: Scenario 1 Validation – FP Fabric

SHOW MAC ADDRESS-TABLE::


FP_Core01# show mac address-table address 0050.5682.bdbf

Note: MAC table entries displayed are getting read from software.

Use the 'hardware-age' keyword to get information related to 'Age'


Legend:

* - primary entry, G - Gateway MAC, (R) - Routed MAC, O - Overlay MAC

age - seconds since last seen,+ - primary entry using vPC Peer-Link,

(T) - True, (F) - False , ~~~ - use 'hardware-age' keyword to retrieve age info

VLAN MAC Address Type age Secure NTFY Ports/SWID.SSID.LID

---------+-----------------+--------+---------+------+----+------------------

* 100 0050.5682.bdbf dynamic ~~~ F F Po11


SHOW IP ARP::


FP_Core01# show ip arp 100.1.1.101


Flags: * - Adjacencies learnt on non-active FHRP router

+ - Adjacencies synced via CFSoE

# - Adjacencies Throttled for Glean

D - Static Adjacencies attached to down interface


IP ARP Table

Total number of entries: 1

Address Age MAC Address Interface

100.1.1.101 00:00:25 0050.5682.bdbf Vlan100

FP_Core01#


Figure 99: Scenario 1 Validation – FP Fabric Routing Table

SHOW IP ROUTE::


FP_Core01# show ip route 100.1.1.0/24

IP Route Table for VRF "default"

'*' denotes best ucast next-hop

'**' denotes best mcast next-hop

'[x/y]' denotes [preference/metric]

'%<string>' in via output denotes VRF <string>


100.1.1.0/24, ubest/mbest: 1/0, attached

*via 100.1.1.2, Vlan100, [0/0], 00:03:51, direct

FP_Core01#


The third validation step includes the following connectivity test from the ACI fabric. The endpoint connectivity and the route validation are used to confirm the endpoint discovery process and correct routing entry.

In the following example, note that the ACI fabric now sees the AppOneWeb VM as an endpoint. With the show endpoint command, you can see both the IP and MAC address for the VM.

Figure 100: Scenario 1 Validation – ACI Fabric

SHOW ENDPOINT::


Leaf1# show endpoint ip 100.1.1.101

Legend:

O - peer-attached H - vtep a - locally-aged S - static

V - vpc-attached p - peer-aged L - local M - span

s - static-arp B - bounce

+-------------------+---------------+-----------------+--------------+-------------+

VLAN/ Encap MAC Address MAC Info/ Interface

Domain VLAN IP Address IP Info

+-------------------+---------------+-----------------+--------------+-------------+

22 vlan-100 0050.5682.bdbf LV po13

Scenario1:VRF100 vlan-100 100.1.1.101 LV


Leaf1#


Figure 101: Scenario 1 Validation – ACI Fabric Routing Table

SHOW IP ROUTE::


Leaf1# show ip route vrf Scenario1:VRF100 100.1.1.0/24

IP Route Table for VRF "Scenario1:VRF100"

'*' denotes best ucast next-hop

'**' denotes best mcast next-hop

'[x/y]' denotes [preference/metric]

'%<string>' in via output denotes VRF <string>


100.1.1.0/24, ubest/mbest: 1/0, attached, direct, pervasive

*via 10.0.40.65%overlay-1, [1/0], 01w07d, static

recursive next hop: 10.0.40.65/32%overlay-1

Leaf1#


Now that you have the AppOneWeb VM on the ESXi host in the ACI fabric, the next step in the migration involves migrating the application AppOneWeb from the standard DVS to the ACI-managed DVS. This step requires a vCenter administrator to edit the settings of the virtual machine to modify the VMNIC association.

Following the port group migration, the gateway remains within the FabricPath domain.

Move the VMNIC from the standard DVS to the ACI-managed DVS.

Figure 102: Scenario 1 vCenter Port Group Migration


To validate the port group change of the virtual machine and the continued communication with the infrastructure, start a continuous ping from AppOneWeb to the gateway. During the host port group association change, document any packet loss or abnormality.

In the following example, note that you can still ping the default gateway (still the FabricPath Spine) and no connectivity is lost while migrating the VMNIC between port groups.

Figure 103: Scenario 1 Port Group Migration Validations


cisco@AppOneWeb:~$ ping 100.1.1.1

PING 100.1.1.1 (100.1.1.1) 56(84) bytes of data.

64 bytes from 100.1.1.1: icmp_seq=1 ttl=255 time=1.11 ms

64 bytes from 100.1.1.1: icmp_seq=2 ttl=255 time=1.54 ms

64 bytes from 100.1.1.1: icmp_seq=3 ttl=255 time=1.01 ms

64 bytes from 100.1.1.1: icmp_seq=4 ttl=255 time=1.06 ms

64 bytes from 100.1.1.1: icmp_seq=5 ttl=255 time=0.978 ms

^C

--- 100.1.1.1 ping statistics ---

24 packets transmitted, 24 received, 0% packet loss, time 23029ms

rtt min/avg/max/mdev = 0.978/1.102/1.540/0.111 ms

cisco@AppOneWeb:~$


Following the port group association update, you can now see that the host appears within the topology connected to the second ESXi host on the ACI-managed DVS and port group.

Figure 104: Scenario 1 Migrate Port Group Migration


Now that you have changed the port group for the AppOneWeb VM, you will repeat all of the verification steps.

Figure 105: Scenario 1 Migrate Host to ACI – AppOneWeb

IFCONFIG::


cisco@AppOneWeb:~$ ifconfig eth0

eth0 Link encap:Ethernet HWaddr 00:50:56:82:bd:bf

inet addr:100.1.1.101 Bcast:100.1.1.255 Mask:255.255.255.0

inet6 addr: fe80::250:56ff:fe82:bdbf/64 Scope:Link

UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1

RX packets:1335 errors:0 dropped:33 overruns:0 frame:0

TX packets:925 errors:0 dropped:0 overruns:0 carrier:0

collisions:0 txqueuelen:1000

RX bytes:94188 (94.1 KB) TX bytes:59038 (59.0 KB)


cisco@AppOneWeb:~$


Figure 106: Scenario 1 Migrate Host to ACI – AppOneWeb ARP Validation

ARP –A:


cisco@AppOneWeb:~$ arp –a

? (100.1.1.102) at 00:50:56:82:d4:69 [ether] on eth0

? (100.1.1.1) at 00:00:0c:07:ac:64 [ether] on eth0

? (100.1.1.103) at 00:50:56:82:d5:05 [ether] on eth0

cisco@AppOneWeb:~$


In the following example, note that there is still ICMP reachability to all other Application VMs in VLAN 100.

Figure 107: Scenario 1 Migrate Host to ACI – AppOneWeb Ping Validation

PING::


cisco@AppOneWeb:~$ ping 100.1.1.1 -c 1

PING 100.1.1.1 (100.1.1.1) 56(84) bytes of data.

64 bytes from 100.1.1.1: icmp_seq=1 ttl=255 time=1.20 ms


--- 100.1.1.1 ping statistics ---

1 packets transmitted, 1 received, 0% packet loss, time 0ms

rtt min/avg/max/mdev = 1.206/1.206/1.206/0.000 ms

cisco@AppOneWeb:~$


cisco@AppOneWeb:~$ ping 100.1.1.102 -c 1

PING 100.1.1.102 (100.1.1.102) 56(84) bytes of data.

64 bytes from 100.1.1.102: icmp_seq=1 ttl=64 time=0.431 ms


--- 100.1.1.102 ping statistics ---

1 packets transmitted, 1 received, 0% packet loss, time 0ms

rtt min/avg/max/mdev = 0.431/0.431/0.431/0.000 ms

cisco@AppOneWeb:~$


cisco@AppOneWeb:~$ ping 100.1.1.103 -c 1

PING 100.1.1.103 (100.1.1.103) 56(84) bytes of data.

64 bytes from 100.1.1.103: icmp_seq=1 ttl=64 time=0.325 ms


--- 100.1.1.103 ping statistics ---

1 packets transmitted, 1 received, 0% packet loss, time 0ms

rtt min/avg/max/mdev = 0.325/0.325/0.325/0.000 ms

cisco@AppOneWeb:~$


cisco@AppOneWeb:~$ ping 199.199.199.1 -c 1

ING 199.199.199.1 (199.199.199.1) 56(84) bytes of data.

64 bytes from 199.199.199.1: icmp_seq=1 ttl=254 time=0.516 ms


--- 199.199.199.1 ping statistics ---

1 packets transmitted, 1 received, 0% packet loss, time 0ms

rtt min/avg/max/mdev = 0.516/0.516/0.516/0.000 ms

cisco@AppOneWeb:~$


In the following example, note that the traceroute still shows that you are going through the FabricPath environment to reach the loopback (199.199.199.1) on DCCORE01.

Figure 108: Scenario 1 Migrate Host to ACI – AppOneWeb Traceroute Validation

TRACEROUTE::


cisco@AppOneWeb:~$ traceroute 100.1.1.1

traceroute to 100.1.1.1 (100.1.1.1), 30 hops max, 60 byte packets

1 100.1.1.1 (100.1.1.1) 2.854 ms 3.329 ms 3.397 ms

cisco@AppOneWeb:~$


cisco@AppOneWeb:~$ traceroute 199.199.199.1

traceroute to 199.199.199.1 (199.199.199.1), 30 hops max, 60 byte packets

1 100.1.1.3 (100.1.1.3) 0.675 ms 0.694 ms 100.1.1.2 (100.1.1.2) 0.879 ms

2 199.199.199.1 (199.199.199.1) 1.191 ms 1.266 ms 1.368 ms

cisco@AppOneWeb:~$


Figure 109: Scenario 1 Validation – FabricPath Domain

SHOW MAC ADDRESS-TABLE::


FP_Core01# show mac address-table address 0050.5682.bdbf

Note: MAC table entries displayed are getting read from software.

Use the 'hardware-age' keyword to get information related to 'Age'


Legend:

* - primary entry, G - Gateway MAC, (R) - Routed MAC, O - Overlay MAC

age - seconds since last seen,+ - primary entry using vPC Peer-Link,

(T) - True, (F) - False , ~~~ - use 'hardware-age' keyword to retrieve age info

VLAN MAC Address Type age Secure NTFY Ports/SWID.SSID.LID

---------+-----------------+--------+---------+------+----+------------------

* 100 0050.5682.bdbf dynamic ~~~ F F Po11


FP_Core01#


Figure 110: Scenario 1 Validation – FabricPath Domain ARP Validation

SHOW IP ARP::


FP_Core01# show ip arp 100.1.1.101


Flags: * - Adjacencies learnt on non-active FHRP router

+ - Adjacencies synced via CFSoE

# - Adjacencies Throttled for Glean

D - Static Adjacencies attached to down interface


IP ARP Table

Total number of entries: 1

Address Age MAC Address Interface

100.1.1.101 00:00:29 0050.5682.bdbf Vlan100

FP_Core01#


Figure 111: Scenario 1 Validation – FabricPath Domain Routing Table Validation

SHOW IP ROUTE::


FP_Core01# show ip route 100.1.1.0/24

IP Route Table for VRF "default"

'*' denotes best ucast next-hop

'**' denotes best mcast next-hop

'[x/y]' denotes [preference/metric]

'%<string>' in via output denotes VRF <string>


100.1.1.0/24, ubest/mbest: 1/0, attached

*via 100.1.1.2, Vlan100, [0/0], 00:07:51, direct

FP_Core01#


Figure 113: Scenario 1 Validation – ACI Domain Endpoint Table Validation

SHOW ENDPOINT::


Leaf1# show endpoint ip 100.1.1.101

Legend:

O - peer-attached H - vtep a - locally-aged S - static

V - vpc-attached p - peer-aged L - local M - span

s - static-arp B - bounce

+-------------------+---------------+-----------------+--------------+-------------+

VLAN/ Encap MAC Address MAC Info/ Interface

Domain VLAN IP Address IP Info

+-------------------+---------------+-----------------+--------------+-------------+

25 vlan-1006 0050.5682.bdbf LV po14

Scenario1:VRF100 vlan-1006 100.1.1.101 LV


Leaf1#


Figure 114: Scenario 1 Validation – ACI Fabric Routing Table Validation

SHOW IP ROUTE::


Leaf1# show ip route vrf Scenario1:VRF100 100.1.1.0/24

IP Route Table for VRF "Scenario1:VRF100"

'*' denotes best ucast next-hop

'**' denotes best mcast next-hop

'[x/y]' denotes [preference/metric]

'%<string>' in via output denotes VRF <string>


100.1.1.0/24, ubest/mbest: 1/0, attached, direct, pervasive

*via 10.0.40.65%overlay-1, [1/0], 01w07d, static

recursive next hop: 10.0.40.65/32%overlay-1

Leaf1#


Note: Now that all of the VMs are in their new ACI-managed DVS, the application “migration” is completed and it is time to move the gateway from the FabricPath environment into the ACI fabric.

Before beginning the migration, start a ping from the AppOneWeb VM to the GW address.

Note: Static routes are configured on DCCORE01/02 to reach the IP subnet associated to VLAN 100 (100.1.1.0/24) via the ACI fabric. These static routes have an admin distance of 254, which implies this static routing information won’t be leveraged until the OSPF-learned routes from the FabricPath environment are removed from the routing table. This will happen once the default gateway is disabled on the FP spines and migrated to the ACI fabric.

The following example shows the verification of connectivity during the migration process of the VLAN 100 gateway from the FabricPath domain to the ACI fabric.

Figure 112: Scenario 1 Validation – ACI Fabric


cisco@AppOneWeb:~$ ping 100.1.1.1

PING 100.1.1.1 (100.1.1.1) 56(84) bytes of data.

64 bytes from 100.1.1.1: icmp_seq=1 ttl=255 time=1.14 ms

64 bytes from 100.1.1.1: icmp_seq=2 ttl=255 time=1.10 ms

64 bytes from 100.1.1.1: icmp_seq=3 ttl=255 time=1.04 ms

64 bytes from 100.1.1.1: icmp_seq=6 ttl=63 time=0.212 ms

64 bytes from 100.1.1.1: icmp_seq=7 ttl=63 time=0.207 ms

64 bytes from 100.1.1.1: icmp_seq=8 ttl=63 time=0.201 ms

64 bytes from 100.1.1.1: icmp_seq=9 ttl=63 time=0.208 ms

64 bytes from 100.1.1.1: icmp_seq=10 ttl=63 time=0.265 ms

^C

--- 100.1.1.1 ping statistics ---

10 packets transmitted, 8 received, 20% packet loss, time 9017ms

rtt min/avg/max/mdev = 0.201/0.548/1.146/0.426 ms

cisco@AppOneWeb:~$


As noticed above, only two (2) packets are lost during the gateway migration.

The following process was used for the gateway migration.

1. Shut down (manually) the interface SVI VLAN 100 on FP_CORE1 and FP_CORE2 at the same time.

2. At about the same time, an XML post was used to configure the IP and MAC addresses of the GW in the ACI fabric, shown as follows:


Figure 116: XML Post to migrate the HSRP Gateway to the ACI Fabric BD


<fvBD arpFlood="yes" descr="" dn="uni/tn-Scenario1/BD-BD100" epMoveDetectMode="" limitIpLearnToSubnets="yes" llAddr="::" mac="00:00:0C:07:AC:64" multiDstPktAct="encap-flood" name="BD100" unicastRoute="yes" unkMacUcastAct="flood" unkMcastAct="flood"><fvRsBDToNdP tnNdIfPolName=""/><fvRsCtx tnFvCtxName="VRF100"/><fvRsIgmpsn tnIgmpSnoopPolName=""/><fvSubnet ctrl="" descr="" ip="100.1.1.254/24" name="" preferred="no" scope="public" status="deleted"/><fvSubnet ctrl="" descr="" ip="100.1.1.1/24" name="" preferred="no" scope="public"/><fvRsBdToEpRet resolveAct="resolve" tnFvEpRetPolName=""/></fvBD>


Important parts of the above XML Post:

· Configured BD100

· Changed the gateway MAC address to the SAME MAC address of the gateway previously used in the FP spines (HSRP vMAC); this ensures that VMs will not have to re-ARP for the gateway and speeds up convergence time.

· Removed the 100.1.1.254/24 address – this was used to test L3Out connectivity

· Added the 100.1.1.1 GW address

The XML post takes less than a second to reconfigure the BD across the fabric. The following is a screenshot of how to post to the fabric.

Note: This can also be achieved with Postman (for Google Chrome) or other XML or JSON programs.

Figure 113: Scenario 1 POST XML Example


All of the VMs are now on the ACI-managed DVS, and the GW is now on the ACI fabric.

Figure 114: Scenario 1 Migrate Gateway from the FabricPath Domain to ACI Fabric


Now that you have moved the gateway functionality from the FabricPath domain to the ACI fabric, you will repeat all of the verification steps.

Figure 115: Scenario 1 Migrate Host to ACI – AppOneWeb

IFCONFIG::


cisco@AppOneWeb:~$ ifconfig eth0

eth0 Link encap:Ethernet HWaddr 00:50:56:82:bd:bf

inet addr:100.1.1.101 Bcast:100.1.1.255 Mask:255.255.255.0

inet6 addr: fe80::250:56ff:fe82:bdbf/64 Scope:Link

UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1

RX packets:25 errors:0 dropped:3 overruns:0 frame:0

TX packets:75 errors:0 dropped:0 overruns:0 carrier:0

collisions:0 txqueuelen:1000

RX bytes:2204 (2.2 KB) TX bytes:10778 (10.7 KB)


cisco@AppOneWeb:~$


In the following example, note that the ARP still shows the same MAC for the default Gateway. This is because the MAC address on BD100 on the fabric was changed to mimic the vMAC address from the HSRP configuration on the old FabricPath environment.

Figure 120: Scenario 1 Migrate Host to ACI – AppOneWeb ARP Validation

ARP –A:


cisco@AppOneWeb:~$ arp –a

? (100.1.1.102) at 00:50:56:82:d4:69 [ether] on eth0

? (100.1.1.103) at 00:50:56:82:d5:05 [ether] on eth0

? (100.1.1.1) at 00:00:0c:07:ac:64 [ether] on eth0


cisco@AppOneWeb:~$


Figure 121: Scenario 1 Migrate Host to ACI – AppOneWeb Ping Validation

PING::


cisco@AppOneWeb:~$ ping 100.1.1.1 -c 1

PING 100.1.1.1 (100.1.1.1) 56(84) bytes of data.

64 bytes from 100.1.1.1: icmp_seq=1 ttl=63 time=0.218 ms


--- 100.1.1.1 ping statistics ---

1 packets transmitted, 1 received, 0% packet loss, time 0ms

rtt min/avg/max/mdev = 0.218/0.218/0.218/0.000 ms

cisco@AppOneWeb:~$


cisco@AppOneWeb:~$ ping 100.1.1.102 -c 1

PING 100.1.1.102 (100.1.1.102) 56(84) bytes of data.

64 bytes from 100.1.1.102: icmp_seq=1 ttl=64 time=0.285 ms


--- 100.1.1.102 ping statistics ---

1 packets transmitted, 1 received, 0% packet loss, time 0ms

rtt min/avg/max/mdev = 0.285/0.285/0.285/0.000 ms

cisco@AppOneWeb:~$


cisco@AppOneWeb:~$ ping 100.1.1.103 -c 1

PING 100.1.1.103 (100.1.1.103) 56(84) bytes of data.

64 bytes from 100.1.1.103: icmp_seq=1 ttl=64 time=0.163 ms


--- 100.1.1.103 ping statistics ---

1 packets transmitted, 1 received, 0% packet loss, time 0ms

rtt min/avg/max/mdev = 0.163/0.163/0.163/0.000 ms

cisco@AppOneWeb:~$


cisco@AppOneWeb:~$ ping 199.199.199.1 -c 1

PING 199.199.199.1 (199.199.199.1) 56(84) bytes of data.

64 bytes from 199.199.199.1: icmp_seq=1 ttl=253 time=0.577 ms


--- 199.199.199.1 ping statistics ---

1 packets transmitted, 1 received, 0% packet loss, time 0ms

rtt min/avg/max/mdev = 0.577/0.577/0.577/0.000 ms

cisco@AppOneWeb:~$


Figure 122: Scenario 1 Migrate Host to ACI – AppOneWeb Traceroute Validation

TRACEROUTE::


cisco@AppOneWeb:~$ traceroute 100.1.1.1

traceroute to 100.1.1.1 (100.1.1.1), 30 hops max, 60 byte packets

1 100.1.1.1 (100.1.1.1) 0.774 ms 0.845 ms 0.955 ms

cisco@AppOneWeb:~$


cisco@AppOneWeb:~$ traceroute 199.199.199.1

traceroute to 199.199.199.1 (199.199.199.1), 30 hops max, 60 byte packets

1 100.1.1.1 (100.1.1.1) 0.381 ms 0.528 ms 0.683 ms

2 100.1.1.1 (100.1.1.1) 0.839 ms 1.038 ms 1.145 ms

3 199.199.199.1 (199.199.199.1) 1.183 ms 1.257 ms 1.357 ms

cisco@AppOneWeb:~$


Figure 116: Scenario 1 Validation – FabricPath domain

SHOW MAC ADDRESS-TABLE::


FP_Core01# show mac address-table address 0050.5682.bdbf

Note: MAC table entries displayed are getting read from software.

Use the 'hardware-age' keyword to get information related to 'Age'


Legend:

* - primary entry, G - Gateway MAC, (R) - Routed MAC, O - Overlay MAC

age - seconds since last seen,+ - primary entry using vPC Peer-Link,

(T) - True, (F) - False , ~~~ - use 'hardware-age' keyword to retrieve age info

VLAN MAC Address Type age Secure NTFY Ports/SWID.SSID.LID

---------+-----------------+--------+---------+------+----+------------------

* 100 0050.5682.bdbf dynamic ~~~ F F Po11


FP_Core01#

In the figure below, note that there are no more ARP entries in the FabricPath environment. This is a consequence of having shut down the SVIs for VLAN 100 on the FP spines.

Figure 124: Scenario 1 Migrate Host to ACI – FabricPath Core ARP Validation

SHOW IP ARP::


FP_Core01# show ip arp 100.1.1.101


Flags: * - Adjacencies learnt on non-active FHRP router

+ - Adjacencies synced via CFSoE

# - Adjacencies Throttled for Glean

D - Static Adjacencies attached to down interface


IP ARP Table

Total number of entries: 0

Address Age MAC Address Interface

FP_Core01#


In the figure below, note that the routing table for FabricCore01 looks different. You can now learn the 100.1.1.0/24 network via OSPF from DCCORE01/02 (it is not a directly connected IP subnet anymore).

Figure 125: Scenario 1 Migrate Host to ACI – FabricPath Core Routing Table Validation

SHOW IP ROUTE::


FP_Core01# show ip route 100.1.1.0/24

IP Route Table for VRF "default"

'*' denotes best ucast next-hop

'**' denotes best mcast next-hop

'[x/y]' denotes [preference/metric]

'%<string>' in via output denotes VRF <string>


100.1.1.0/24, ubest/mbest: 2/0

*via 172.19.254.2, Eth3/5, [110/20], 00:00:21, ospf-3369, type-2

*via 172.19.254.6, Eth3/6, [110/20], 00:00:21, ospf-3369, type-2

FP_Core01#


Figure 126: Scenario 1 Validation – ACI Fabric

SHOW ENDPOINT::


Leaf1# show endpoint ip 100.1.1.101

Legend:

O - peer-attached H - vtep a - locally-aged S - static

V - vpc-attached p - peer-aged L - local M - span

s - static-arp B - bounce

+-------------------+---------------+-----------------+--------------+-------------+

VLAN/ Encap MAC Address MAC Info/ Interface

Domain VLAN IP Address IP Info

+-------------------+---------------+-----------------+--------------+-------------+

25 vlan-1006 0050.5682.bdbf LV po13

Scenario1:VRF100 vlan-1006 100.1.1.101 LV


Leaf1#


Figure 127: Scenario 1 Migrate Host to ACI – ACI Fabric Routing Table Validation

SHOW IP ROUTE::


Leaf1# show ip route vrf Scenario1:VRF100 100.1.1.0/24

IP Route Table for VRF "Scenario1:VRF100"

'*' denotes best ucast next-hop

'**' denotes best mcast next-hop

'[x/y]' denotes [preference/metric]

'%<string>' in via output denotes VRF <string>


100.1.1.0/24, ubest/mbest: 1/0, attached, direct, pervasive

*via 10.0.40.65%overlay-1, [1/0], 6d04h, static

recursive next hop: 10.0.40.65/32%overlay-1

Leaf1#


Fabric Optimization

The final step (once all servers have been migrated from the FabricPath environment to the ACI fabric) is to enable Optimized Layer 2 forwarding on the bridge domain.

· Optimized forwarding of unknown Layer 2 unicast packets removes the needless flooding of unknown unicast packets on the bridge domain. If the Layer 2 destination of a packet is not known to the fabric, it is discarded by the ACI spines (the ACI leaf nodes by default send the traffic to the spines when they don’t know how to reach the destination).

· Disabling ARP flooding reduces the amount of broadcast packets on the bridge domain. ARP requests are sent in unicast mode by the fabric to known destinations as opposed to flooding.

· Changing the multidestination flooding to “Flood in Encapsulation” ensures that all broadcast level packets are flooded inside of the EPG encapsulation, and not at the bridge domain level.

Note: It is important to clarify that “Flood in Encapsulation” it is not synonymous of “Flood in EPG”. As previously mentioned, it is possible to associate a different VLAN ID tag to the same EPG on separate physical interfaces (on the same leaf or across separate leaf nodes). The “Flood in Encapsulation” option ensures that traffic is flooded to all the interfaces that are mapped to the same EPG and use the same VLAN encapsulation.

Table 26: Scenario 1 Forwarding Semantics

Forwarding Semantics

Configuration

Layer 2 Unknown Unicast

Hardware Proxy

Layer 2 Unknown Multicast Flooding

Flood

Multidestination Flooding

Flood in Encapsulation

Unicast Routing

Enabled

Enforce subnet check for IP learning

Enabled

ARP Flooding

Disabled

Now any links to the FabricPath environment can be deleted. Before ACI, there were servers from an assortment of applications on the same broadcast domain (VLAN100), which caused many problems for security compliance. Now with ACI, the IP addressing can be maintained for the servers, while ensuring that they cannot talk to each other unless allowed via contracts.


Figure 128: Scenario 1 Final State after the Migration is Complete


Migration Scenario 2

After completing the discussion for the migration Scenario 1, it is now time to discuss a second use case. The main differences between Scenario 1 and Scenario 2 are the following:

· The applications that are migrated between the FP and the ACI networks are already deployed into isolated VLANs/VRFs also in the FP network. That logical isolation must be maintained when relocating them to the ACI fabric.

· FW services nodes are deployed in the FP fabric to ensure every communication between different applications belonging to separate VRFs is subject to the FW security policy enforcement. The same behavior must be maintained after migrating the applications to the ACI fabric, so a pair of FWs is connected to the fabric to control inter-VRF and north-south traffic flows.


For the rest, the design looks quite similar, including the use of vPC logical connections to establish Layer 2 and Layer 3 connectivity between the FP and ACI domains. As it has been the case for Scenario 1, the following sections discuss in detail the various configuration steps, dividing them between infrastructure and tenant configurations specific to Scenario 2.

1. Fabric Access Policy Configuration

a. Static VLAN Pool

b. Physical Domain

c. External Routed Domain that will be used to create the L3Out connections between different VRFs in the ACI fabric and the ASA FWs.

2. Tenant Configuration for Scenario 2

a. Tenant creation (named “Scenario 2”)

b. Private Networks (i.e., VRF210, VRF211 and VRF212; these VRFs will map to the appropriate VRFs in the FabricPath domain)

c. Bridge Domains (i.e., BD210, BD211, BD212; these will map to the appropriate VLANs in the FabricPath domain)

d. Application Profiles and EPGs (i.e., Application Profiles APP1 and Outside)

i. EPG Outside

ii. EPG AppOneWeb

iii. EPG AppTwoWeb

iv. EPG Middleware

e. Layer 3 Connectivity (ASA FW to DCCORE01/02)


Note: The APIC GUI screenshots shown for each configuration step in Scenario 1 will not be repeated in the following sections, as they are essentially identical. Please refer to Scenario 1 for more details.

Figure 129: Scenario 2 ACI Design


Fabric Access Policy Configuration

A static VLAN pool (vlanPool_Scenario2) needs to be created for Scenario 2 with the following encap blocks:

· “Outside_EPG” Encap Block: defines the VLAN tag that is going to be used on the Layer 2 vPC connections to the ASA outside interface and to the DC core devices (associated to the “Outside” EPG). From a Layer 3 point of view, the static routing information configured on the ASA FW points directly to the HSRP VIP address of the DC core devices as next hop, so the ACI fabric only performs Layer 2 transport functionalities in this case. This is different from what discussed in Scenario 1 where the ASA was not present and the ACI Fabric was connecting at Layer 3 to the DC core devices via the definition of a dedicated L3Out connection.

· “Layer 2” Encap Block: defines the set of VLANs used to establish Layer 2 connectivity between the FP and the ACI networks. As previously shown in Figure 129, VLANs 210, 211, and 212 are carried between the networks for establishing end-to-end Layer 2 communication.

· “Layer 3” Encap Block: defines the set of VLANs to be used to establish Layer 3 connectivity between each application (part of a dedicated VRF) and the ASA FW. A dedicated L3Out for each application will be defined for this purpose.


Table 27: Scenario 2 Static VLAN Pool

VLAN Pool

Configuration

Description

Name

vlanPool_Scenario2

-

Allocation Mode

Static Allocation

-

Encap Block

502

200-212

600-612

Outside_EPG

Layer 2

Layer 3

To configure the Static VLAN Pool, log in to the APIC GUI with administrator privileges and follow the path below:

Fabric à Access Policies à Pools à VLAN à [vlanPool_Scenario2]

Figure 130: Scenario 2 Static VLAN Pool


XML 35: Scenario 2 Static VLAN Pool


<fvnsVlanInstP allocMode="static" descr="" dn="uni/infra/vlanns-[vlanPool_Scenario2]-static" name="vlanPool_Scenario2" >

<fvnsEncapBlk allocMode="inherit" descr="" from="vlan-200" name="" to="vlan-212"/>

<fvnsEncapBlk allocMode="inherit" descr="" from="vlan-502" name="" to="vlan-502"/>

<fvnsEncapBlk allocMode="inherit" descr="" from="vlan-600" name="" to="vlan-612"/>

</fvnsVlanInstP>


As already discussed for Scenario 1, a physical domain is defined to allow connectivity from the ACI fabric to the VMs that have not yet been migrated and are still connected to the FP network, as well as to the VMs that have been migrated to the ACI fabric but are still connected to the vCenter-managed DVS.

Note: In multitenant deployments, a separate physical domain will likely be created for each tenant. This allows to granularly manage resources associated with each tenant.

Table 28: Scenario 2 Physical Domain

Physical Domain

Configuraiton

Name

phyDomain_Scenario2

VLAN Pool

vlanPool_Scenario2

To configure the physical domain, log in to the APIC GUI with administrator privileges and follow the path below:

Fabric à Access Policies à Physical and External Domains à Physical Domains à [phyDomain_Scenario2]

In this section, use the following details to create an external routed domain and associate with the VLAN pool created earlier. The VLANs specified in the encapsulation blocks previously discussed will be used for both connecting the ACI fabric to the DC core devices and to connect each internal VRF to the ASA FW.

Table 29: Scenario2 External Routed Domain

External Routed Domain

Configuration

Name

extRoutedDomain_Scenario2

VLAN Pool

vlanPool_Scenario2

To configure the external routed domain, log in to the APIC GUI with administrator privileges and follow the path below:

Fabric à Access Policies à Physical and External Domains à External Routed Domain à [extRoutedDomain_Scenario2]

Tenant Configuration

Note: The use of the Quick Start guide is not used in order to demonstrate the object relationship for the configuration parameters. Additionally, while Quick Start menus can change from version to version, the method of configuration displayed in this document will not change.

A second tenant {Scenario2) is created in this case. Notice that the different applications migrated to the ACI fabric will be part of their own dedicated VRF. However, all the VRFs are considered part of the same tenant.

Table 30: Scenario 2 Tenant

Tenant

Configuration

Name

Scenario2


Note: A tenant represents a unit of isolation from a policy perspective and can represent a customer, an organization, or domain in an enterprise setting, or just a convenient grouping of policies.

To configure the Tenant, log in to the APIC GUI with administrator privileges and follow the path below:

Tenants à ADD TENANT à [Create Tenant Scenario2]

For Scenario 2, you will need to configure a total of four private networks (VRFs). The first three VRFs will match the existing FabricPath environment (VRF210, VRF211, and VRF212), and the final VRF will be used to provide Layer 3 connectivity between the FW and the DC core switches.

Table 31: Scenario 2 Private Networks

Private Network

Configuraiton

VRF210

Unenforced

VRF211

Unenforced

VRF212

Unenforced

Outside

Unenforced

Differently from Scenario 1, you will be selecting “unenforced” policy control for each defined private network. This essentially turns the ACI fabric into a router, without the need to define Contracts to establish communication between EPGs. You have selected this configuration for Scenario 2 because by design all inter-VRF communication is subject to the security enforcement applied by the stateful ASA FW.

To configure the private network, log in to the APIC GUI with administrator privileges and follow the path below:

Tenants à [Scenario2] à Networking à Private Networks à [Create VRF210]

Tenants à [Scenario2] à Networking à Private Networks à [Create VRF211]

Tenants à [Scenario2] à Networking à Private Networks à [Create VRF212]

Tenants à [Scenario2] à Networking à Private Networks à [Create Outside]

Different bridge domains need to be created in Scenario 2 to extend the Layer 2 broadcast domains between the FP and the ACI networks. This is because of the design choice of mapping each VLANs used in the FP network to a dedicated BD and EPG on the ACI fabric (VLAN = EPG = BD).

All the bridge domains are characterized by the same configuration parameters shown in the tables below and already discussed when creating the bridge domain for the previously discussed Scenario 1.

Table 32: Scenario 2 Bridge Domain BD210

Bridge Domain

Configuration

Name

BD210

Private Network

Scenario2/VRF210

Layer 2 Unknown Unicast

Flood

Layer 2 Unknown Multicast Flooding

Flood

Multi Destination Flooding

Flood within BD

Unicast Routing

Enabled

ARP Flooding

Enabled

Enforce subnet check for IP learning

Enabled


Table 33: Scenario 2 Bridge Domain BD211

Bridge Domain

Configuration

Name

BD211

Private Network

Scenario2/VRF211

Layer 2 Unknown Unicast

Flood

Layer 2 Unknown Multicast Flooding

Flood

Multi Destination Flooding

Flood within BD

Unicast Routing

Enabled

ARP Flooding

Enabled

Enforce subnet check for IP learning

Enabled


Table 34: Scenario 2 Bridge Domain BD212

Bridge Domain

Configuration

Name

BD212

Private Network

Scenario2/VRF212

Layer 2 Unknown Unicast

Flood

Layer 2 Unknown Multicast Flooding

Flood

Multi Destination Flooding

Flood within BD

Unicast Routing

Enabled

ARP Flooding

Enabled

Enforce subnet check for IP learning

Enabled


To configure the Bridge Domains, log in to the APIC GUI with administrator privileges and follow the path below:

Tenants à [Scenario2] à Networking à Bridge Domains à [Create BD210]

Tenants à [Scenario2] à Networking à Bridge Domains à [Create BD211]

Tenants à [Scenario2] à Networking à Bridge Domains à [Create BD212]


Each bridge domain will be associated to a dedicated private network (VRF) and route traffic for its IP subnet, as highlighted in the tables below.

Table 35: Scenario 2 Bridge Domain Subnet

Bridge Domain Subnet

Configuration

Description

Subnet

210.1.1.254/24

Pervasive Gateway

Scope

Public

-


Table 36: Scenario 2 Bridge Domain Subnet

Bridge Domain Subnet

Configuration

Description

Subnet

211.1.1.254/24

Pervasive Gateway

Scope

Public

-


Table 37: Scenario 2 Bridge Domain Subnet

Bridge Domain Subnet

Configuration

Description

Subnet

212.1.1.254/24

Pervasive Gateway

Scope

Public

-


Notice how the IP address associated to each Bridge Domain (.254) is temporary used until the default gateway is migrated from the FP spines to the ACI fabric.

To configure the bridge domain subnet, log in to the APIC GUI with administrator privileges and follow the path below:

Tenants à [Scenario2] à Networking à Bridge Domains à [BD210] à Subnets

Tenants à [Scenario2] à Networking à Bridge Domains à [BD211] à Subnets

Tenants à [Scenario2] à Networking à Bridge Domains à [BD212] à Subnets

No contracts are needed for this scenario because you have selected “un-enforced” policy on all private networks (VRFs).

For Layer 3 connectivity to the outside world, each private network (VRF) will have a separate L3Out connection to a stateful ASA firewall, as previously shown in Figure 129. All communication between the VRFs on the ACI fabric will have to leave the fabric on a given L3Out connection, be inspected by the firewall, and then return through a second L3Out connection.

Requirements:

· Each VRF on the ACI fabric will have a 0.0.0.0/0 route from each border leaf pointing to each respective FW GW address for that VRF.

· The FW will have static routes pointing to the internal ACI subnets for the respective VRFs.

· The FW will also use a 0.0.0.0/0 route pointing to the HSRP VIP address on the DC core devices to establish communication with the external Layer 3 domain.

In order to provide a resilient solution, the deployment of a pair of ASA FWs is recommended, working in Active/Standby mode. This implies that the ACI fabric needs to connect via vPC logical connections to both ASA nodes, as shown in figure below. This requires a specific L3Out configuration discussed below.

Figure 131: ACI Fabric Connecting to an Active/Standby ASA FW Pair


In order to complete the configuration of the required L3Outs, the following steps are required:

1. Configure L3Out Properties

2. Configure Logical Node Profiles

3. Configure Logical Interface Profiles

4. Configure L3Out EPG parameters

Note: As previously mentioned, three L3Out connections are required to interconnect each VRF to the SA FW. The following sections provide the configuration required for one of them, since the other two would be similar.

In this section the L3Out connection, named “L3OUT_AppOneWeb”, will be created. The L3Out will define the network details for reaching networks outside of the fabric. The configuration details for this L3Out connection are described in Table 38


AppOneWeb

Table 38: Scenario 2 L3OUT_AppOneWeb Properties

L3OUT

Configruation

Description

Name

L3OUT_AppOneWeb

-

Private Network

Scenario2/VRF210

Associate the L3Out with the proper Private Network

External Routed Domain

extRoutedDomain_Scenario2

Associate the L3Out with the proper External Routed Domain (this is a domain that contains Vlans, which can be used by the L3Out for connectivity (SVI-based, etc.).


To configure the external routed network, log in to the APIC GUI with administrator privileges and follow the path below:

Tenants à [Scenario2] à Networking à External Routed Networks à [Create L3OUT_AppOneWeb]

Repeat the steps above for both AppTwoWeb and Middleware using VRF211 and VRF212 respectively.

In this section, the external routed network node profile will be created. The node profile will define fabric nodes that will participate in the L3Out connectivity and provide the static route. A single logical node profile will be created, including the two physical border leaf nodes and the static route that will be added for each node. The static route will point to the IP address of the ASA interface used for this specific VRF.

Table 39: Scenario 2 Node Profile

Node Profile

Configuration

Description

Name

Leaf1_2_Node_Profile

-

Node ID

topology/pod-1/node-101

-

Router ID

100.100.100.100

(This needs to be a unique IP address which is NOT in use). The ACI fabric will automatically create a loopback on the associated border leaf with this IP.

Static Route

0.0.0.0/0

100.110.0.4

Node ID

topology/pod-1/node-102

-

Router ID

200.200.200.200

(This needs to be a unique IP address which is NOT in use). The ACI Fabric will automatically create a loopback on the associated border leaf with this IP.

Static Route

0.0.0.0/0

100.110.0.4


To configure the node profiles, log in to the APIC GUI with administrator privileges and follow the path below:

Tenants à [Scenario2] à Networking à External Routed Networks à [L3OUT_AppOneWeb] à Logical Node Profiles à [Create Node_Profile]

As previously mentioned, two vPC logical connections are defined in this Scenario 2 to ensure connectivity between the ACI fabric and the ASA FW nodes (Active and Standby). Both vPC connections must be associated to the same L3Out connection, as they will both be used depending which FW node is running as active.

Table 40: Scenario 2 Interface Profile 1

Interface Profile

Configuration

Description

Name

Leaf1_2_Int_Profile_1

-

Interface Type

SVI

-

Path Type

Virtual Port Channel

-

Path

Node101-102/policyGrpVPC_ASA_IN_1

vPC to the Active ASA Node

Encap

Vlan-610

-

Site A IP Address

100.110.0.2/24

-

Site A Secondary IP Address

100.110.0.1/24

The secondary IP address for each Site A MUST match Site B

Site B IP Address

100.110.0.3/24

-

Site B Secondary IP Address

100.110.0.1/24

The secondary IP address for each Site A MUST match Site B

MTU

9000

By default the fabric will “inherit” the system MTU, which is 9000. It is considered best practice to manually set the fabric MTU on your interface profile to match the device on the other side.


Table 41: Scenario 2 Interface Profile 2

Interface Profile

Configuration

Description

Name

Leaf1_2_Int_Profile_2

-

Interface Type

SVI

-

Path Type

Virtual Port Channel

-

Path

Node101-102/policyGrpVPC_ASA_IN_2

vPC to the Standby ASA Node

Encap

Vlan-610

-

Site A IP Address

100.110.0.2/24

-

Site A Secondary IP Address

100.110.0.1/24

The secondary IP address for each Site A MUST match Site B

Site B IP Address

100.110.0.3/24

-

Site B Secondary IP Address

100.110.0.1/24

The secondary IP address for each Site A MUST match Site B

MTU

9000

By default the fabric will “inherit” the system MTU, which is 9000. It is considered best practice to manually set the fabric MTU on your interface profile to match the router on the other side.


To configure the interface profiles, log in to the APIC GUI with administrator privileges and follow the path below:

Tenants à [Scenario2] à Networking à External Routed Networks à [L3OUT_AppOneWeb] à Logical Node Profiles à [Leaf1_2_Node_Profile] à Logical Interface Profiles à [Create Interface_Profiles]

The L3Out External Network (aka External EPG) configured define the external IP prefixes capable of accessing fabric resources within the tenant. External EPGs are defined using IP prefix and mask. More than one external EPGs may be configured, depending if different policies need to be applied to these external EPGs (IP prefixes).

Contracts will be needed to allow communication to occur between internal EPGs on the Private Network (VRF) and the external EPGs associated to the L3Out. Without contract, all connectivity from outside is blocked and external routes will not be learnt when using a dynamic routing protocol.

Table 42: Scenario 2 External EPG

EPG

Configuration

Description

Name

L3EPG

-

Subnet

0.0.0.0/0

Defines the external subnets/networks that will be allowed to communicate with resources internal to the fabric (assuming a contract will be added as well).

Scope

Security Import Subnet

The use of the scope and subnet field allows for the control of traffic coming into the fabric. The field has other functions that are not required to support the use case discussed.


To configure the external EPG, log in to the APIC GUI with administrator privileges and follow the path below:

Tenants à [Scenario2] à Networking à External Routed Networks à [L3OUT_AppOneWeb] à Networks à [L3EPG]

In this section under the contracts tab for the L3EPG, provide the L3Out contract (the Permit any any), which was previously created, to ensure successful connectivity between the fabric and the external Layer 3 network domain.

Table 43: Scenario 2 External EPG Provider Contract

Provider Contracts

Configuration

Description

Name

L3OUT_Permit_Any

-


To configure the contract filter association, login to the APIC GUI with administrator privileges and follow the path below:

Tenants à [Scenario2] à Networking à External Routed Networks à [L3OUT_AppOneWeb] à Networks à [L3EPG]

Under the contracts tab for the L3EPG, provide the L3Out contract (the Permit any any), which was previously created.

Integration Phase – Scenario 2

Next is the Integration Phase. Now that the ACI fabric has been staged, you are going to begin the configuration sections in ACI where you will be establishing connectivity to the FabricPath environment via the vPCs.

Figure 132: Scenario 2 Integration Phase


For Scenario 2, you will create two application profiles; one called APP1, which will house the EPGs where to migrate the endpoints initially connected to the FP VLANs, and one called Outside, which act as a Layer 2 only EPG for establishing Layer 3 connectivity between the FW and the DC core routers.

Figure 133: Scenario 2 Application Profile


For Scenario 2, application profile APP1 will contain the EPGs for the newly managed applications migrated from the FabricPath domain.

Table 44: Scenario 2 Application Profile APP1

Application Profile

Configuration

Description

Name

APP1

-


To configure the application profile, log in to the APIC GUI with administrator privileges and follow the path below:

Tenants à [Scenario2] à Application Profile à [APP1]

In this section, use the information in the table below to create the endpoint group AppOneWeb within the previously created application profile.

Table 45: Scenario 2 EPG AppOneWeb

EPG

Bridge Domain

Domain

AppOneWeb

BD210

phyDomain_Scenario2 and VMware/ACI_VMM


Note that the EPG AppOneWeb (and this applies also to the other EPGs discussed in the following sections) is associated to both previously created Physical Domain (phyDomain_Scenario2) and VMM Domain (ACI_VMM). This is required because the workloads migrated to the ACI fabric will be first connected to the vCenter managed DVS (hence seen as physical hosts in ACI) and successively moved to the ACI-managed DVS (therefore becoming part of the VMM domain).

To configure the EPG AppOneWeb, log in to the APIC GUI with administrator privileges and follow the path below:

Tenants à [Scenario2] à Application Profile à [APP1] à Application EPGs à [AppOneWeb]

In this section, use the information in the table below to create the endpoint group AppTwoWeb within the previously created application profile.

Table 46: Scenario 2 EPG AppTwoWeb

EPG

Bridge Domain

Domain

AppTwoWeb

BD211

phyDomain_Scenario2 and VMware/ACI_VMM


To configure the EPG AppOneWeb, log in to the APIC GUI with administrator privileges and follow the path below:

Tenants à [Scenario2] à Application Profile à [APP1] à Application EPGs à [AppTwoWeb]

In this section, use the information in the table below to create the endpoint group Middleware within the previously created application profile.

Table 47: Scenario 2 EPG Middleware

EPG

Bridge Domain

Domain

Middleware

BD212

phyDomain_Scenario2 and VMware/ACI_VMM


To configure the EPG AppOneWeb, log in to the APIC GUI with administrator privileges and follow the path below:

Tenants à [Scenario2] à Application Profile à [APP1] à Application EPGs à [MiddleWare]

In this section, use the information in the following table to create the static bindings for the APP1 EPGs. The static bindings would serve two purposes:

· Providing L2 connectivity to the endpoints that are still connected to the FabricPath network. For this to happen, the static binding will be performed with the Layer 2 vPC logical connection to the FP spines.

· Connecting endpoints to the previously created AppOneWeb, AppTwoWeb, and Middleware EPGs. Those endpoints are VMs still connected to the vSphere managed DVS but migrated on the ESXi hosts part of the UCSB-Mini chassis connected to the ACI leaf nodes via FIs.

Table 48: Scenario 1 APP1 EPGs Statis Bindings

EPG

Path

Encap

AppOneWeb

Node-101-102/policyGrpVPC_FI_A

vlan-210

AppOneWeb

Node-101-102/policyGrpVPC_FI_B

vlan-210

AppOneWeb

Node-101-102/policyGrpVPC_FPCORE

vlan-210

AppTwoWeb

Node-101-102/policyGrpVPC_FI_A

vlan-211

AppTwoWeb

Node-101-102/policyGrpVPC_FI_B

vlan-211

AppTwoWeb

Node-101-102/policyGrpVPC_FPCORE

vlan-211

Middleware

Node-101-102/policyGrpVPC_FI_A

vlan-212

Middleware

Node-101-102/policyGrpVPC_FI_B

vlan-212

Middleware

Node-101-102/policyGrpVPC_FPCORE

vlan-212

To configure Static Bindings, log in to the APIC GUI with administrator privileges and follow the path below:

Tenants à [Scenario2] à Application Profileà [APP1] à Application EPGs à [AppOneWeb] à Static Bindings

Tenants à [Scenario2] à Application Profileà [APP1] à Application EPGs à [AppTwoWeb] à Static Bindings

Tenants à [Scenario2] à Application Profileà [APP1] à Application EPGs à [Middleware] à Static Bindings

For Scenario 2, the application profile Outside is used to provide Layer 2 connectivity between the FW and the DC core devices.

Table 48: Scenario 2 Application Profile Outside

Application Profile

Configruation

Description

Name

Outside

-


To configure the Application Profile, log in to the APIC GUI with administrator privileges and follow the path below:

Tenants à [Scenario2] à Application Profile à [Outside]

In this section, use the information in the table below to create the endpoing group Outside within the previously created application profile.

Table 49: Scenario 2 EPG Outside

EPG

Bridge Domain

Domain

Outside

Outside

phyDomain_Scenario2


The EPG Outside is only associated to the Physical Domain as it is used to connect the FW and the DC core devices that represent physical endpoints.

To configure the EPG AppOneWeb, log in to the APIC GUI with administrator privileges and follow the path below:

Tenants à [Scenario2] à Application Profile à [Outside] à Application EPGs à [Outside]

In this section, use the information in the table below to create the static bindings for the EPG Outside. The static bindings will allow connecting endpoints to the previously created EPG Outside. Those endpoints are the ASA FW nodes (their outside interfaces) and the DC Core devices. As previously mentioned, the Outside EPG provides the Layer 2 connectivity allowing L3 communication between the ASA FWs and the DC core.

Table 50: Scenario 1 EPG Outside Static Bindings

EPG

Path

Encap

Outside

Node-101-102/policyGrpVPC_ASA_OUT

vlan-502

Outside

Node-101-102/policyGrpVPC_DCCORE

vlan-502


To configure Static Bindings, log in to the APIC GUI with administrator privileges and follow the path below:

Tenants à [Scenario2] à Application Profileà [AP_Outside] à Application EPGs à [Outside] à Static Bindings


Migration Phase – Scenario 2

Now that Layer 2 and Layer 3 connectivity has been established between the ACI fabric and the FabricPath environment and the VM integration to vCenter is complete, it is time to start migrating the application endpoints from the FabricPath environment into the ACI fabric.

Figure 117: Scenario 2 Migration Phase


Applications within Scenario 2 are initially deployed in separate VLANs and associated routing tables (VRFs) in the FabricPath environment. The intent of the migration is to demonstrate a network-centric migration, where you map VRFs to ACI private networks and VLANs to EPGs / BDs.

Note: Although this topology is utilized as part of the migration efforts described herein, STP, vPC, or other topologies could leverage the overall strategy and process.

The migration plan includes the following steps detailed in the upcoming sections:

1. Premigration Validation: the intent of this step is to ensure that the current environment including applications is behaving as intended and will include confirmation of various connectivity checks.

2. Application Migration: this step will be accomplished by migrating the application within vCenter from the ESXi host connected to the FabricPath network to the ESXi host connected to the ACI Fabric using vMotion.

3. Port Group Migration: this step involves migrating the host VM vmnic from the vCenter managed DVS port group to the ACI managed DVS port group.

4. Gateway Migration: this step includes migrating the gateway and Layer 3 functionalities from the FabricPath domain to the ACI fabric.

5. Continue Server Migration: within this step of the migration, additional server migration efforts continue until the point where all applications/servers have been migrated from the FabricPath domain to the ACI fabric.


Figure 118: Scenario 2 Initial State in the FabricPath Domain


The following are some important premigration assumptions:

· All virtual hosts attached to the vCenter managed DVS are using the FabricPath Spines as their Layer 3 gateway. Layer 2 communication between the VMs and the default gateway is achieved by stretching the Layer 2 broadcast domains across the FP network and by trunking VLANs 210-212 from the FP leaf devices down to the UCS chassis where the ESXi host resides.

· All virtual hosts attached to the vCenter managed DVS exit the datacenter via static routing to the ASA FW and ultimately to the DC core switches.

· The ESXi host connected to the FabricPath domain has uplinks connected to the vCenter managed DVS (dvSwitchFabricPath)

· The ESXi host connected to the ACI Fabric has uplinks connected both to the vCenter managed DVS (dvSwitchFabricPath) and to the ACI managed DVS (ACI_VMM).

· All VMs are using shared storage (iSCSI), which is available for both the ESXi hosts in the FabricPath and ACI environments. This is what allows live vMotions to occur.

· Layer 2 connectivity from the FabricPath domain to the ACI fabric is successfully established via the Layer 2 vPC logical connection.

· The same Layer 2 broadcast domains are extended from the FabricPath network to the ACI fabric and allows Layer 2 connectivity between VMs deployed on the ESXi hosts connected to the FabricPath and ACI domains.

The figure below shows the application VM S2-AppOneWeb, which will be the first VM migrated from the FabricPath domain to the ACI fabric.

Figure 136: Scenario 2 AppOneWeb VM


The figure below depicts the entire topology including the Layer 2 connectivity. AppOneWeb is initially connected to the vCenter managed DVS on the ESXi host in the FabricPath environment. Step 1 will consists in performing live migration (vMotion) for this host from the ESXi server in the FabricPath environment to the ESXi server in the ACI environment.

Figure 119: Scenario 2 Premigration Topology


The initial validation step includes the following connectivity test from the host VM, S2-AppOneWeb. The first validation test ensures the correct interface on the VM has the required IP address and ARP entries. Connectivity confirmation via ping and traceroute allows for gateway and core reachability path and response test.

Figure 120: Scenario 2 Premigration Validation – AppOneWeb

IFCONFIG::


cisco@S2-AppOneWeb:~$ ifconfig eth0

eth0 Link encap:Ethernet HWaddr 00:50:56:9e:2a:60

inet addr:210.1.1.201 Bcast:210.1.1.255 Mask:255.255.255.0

inet6 addr: fe80::250:56ff:fe9e:2a60/64 Scope:Link

UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1

RX packets:186 errors:0 dropped:0 overruns:0 frame:0

TX packets:314 errors:0 dropped:0 overruns:0 carrier:0

collisions:0 txqueuelen:1000

RX bytes:12382 (12.3 KB) TX bytes:26071 (26.0 KB)


cisco@S2-AppOneWeb:~$


Figure 139: Scenario 2 Premigration Validation – AppOneWeb ARP Validation

ARP –A:


cisco@S2-AppOneWeb:~$ arp -a

? (210.1.1.2) at 38:ed:18:a2:f1:42 [ether] on eth0

? (210.1.1.1) at 00:00:0c:07:ac:d2 [ether] on eth0

? (210.1.1.3) at 38:ed:18:a2:f3:c2 [ether] on eth0

cisco@S2-AppOneWeb:~$


In the figure below, note that S2-AppOneWeb can ping its GW (210.1.1.1) as well as the loopback interface on DCCORE01 (199.199.199.1).

Figure 140: Scenario 2 Premigration Validation – AppOneWeb Ping Validation

PING::


cisco@S2-AppOneWeb:~$ ping 210.1.1.1 -c 1

PING 210.1.1.1 (210.1.1.1) 56(84) bytes of data.

64 bytes from 210.1.1.1: icmp_seq=1 ttl=255 time=1.04 ms


--- 210.1.1.1 ping statistics ---

1 packets transmitted, 1 received, 0% packet loss, time 0ms

rtt min/avg/max/mdev = 1.047/1.047/1.047/0.000 ms

cisco@S2-AppOneWeb:~$


cisco@S2-AppOneWeb:~$ ping 199.199.199.1 -c 1

PING 199.199.199.1 (199.199.199.1) 56(84) bytes of data.

64 bytes from 199.199.199.1: icmp_seq=1 ttl=254 time=0.808 ms


--- 199.199.199.1 ping statistics ---

1 packets transmitted, 1 received, 0% packet loss, time 0ms

rtt min/avg/max/mdev = 0.808/0.808/0.808/0.000 ms

cisco@S2-AppOneWeb:~$


Figure 141: Scenario 2 Premigration Validation – AppOneWeb Traceroute Validation

TRACEROUTE::


cisco@S2-AppOneWeb:~$ traceroute 210.1.1.1

traceroute to 210.1.1.1 (210.1.1.1), 30 hops max, 60 byte packets

1 210.1.1.1 (210.1.1.1) 3.327 ms 1.593 ms 1.692 ms

cisco@S2-AppOneWeb:~$


cisco@S2-AppOneWeb:~$ traceroute 199.199.199.1

traceroute to 199.199.199.1 (199.199.199.1), 30 hops max, 60 byte packets

1 210.1.1.3 (210.1.1.3) 0.620 ms 0.639 ms 210.1.1.2 (210.1.1.2) 0.997 ms

2 199.199.199.1 (199.199.199.1) 1.262 ms 2.905 ms 1.514 ms

cisco@S2-AppOneWeb:~$


The second validation step includes the following connectivity test from the FabricPath Cisco Nexus 7000 Switches. The MAC address and ARP entries are queried to ensure that the Cisco Nexus 7000 switches can see the S2-AppOneWeb VM.

In the figure below, note that the MAC address of AppOneWeb is 0050.569e.2a60 and 1122 is the FabricPath SwitchID of the access-layer Cisco Nexus 5600 to which the ESXi host is connected.

Figure 121: Scenario 2 Validation – FabricPath Domain

SHOW MAC ADDRESS-TABLE::


FP_Core01# show mac address-table address 00:50:56:9e:2a:60

Note: MAC table entries displayed are getting read from software.

Use the 'hardware-age' keyword to get information related to 'Age'


Legend:

* - primary entry, G - Gateway MAC, (R) - Routed MAC, O - Overlay MAC

age - seconds since last seen,+ - primary entry using vPC Peer-Link,

(T) - True, (F) - False , ~~~ - use 'hardware-age' keyword to retrieve age info

VLAN MAC Address Type age Secure NTFY Ports/SWID.SSID.LID

---------+-----------------+--------+---------+------+----+------------------

210 0050.569e.2a60 dynamic ~~~ F F 1122.0.0

FP_Core01#


Figure 143: Scenario 2 Validation – FabricPath Domain ARP Validation

SHOW IP ARP::


FP_Core01# show ip arp 210.1.1.201 vrf vrf210


Flags: * - Adjacencies learnt on non-active FHRP router

+ - Adjacencies synced via CFSoE

# - Adjacencies Throttled for Glean

D - Static Adjacencies attached to down interface


IP ARP Table

Total number of entries: 1

Address Age MAC Address Interface

210.1.1.201 00:01:38 0050.569e.2a60 Vlan210

FP_Core01#


Figure 144: Scenario 2 Validation – FabricPath Domain Routing Table Validation

SHOW IP ROUTE::


FP_Core01# show ip route 210.1.1.0 vrf vrf210

IP Route Table for VRF "vrf210"

'*' denotes best ucast next-hop

'**' denotes best mcast next-hop

'[x/y]' denotes [preference/metric]

'%<string>' in via output denotes VRF <string>


210.1.1.0/24, ubest/mbest: 1/0, attached

*via 210.1.1.2, Vlan210, [0/0], 02:45:58, direct

FP_Core01#


The third validation step includes the following connectivity test from the Brownfield. The endpoint connectivity and the route validation are used to confirm the endpoint discovery process and correct routing entry.

Note that you expect to see nothing from the ACI perspective during this test. The VM is still on the FabricPath attached ESXi host, and the gateway for VLAN 210 still resides in the FabricPath environment.


Figure 122: Scenario 2 Validation – ACI Fabric

SHOW ENDPOINT::


Leaf1# show endpoint ip 2100.1.1.201

Legend:

O - peer-attached H - vtep a - locally-aged S - static

V - vpc-attached p - peer-aged L - local M - span

s - static-arp B - bounce

+-------------------+---------------+-----------------+--------------+-------------+

VLAN/ Encap MAC Address MAC Info/ Interface

Domain VLAN IP Address IP Info

+-------------------+---------------+-----------------+--------------+-------------+


Leaf1#


Figure 146: Scenario 2 Validation – ACI FabricaPath Routing Table Validation

SHOW IP ROUTE::


Leaf1# show ip route vrf Scenario2:VRF210 210.1.1.0/24

IP Route Table for VRF "Scenario2:VRF210"

'*' denotes best ucast next-hop

'**' denotes best mcast next-hop

'[x/y]' denotes [preference/metric]

'%<string>' in via output denotes VRF <string>


210.1.1.0/24, ubest/mbest: 1/0, attached, direct, pervasive

*via 10.0.40.65%overlay-1, [1/0], 01w10d, static

recursive next hop: 10.0.40.65/32%overlay-1

Leaf1#


Now that you have established where the VM (S2-AppOneWeb) is, and where its gateway is (FabricPath spine), it’s time to send it into the ESXi host in the ACI fabric with a live vMotion.

Following the vMotion event you can now see in the following figure that the host has been migrated to the ESXi host connected to the ACI fabric.

Figure 123: Scenario 2 Host Migration


Figure 124: Scenario 2 vMotion Ping

Right before you initiate the vMotion from vCenter, you begin a ping to the gateway on the FabricPath spine. Two (2) packets are dropped during the vMotion.


cisco@S2-AppOneWeb:~$ ping 210.1.1.1

PING 210.1.1.1 (210.1.1.1) 56(84) bytes of data.

64 bytes from 210.1.1.1: icmp_seq=1 ttl=255 time=1.12 ms

64 bytes from 210.1.1.1: icmp_seq=2 ttl=255 time=1.19 ms

64 bytes from 210.1.1.1: icmp_seq=3 ttl=255 time=1.14 ms

64 bytes from 210.1.1.1: icmp_seq=4 ttl=255 time=1.16 ms

64 bytes from 210.1.1.1: icmp_seq=5 ttl=255 time=1.17 ms

64 bytes from 210.1.1.1: icmp_seq=8 ttl=255 time=1.17 ms

64 bytes from 210.1.1.1: icmp_seq=9 ttl=255 time=1.13 ms

64 bytes from 210.1.1.1: icmp_seq=10 ttl=255 time=1.20 ms

64 bytes from 210.1.1.1: icmp_seq=11 ttl=255 time=1.10 ms

64 bytes from 210.1.1.1: icmp_seq=12 ttl=255 time=1.35 ms

^C

--- 210.1.1.1 ping statistics ---

12 packets transmitted, 10 received, 16% packet loss, time 11023ms

rtt min/avg/max/mdev = 1.106/1.177/1.352/0.080 ms

cisco@S2-AppOneWeb:~$


S2-AppOneWeb VM is now on the vCenter-managed DVS on the ESXi host in the ACI fabric environment. Its default gateway remains on the FabricPath spine, so the Layer 2 vPC connection is used every time the VM wants to communicate to an entity outside its IP subnet.

Figure 125: Scenario 2 Migrate Host from FabricPath Domain to ACI Fabric


Now that the S2-AppOneWeb VM has been vMotioned, the verification steps will be repeated.

Figure 126: Scenario 2 Migrate Host to ACI – AppOneWeb

IFCONFIG::


cisco@S2-AppOneWeb:~$ ifconfig eth0

eth0 Link encap:Ethernet HWaddr 00:50:56:9e:2a:60

inet addr:210.1.1.201 Bcast:210.1.1.255 Mask:255.255.255.0

inet6 addr: fe80::250:56ff:fe9e:2a60/64 Scope:Link

UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1

RX packets:2030 errors:0 dropped:28 overruns:0 frame:0

TX packets:1204 errors:0 dropped:0 overruns:0 carrier:0

collisions:0 txqueuelen:1000

RX bytes:126766 (126.7 KB) TX bytes:68723 (68.7 KB)


cisco@S2-AppOneWeb:~$


Figure 151: Scenario 2 Migrate Host to ACI – AppOneWeb ARP Validation


ARP –A:


cisco@S2-AppOneWeb:~$ arp –a

? (210.1.1.2) at 38:ed:18:a2:f1:42 [ether] on eth0

? (210.1.1.1) at 00:00:0c:07:ac:d2 [ether] on eth0

? (210.1.1.3) at 38:ed:18:a2:f3:c2 [ether] on eth0

cisco@S2-AppOneWeb:~$


In the figure below, note that S2-AppOneWeb can ping its GW (210.1.1.1) as well as the loopback on DCCORE01 (199.199.199.1).

Figure 152: Scenario 2 Migrate Host to ACI – AppOneWeb Ping Validation

PING::


cisco@S2-AppOneWeb:~$ ping 210.1.1.1 -c 1

PING 210.1.1.1 (210.1.1.1) 56(84) bytes of data.

64 bytes from 210.1.1.1: icmp_seq=1 ttl=255 time=0.559 ms


--- 210.1.1.1 ping statistics ---

1 packets transmitted, 1 received, 0% packet loss, time 0ms

rtt min/avg/max/mdev = 0.559/0.559/0.559/0.000 ms

cisco@S2-AppOneWeb:~$


--- 212.1.1.201 ping statistics ---

1 packets transmitted, 1 received, 0% packet loss, time 0ms

rtt min/avg/max/mdev = 0.621/0.621/0.621/0.000 ms

cisco@S2-AppOneWeb:~$

cisco@S2-AppOneWeb:~$ ping 199.199.199.1 -c 1

PING 199.199.199.1 (199.199.199.1) 56(84) bytes of data.

64 bytes from 199.199.199.1: icmp_seq=1 ttl=254 time=0.727 ms


--- 199.199.199.1 ping statistics ---

1 packets transmitted, 1 received, 0% packet loss, time 0ms

rtt min/avg/max/mdev = 0.727/0.727/0.727/0.000 ms

cisco@S2-AppOneWeb:~$

In the figure below, note that traceroute still shows that you are going through the FabricPath environment to reach the loopback (199.199.199.1) on DCCORE01

Figure 153: Scenario 2 Migrate Host to ACI – AppOneWeb Traceroute Validation

TRACEROUTE::


cisco@S2-AppOneWeb:~$ traceroute 210.1.1.1

traceroute to 210.1.1.1 (210.1.1.1), 30 hops max, 60 byte packets

1 210.1.1.1 (210.1.1.1) 1.279 ms 4.170 ms 4.217 ms

cisco@S2-AppOneWeb:~$


cisco@S2-AppOneWeb:~$ traceroute 199.199.199.1

traceroute to 199.199.199.1 (199.199.199.1), 30 hops max, 60 byte packets

1 210.1.1.3 (210.1.1.3) 0.620 ms 0.639 ms 210.1.1.2 (210.1.1.2) 0.997 ms

2 199.199.199.1 (199.199.199.1) 1.262 ms 2.905 ms 1.514 ms

cisco@S2-AppOneWeb:~$


Figure 154: Scenario 2 Migrate Host to ACI – FP_Core Mac-address table Validation

SHOW MAC ADDRESS-TABLE::


FP_Core01# show mac address-table address 00:50:56:9e:2a:60

Note: MAC table entries displayed are getting read from software.

Use the 'hardware-age' keyword to get information related to 'Age'


Legend:

* - primary entry, G - Gateway MAC, (R) - Routed MAC, O - Overlay MAC

age - seconds since last seen,+ - primary entry using vPC Peer-Link,

(T) - True, (F) - False , ~~~ - use 'hardware-age' keyword to retrieve age info

VLAN MAC Address Type age Secure NTFY Ports/SWID.SSID.LID

---------+-----------------+--------+---------+------+----+------------------

* 210 0050.569e.2a60 dynamic ~~~ F F Po11


FP_Core01#


Figure 155: Scenario 2 Migrate Host to ACI – FP_Core ARP Validation

SHOW IP ARP::


FP_Core01# show ip arp 210.1.1.201 vrf vrf210


Flags: * - Adjacencies learnt on non-active FHRP router

+ - Adjacencies synced via CFSoE

# - Adjacencies Throttled for Glean

D - Static Adjacencies attached to down interface


IP ARP Table

Total number of entries: 1

Address Age MAC Address Interface

210.1.1.201 00:02:19 0050.569e.2a60 Vlan210

FP_Core01#


Figure 156: Scenario 2 Migrate Host to ACI – FP_Core Routing Table Validation

SHOW IP ROUTE::


FP_Core01# show ip route 210.1.1.0 vrf vrf210

IP Route Table for VRF "vrf210"

'*' denotes best ucast next-hop

'**' denotes best mcast next-hop

'[x/y]' denotes [preference/metric]

'%<string>' in via output denotes VRF <string>


210.1.1.0/24, ubest/mbest: 1/0, attached

*via 210.1.1.2, Vlan210, [0/0], 1d21h, direct

FP_Core01#


Figure 157: Scenario 2 Migrate Host to ACI – ACI Fabric Endpoint Table Validation

SHOW ENDPOINT::


Leaf1# show endpoint ip 210.1.1.201

Legend:

O - peer-attached H - vtep a - locally-aged S - static

V - vpc-attached p - peer-aged L - local M - span

s - static-arp B - bounce

+-------------------+---------------+-----------------+--------------+-------------+

VLAN/ Encap MAC Address MAC Info/ Interface

Domain VLAN IP Address IP Info

+-------------------+---------------+-----------------+--------------+-------------+

10 vlan-210 0050.569e.2a60 LV po13

Scenario2:VRF210 vlan-210 210.1.1.201 LV


Leaf1#


Figure 158: Scenario 2 Migrate Host to ACI – ACI Fabric Routing Table Validation

SHOW IP ROUTE::


Leaf1# show ip route vrf Scenario2:VRF210 210.1.1.0/24

IP Route Table for VRF "Scenario2:VRF210"

'*' denotes best ucast next-hop

'**' denotes best mcast next-hop

'[x/y]' denotes [preference/metric]

'%<string>' in via output denotes VRF <string>


210.1.1.0/24, ubest/mbest: 1/0, attached, direct, pervasive

*via 10.0.40.65%overlay-1, [1/0], 01w12d, static

recursive next hop: 10.0.40.65/32%overlay-1

Leaf1#


Now that you have the S2-AppOneWeb VM on the ESXi host in the ACI fabric, you are going to move the S2-AppOneWeb VMNIC from the VLAN210 port group (vCenter-managed DVS) to the Scenario2|APP1|ApOneWeb port group on the ACI-managed DVS. Both of these port groups are part of the same Layer 2 broadcast domain (mapped to the same EPG AppOneWeb part of BD210).

Move the VMNIC from the vSphere-managed DVS to the ACI-managed DVS.

Figure 159: Scenario 2 vCenter VMNIC Move


To validate the port group change for the virtual machine and the continued communication with the infrastructure, start a continuous ping from AppOneWeb to its gateway. During the VM port group association change, document any packet loss or abnormality.

In the figure below, note that after the port group change, you can still ping the default gateway (still deployed on the FabricPath spine devices).

Figure 160: Scenario 2 Port Group Migration Validations


cisco@S2-AppOneWeb:~$ ping 210.1.1.1

PING 210.1.1.1 (210.1.1.1) 56(84) bytes of data.

64 bytes from 210.1.1.1: icmp_seq=1 ttl=255 time=0.676 ms

64 bytes from 210.1.1.1: icmp_seq=2 ttl=255 time=0.549 ms

64 bytes from 210.1.1.1: icmp_seq=3 ttl=255 time=0.516 ms

64 bytes from 210.1.1.1: icmp_seq=4 ttl=255 time=0.531 ms

64 bytes from 210.1.1.1: icmp_seq=5 ttl=255 time=0.595 ms

64 bytes from 210.1.1.1: icmp_seq=6 ttl=255 time=0.638 ms

64 bytes from 210.1.1.1: icmp_seq=7 ttl=255 time=0.582 ms

64 bytes from 210.1.1.1: icmp_seq=8 ttl=255 time=0.600 ms

64 bytes from 210.1.1.1: icmp_seq=9 ttl=255 time=0.636 ms

64 bytes from 210.1.1.1: icmp_seq=10 ttl=255 time=0.601 ms

64 bytes from 210.1.1.1: icmp_seq=11 ttl=255 time=0.617 ms

64 bytes from 210.1.1.1: icmp_seq=12 ttl=255 time=0.647 ms

64 bytes from 210.1.1.1: icmp_seq=13 ttl=255 time=0.602 ms

64 bytes from 210.1.1.1: icmp_seq=14 ttl=255 time=0.586 ms

64 bytes from 210.1.1.1: icmp_seq=15 ttl=255 time=0.590 ms

64 bytes from 210.1.1.1: icmp_seq=16 ttl=255 time=0.573 ms

64 bytes from 210.1.1.1: icmp_seq=17 ttl=255 time=0.627 ms

64 bytes from 210.1.1.1: icmp_seq=19 ttl=255 time=1.12 ms

64 bytes from 210.1.1.1: icmp_seq=20 ttl=255 time=1.14 ms

64 bytes from 210.1.1.1: icmp_seq=21 ttl=255 time=1.11 ms

64 bytes from 210.1.1.1: icmp_seq=22 ttl=255 time=1.19 ms

64 bytes from 210.1.1.1: icmp_seq=23 ttl=255 time=1.14 ms

^C

--- 210.1.1.1 ping statistics ---

23 packets transmitted, 22 received, 4% packet loss, time 22005ms

rtt min/avg/max/mdev = 0.516/0.722/1.198/0.232 ms

cisco@S2-AppOneWeb:~$


Figure 161: Scenario 2 Migrate VMNIC from the FabricPath DVS to ACI-Managed DVS


Now that you have changed the port group for the S2-AppOneWeb VM, you will repeat all of the verification steps.

Figure 162: Scenario 2 Migrate Host to ACI – AppOneWeb

IFCONFIG::


cisco@S2-AppOneWeb:~$ ifconfig eth0

eth0 Link encap:Ethernet HWaddr 00:50:56:9e:2a:60

inet addr:210.1.1.201 Bcast:210.1.1.255 Mask:255.255.255.0

inet6 addr: fe80::250:56ff:fe9e:2a60/64 Scope:Link

UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1

RX packets:2030 errors:0 dropped:28 overruns:0 frame:0

TX packets:1204 errors:0 dropped:0 overruns:0 carrier:0

collisions:0 txqueuelen:1000

RX bytes:126766 (126.7 KB) TX bytes:68723 (68.7 KB)


cisco@S2-AppOneWeb:~$


Figure 163: Scenario 2 Migrate Host to ACI – AppOneWeb ARP Validation

ARP –A:


cisco@S2-AppOneWeb:~$ arp –a

? (210.1.1.2) at 38:ed:18:a2:f1:42 [ether] on eth0

? (210.1.1.1) at 00:00:0c:07:ac:d2 [ether] on eth0

? (210.1.1.3) at 38:ed:18:a2:f3:c2 [ether] on eth0

cisco@S2-AppOneWeb:~$


In the figure below, note that S2-AppOneWeb can ping its gateway (210.1.1.1) as well as the loopback on DCCORE01 (199.199.199.1).

Figure 164: Scenario 2 Migrate Host to ACI – AppOneWeb Ping Validation

PING::


cisco@S2-AppOneWeb:~$ ping 210.1.1.1 -c 1

PING 210.1.1.1 (210.1.1.1) 56(84) bytes of data.

64 bytes from 210.1.1.1: icmp_seq=1 ttl=255 time=1.70 ms


--- 210.1.1.1 ping statistics ---

1 packets transmitted, 1 received, 0% packet loss, time 0ms

rtt min/avg/max/mdev = 1.700/1.700/1.700/0.000 ms

cisco@S2-AppOneWeb:~$


cisco@S2-AppOneWeb:~$ ping 199.199.199.1 -c 1

PING 199.199.199.1 (199.199.199.1) 56(84) bytes of data.

64 bytes from 199.199.199.1: icmp_seq=1 ttl=254 time=0.775 ms


--- 199.199.199.1 ping statistics ---

1 packets transmitted, 1 received, 0% packet loss, time 0ms

rtt min/avg/max/mdev = 0.775/0.775/0.775/0.000 ms

cisco@S2-AppOneWeb:~$


In the figure below, note that traceroute still shows that you are going through the FabricPath environment to reach the loopback (199.199.199.1) on DCCORE01.


Figure 165: Scenario 2 Migrate Host to ACI – AppOneWeb Traceroute Validation

TRACEROUTE::


cisco@S2-AppOneWeb:~$ traceroute 210.1.1.1

traceroute to 210.1.1.1 (210.1.1.1), 30 hops max, 60 byte packets

1 210.1.1.1 (210.1.1.1) 3.945 ms 4.387 ms 4.473 ms

cisco@S2-AppOneWeb:~$


cisco@S2-AppOneWeb:~$ traceroute 199.199.199.1

traceroute to 199.199.199.1 (199.199.199.1), 30 hops max, 60 byte packets

1 210.1.1.2 (210.1.1.2) 0.754 ms 210.1.1.3 (210.1.1.3) 0.598 ms 210.1.1.2 (210.1.1.2) 0.760 ms

2 199.199.199.1 (199.199.199.1) 1.566 ms 3.465 ms 3.673 ms

cisco@S2-AppOneWeb:~$


Figure 166: Scenario 2 Validation – FabricPath domain

SHOW MAC ADDRESS-TABLE::


FP_Core01# show mac address-table address 00:50:56:9e:2a:60

Note: MAC table entries displayed are getting read from software.

Use the 'hardware-age' keyword to get information related to 'Age'


Legend:

* - primary entry, G - Gateway MAC, (R) - Routed MAC, O - Overlay MAC

age - seconds since last seen,+ - primary entry using vPC Peer-Link,

(T) - True, (F) - False , ~~~ - use 'hardware-age' keyword to retrieve age info

VLAN MAC Address Type age Secure NTFY Ports/SWID.SSID.LID

---------+-----------------+--------+---------+------+----+------------------

* 210 0050.569e.2a60 dynamic ~~~ F F Po11


FP_Core01#


Figure 167: Scenario 2 Validation – FabricPath domain ARP Validation

SHOW IP ARP::


FP_Core01# show ip arp 210.1.1.201 vrf vrf210


Flags: * - Adjacencies learnt on non-active FHRP router

+ - Adjacencies synced via CFSoE

# - Adjacencies Throttled for Glean

D - Static Adjacencies attached to down interface


IP ARP Table

Total number of entries: 1

Address Age MAC Address Interface

210.1.1.201 00:09:32 0050.569e.2a60 Vlan210

FP_Core01#


Figure 168: Scenario 2 Validation – FabricPath domain Routing Table Validation

SHOW IP ROUTE::


FP_Core01# show ip route 210.1.1.0 vrf vrf210

IP Route Table for VRF "vrf210"

'*' denotes best ucast next-hop

'**' denotes best mcast next-hop

'[x/y]' denotes [preference/metric]

'%<string>' in via output denotes VRF <string>


210.1.1.0/24, ubest/mbest: 1/0, attached

*via 210.1.1.2, Vlan210, [0/0], 1d23h, direct

FP_Core01#


Figure 169: Scenario 2 Validation – ACI Fabric Endpoint Table Validation

SHOW ENDPOINT::


Leaf1# show endpoint ip 210.1.1.201

Legend:

O - peer-attached H - vtep a - locally-aged S - static

V - vpc-attached p - peer-aged L - local M - span

s - static-arp B - bounce

+-------------------+---------------+-----------------+--------------+-------------+

VLAN/ Encap MAC Address MAC Info/ Interface

Domain VLAN IP Address IP Info

+-------------------+---------------+-----------------+--------------+-------------+

42 vlan-1007 0050.569e.2a60 LV po13

Scenario2:VRF210 vlan-1007 210.1.1.201 LV


Leaf1#

Figure 170: Scenario 2 Validation – ACI Fabric Routing Table Validation

SHOW IP ROUTE::


Leaf1# show ip route vrf Scenario2:VRF210 210.1.1.0/24

IP Route Table for VRF "Scenario2:VRF210"

'*' denotes best ucast next-hop

'**' denotes best mcast next-hop

'[x/y]' denotes [preference/metric]

'%<string>' in via output denotes VRF <string>


210.1.1.0/24, ubest/mbest: 1/0, attached, direct, pervasive

*via 10.0.40.65%overlay-1, [1/0], 00:08:56, static

recursive next hop: 10.0.40.65/32%overlay-1

Leaf1#

Now that all of the VMs have been relocated in their new ACI-managed DVS, the default gateway can be moved from the FabricPath environment into the ACI fabric.

Figure 171: Scenario 2 – Hosts and Clusters


Before starting the migration, start a ping from the S2-AppOneWeb VM to the gateway address. After that, migrate the VLAN 210 gateway from the FabricPath domain to the ACI fabric (into BD210).

Figure 172: Scenario 2 Validation – ACI Fabric


cisco@S2-AppOneWeb:~$ ping 210.1.1.1

PING 210.1.1.1 (210.1.1.1) 56(84) bytes of data.

64 bytes from 210.1.1.1: icmp_seq=1 ttl=255 time=0.676 ms

64 bytes from 210.1.1.1: icmp_seq=2 ttl=255 time=0.549 ms

64 bytes from 210.1.1.1: icmp_seq=3 ttl=255 time=0.516 ms

64 bytes from 210.1.1.1: icmp_seq=4 ttl=255 time=0.531 ms

64 bytes from 210.1.1.1: icmp_seq=5 ttl=255 time=0.595 ms

64 bytes from 210.1.1.1: icmp_seq=6 ttl=255 time=0.638 ms

64 bytes from 210.1.1.1: icmp_seq=7 ttl=255 time=0.582 ms

64 bytes from 210.1.1.1: icmp_seq=8 ttl=255 time=0.600 ms

64 bytes from 210.1.1.1: icmp_seq=9 ttl=255 time=0.636 ms

64 bytes from 210.1.1.1: icmp_seq=10 ttl=255 time=0.601 ms

64 bytes from 210.1.1.1: icmp_seq=11 ttl=255 time=0.617 ms

64 bytes from 210.1.1.1: icmp_seq=12 ttl=255 time=0.647 ms

64 bytes from 210.1.1.1: icmp_seq=13 ttl=255 time=0.602 ms

64 bytes from 210.1.1.1: icmp_seq=14 ttl=255 time=0.586 ms

64 bytes from 210.1.1.1: icmp_seq=15 ttl=255 time=0.590 ms

64 bytes from 210.1.1.1: icmp_seq=16 ttl=255 time=0.573 ms

64 bytes from 210.1.1.1: icmp_seq=17 ttl=255 time=0.627 ms

64 bytes from 210.1.1.1: icmp_seq=19 ttl=255 time=1.12 ms

64 bytes from 210.1.1.1: icmp_seq=20 ttl=255 time=1.14 ms

64 bytes from 210.1.1.1: icmp_seq=21 ttl=255 time=1.11 ms

64 bytes from 210.1.1.1: icmp_seq=22 ttl=255 time=1.19 ms

64 bytes from 210.1.1.1: icmp_seq=23 ttl=255 time=1.14 ms

^C

--- 210.1.1.1 ping statistics ---

23 packets transmitted, 22 received, 4% packet loss, time 22005ms

rtt min/avg/max/mdev = 0.516/0.722/1.198/0.232 ms

cisco@S2-AppOneWeb:~$


As seen above, one packet is dropped during the gateway migration.

The following process was used for the gateway migration:

1. Shut down (manually) the SVI interfaces for VLAN 210 on FP_CORE1 and FP_CORE2 at the same time.

2. Add static routes on DCCORE01/02 to the appropriate updated interfaces pointing to the new ASA firewalls connected to the ACI fabric for VLAN 210 (and later for VLANs 211-212).

3. At about the same time, leverage an XML post to configure the default gateway MAC and IP address in the ACI fabric.


Figure 173: XML Post to migrate the HSRP Gateway to the ACI Fabric BD


<fvBD arpFlood="yes" descr="" dn="uni/tn-Scenario2/BD-BD210" epMoveDetectMode="" limitIpLearnToSubnets="yes" llAddr="::" mac="00:00:0c:07:ac:d2" multiDstPktAct="encap-flood" name="BD100" unicastRoute="yes" unkMacUcastAct="flood" unkMcastAct="flood"><fvRsBDToNdP tnNdIfPolName=""/><fvRsCtx tnFvCtxName="VRF210"/><fvRsIgmpsn tnIgmpSnoopPolName=""/><fvSubnet ctrl="" descr="" ip="210.1.1.254/24" name="" preferred="no" scope="public" status="deleted"/><fvSubnet ctrl="" descr="" ip="210.1.1.1/24" name="" preferred="no" scope="public"/><fvRsBdToEpRet resolveAct="resolve" tnFvEpRetPolName=""/></fvBD>


Important parts of the above XML Post:

· Configured BD210

· Changed his MAC address to the SAME MAC address of the gateway on the FP spines (HSRP vMAC). This ensures that hosts will not have to re-ARP for the gateway and speeds up convergence time.

· Removed the 210.1.1.254/24 address – this was initially used to test the L3Out connectivity.

· Added the 210.1.1.1 GW address.

The XML post takes less than a second to reconfigure the BD across the fabric. Below is a screenshot of how to post to the fabric. This can also be achieved with Postman (for Google Chrome) or other XML or JSON programs.


Figure 174: Scenario 2 Example POST


All the VMs are now on the ACI-managed DVS, and their default gateway is now on the ACI fabric.


Figure 175: Scenario 2 Migrate Default Gateway from the FabricPath Domain to ACI Fabric


Figure 176: Scenario 2 Migrate Host to ACI – AppOneWeb

IFCONFIG::


cisco@S2-AppOneWeb:~$ ifconfig eth0

eth0 Link encap:Ethernet HWaddr 00:50:56:9e:2a:60

inet addr:210.1.1.201 Bcast:210.1.1.255 Mask:255.255.255.0

inet6 addr: fe80::250:56ff:fe9e:2a60/64 Scope:Link

UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1

RX packets:2030 errors:0 dropped:28 overruns:0 frame:0

TX packets:1204 errors:0 dropped:0 overruns:0 carrier:0

collisions:0 txqueuelen:1000

RX bytes:126766 (126.7 KB) TX bytes:68723 (68.7 KB)


cisco@S2-AppOneWeb:~$


In the figure below, note that ARP still shows the same MAC for the default gateway. This is because the MAC address on BD210 on the fabric was changed to match the HSRP vMAC address on the old FabricPath environment.


Figure 177: Scenario 2 Migrate Host to ACI – AppOneWeb ARP Validation

ARP –A:


cisco@S2-AppOneWeb:~$ arp –a

? (210.1.1.2) at 38:ed:18:a2:f1:42 [ether] on eth0

? (210.1.1.1) at 00:00:0c:07:ac:d2 [ether] on eth0

? (210.1.1.3) at 38:ed:18:a2:f3:c2 [ether] on eth0

cisco@S2-AppOneWeb:~$


In the figure below, note that S2-AppOneWeb can ping its gateway (210.1.1.1) as well as the loopback on DCCORE01 (199.199.199.1), just as you could do before. All traffic is now flowing through the ACI fabric, up to the FWs, and then back down into the appropriate VRF (private network).

Figure 178: Scenario 2 Migrate Host to ACI – AppOneWeb Ping Validation

PING::


cisco@S2-AppOneWeb:~$ ping 210.1.1.1 -c 1

PING 210.1.1.1 (210.1.1.1) 56(84) bytes of data.

64 bytes from 210.1.1.1: icmp_seq=1 ttl=63 time=0.182 ms


--- 210.1.1.1 ping statistics ---

1 packets transmitted, 1 received, 0% packet loss, time 0ms

rtt min/avg/max/mdev = 0.182/0.182/0.182/0.000 ms

cisco@S2-AppOneWeb:~$


cisco@S2-AppOneWeb:~$ ping 211.1.1.201 -c 1

PING 211.1.1.201 (211.1.1.201) 56(84) bytes of data.

64 bytes from 211.1.1.201: icmp_seq=1 ttl=60 time=0.465 ms


--- 211.1.1.201 ping statistics ---

1 packets transmitted, 1 received, 0% packet loss, time 0ms

rtt min/avg/max/mdev =

0.465/0.465/0.465/0.000 ms

cisco@S2-AppOneWeb:~$

cisco@S2-AppOneWeb:~$ ping 212.1.1.201 -c 1

PING 212.1.1.201 (212.1.1.201) 56(84) bytes of data.

64 bytes from 212.1.1.201: icmp_seq=1 ttl=60 time=0.477 ms


--- 212.1.1.201 ping statistics ---

1 packets transmitted, 1 received, 0% packet loss, time 0ms

rtt min/avg/max/mdev = 0.477/0.477/0.477/0.000 ms

cisco@S2-AppOneWeb:~$


cisco@S2-AppOneWeb:~$ ping 199.199.199.1 -c 1

PING 199.199.199.1 (199.199.199.1) 56(84) bytes of data.

64 bytes from 199.199.199.1: icmp_seq=1 ttl=253 time=0.787 ms


--- 199.199.199.1 ping statistics ---

1 packets transmitted, 1 received, 0% packet loss, time 0ms

rtt min/avg/max/mdev = 0.787/0.787/0.787/0.000 ms

cisco@S2-AppOneWeb:~$


Figure 179: Scenario 2 Migrate Host to ACI – AppOneWeb Traceroute Validation

TRACEROUTE::


cisco@S2-AppOneWeb:~$ traceroute 210.1.1.1

traceroute to 210.1.1.1 (210.1.1.1), 30 hops max, 60 byte packets

1 210.1.1.1 (210.1.1.1) 3.945 ms 4.387 ms 4.473 ms

cisco@S2-AppOneWeb:~$


cisco@S2-AppOneWeb:~$ traceroute 199.199.199.1

traceroute to 199.199.199.1 (199.199.199.1), 30 hops max, 60 byte packets

1 210.1.1.2 (210.1.1.2) 0.754 ms 210.1.1.3 (210.1.1.3) 0.598 ms 210.1.1.2 (210.1.1.2) 0.760 ms

2 199.199.199.1 (199.199.199.1) 1.566 ms 3.465 ms 3.673 ms

cisco@S2-AppOneWeb:~$


Figure 180: Scenario 2 Validation – FabricPath Domain

SHOW MAC ADDRESS-TABLE::


FP_Core01# show mac address-table address 00:50:56:9e:2a:60

Note: MAC table entries displayed are getting read from software.

Use the 'hardware-age' keyword to get information related to 'Age'


Legend:

* - primary entry, G - Gateway MAC, (R) - Routed MAC, O - Overlay MAC

age - seconds since last seen,+ - primary entry using vPC Peer-Link,

(T) - True, (F) - False , ~~~ - use 'hardware-age' keyword to retrieve age info

VLAN MAC Address Type age Secure NTFY Ports/SWID.SSID.LID

---------+-----------------+--------+---------+------+----+------------------

* 210 0050.569e.2a60 dynamic ~~~ F F Po11


FP_Core01#


In the figure below, note that no ARP entry shows up, as the SVI for VLAN 210 is now shutdown on FP_Core01.

Figure 181: Scenario 2 Validation – FabricPath Domain ARP Validation

SHOW IP ARP::


FP_Core01# show ip arp 210.1.1.201 vrf vrf210


Flags: * - Adjacencies learnt on non-active FHRP router

+ - Adjacencies synced via CFSoE

# - Adjacencies Throttled for Glean

D - Static Adjacencies attached to down interface


IP ARP Table

Total number of entries: 1

Address Age MAC Address Interface

FP_Core01#


In the figure below, note that route to 210.1.0.0/24 no longer shows up as directly connected; routing will follow the static 0.0.0.0/0 to the FW next-hop (100.10.0.4).

Figure 182: Scenario 2 Validation – FabricPath Domain Routing Table Validation

SHOW IP ROUTE::


FP_Core01# show ip route 210.1.1.0 vrf vrf210

IP Route Table for VRF "vrf210"

'*' denotes best ucast next-hop

'**' denotes best mcast next-hop

'[x/y]' denotes [preference/metric]

'%<string>' in via output denotes VRF <string>


0.0.0.0/0, ubest/mbest: 1/0

*via 100.10.0.4, [1/0], 08:52:47, static


FP_Core01#


Figure 183: Scenario 2 Validation – ACI Fabric Endpoint Table Validation

SHOW ENDPOINT::


Leaf1# show endpoint ip 210.1.1.201

Legend:

O - peer-attached H - vtep a - locally-aged S - static

V - vpc-attached p - peer-aged L - local M - span

s - static-arp B - bounce

+-------------------+---------------+-----------------+--------------+-------------+

VLAN/ Encap MAC Address MAC Info/ Interface

Domain VLAN IP Address IP Info

+-------------------+---------------+-----------------+--------------+-------------+

42 vlan-1007 0050.569e.2a60 LV po13

Scenario2:VRF210 vlan-1007 210.1.1.201 LV


Leaf1#


Figure 184: Scenario 2 Validation – ACI Fabric Routing Table Validation

SHOW IP ROUTE::


Leaf1# show ip route vrf Scenario2:VRF210 210.1.1.0/24

IP Route Table for VRF "Scenario2:VRF210"

'*' denotes best ucast next-hop

'**' denotes best mcast next-hop

'[x/y]' denotes [preference/metric]

'%<string>' in via output denotes VRF <string>


210.1.1.0/24, ubest/mbest: 1/0, attached, direct, pervasive

*via 10.0.40.65%overlay-1, [1/0], 00:08:56, static

recursive next hop: 10.0.40.65/32%overlay-1

Leaf1#


Fabric Optimization

The final step (once all servers have been migrated from the FabricPath environment to the ACI fabric) is to enable optimized Layer 2 forwarding on all bridge domains.

· Optimized forwarding of unknown Layer 2 unicast packets reduces the needless flooding of unknown unicast packets on the bridge domain. If a packet is not known to the fabric, it is discarded.

· Disabling ARP flooding reduces the amount of broadcast packets on the bridge domain. ARP requests are routed by the fabric to known destinations as opposed to flooding.

· Changing the multi-destination flooding to “Flood in Encapsulation” ensures that all broadcast level packets are flooded inside of an EPG, and not at the bridge domain level.

Table 51: Scenario 2 Forwarding Semantics

Forwarding Semantic

Configuration

Layer 2 Unknown Unicast

Hardware Proxy

Layer 2 Unknown Multicast Flooding

Flood

Multi Destination Flooding

Flood in Encapsulation

Unicast Routing

Enabled

Enforce subnet check for IP learning

Enabled

ARP Flooding

Disabled

Once all servers have been moved into ACI fabric and Layer 2 extensions to the FabricPath environment are no longer needed, it is possible to disconnect the cables and deconfigure the Layer 2 vPC logical connection. The figure below highlights the connectivity that remains in place after the migration is completed.

Figure 185: Scenario 2 End State after Migration is Complete


Lessons Learned

In the lessons learned section, you are guided through some of the issues experienced during time spent migrating the workloads from the FabricPath environment to the ACI fabric.

UCS B-series and ACI integration considerations

After enabling CDP in UCS manager, CDP did not show up as configured on the VMNIC interfaces in vCenter until after the associated ESXi hosts were rebooted.

Note: It is necessary to enable CDP from the UCS FIs towards the ESX hosts for VM networking integration with ACI. VM integration with UCS-B Series and ACI requires specific configurations. Refer to the following document: http://www.cisco.com/c/en/us/support/docs/cloud-systems-management/application-policy-infrastructure-controller-apic/118965-config-vmm-aci-ucs-00.html.

Infra Address Pool

When configuring the infra address pool while bringing up the fabric, you are asked to provide a pool of IP addresses, which are used to allocate addresses for VTEPs for leaf switches, spines, and APICs. While this address pool is constrained to its own VRF, there are routing implications on the APICs.

Figure 186: APIC routing table – Netstat -r


For example, for the APIC above, you used the default 10.0.0.0/16 address pool for a VTEP infra address pool. However, a problem can arise if you need to reach a vCenter for VM networking integration with an address of 10.255.1.1. Because the APIC has a route going to 10.0.0.0/16 already, the traffic to the vCenter will be blackholed.

The workarounds are very useful; you can insert static routes via Linux commands, but these do not survive a reload. Additionally, the only way to redo the infra address pool is to wipe the fabric and reconfigure.

Note: It is important to correctly address the infra address pool the first time.

· Use unique addressing for your infra addressing pool

· A /23 is the minimum addressing option

ACI Object Naming Conventions

Anytime you are configuring something on ACI, whether it is an EPG, a private network, an interface policy group, or a switch selector, you are configuring a managed object. In ACI once you configure a managed object, it cannot be renamed. You must delete and recreate it if you want to modify its name. You can’t rename and object because once it’s been created, an object automatically has child objects that filter up to it. (This is comparable to a root file structure.) You cannot change the “root” object without affecting all of the downstream objects. Before you start configuring anything, come up with a naming convention for your enterprise for ACI. The following is just a starting point for objects to name.

Application Profiles were originally developed to house groups of end point groups that make up a common application. However, if you are deploying your fabric in network-centric mode (i.e., VLAN=EPG=BD), does this application profile really matter?

If you are deploying your ACI fabric in application-centric mode, or even in hybrid mode (i.e., some ACI-centric Apps, and some network centric), I would make the argument that application profiles for ACI-centric mode are fairly straight forward. Below is an example of an application-centric application profile.

Application Profile MS_Exchange_Corp

· EPG Exchange_Middleware

· EPG DB_for_Exchange

· EPG OWA

· EPG Exchange_MISC

However, when you start to deploy application profiles for network-centric mode, this is when more questions start to creep in.

1. Make up a list of all VLANs and EPGs that will make up your fabric (that you know about).

2. Will some application profiles be ACI-centric?

3. For the EPGs, which will not fall under an ACI-centric application profile, how should you group the remaining EPGs?

a. One large “Legacy” application profile?

b. Should you group the Network-Centric EPGs by function?

i. Network Management VLAN – Application Profile Network_Mgmt

ii. Management_Vlan_For_Compute

Example of a hybrid ACI fabric:

Figure 187: Hybrid ACI Fabric Example

· The EPIC application profile holds EPGs that relate to an application called “EPIC”.

· The L4_7_Clustering application profile holds all VLANs/EPGs which support Layer 4 to Layer 7 clustered for ASAs and load balancers.

· The Legacy_DC application profile is the “catch-all” application profile for the remaining network-centric VLANs, which will be migrated into the ACI fabric, but do not have a specific purpose.

· VblockMgmt is an application profile, which contains management VLANs and port groups for the UCS servers in the Vblock (i.e., vmkernel, management).

Interface policy groups allow users to apply configurations across a potentially large number of switches. An administrator defines switch profiles that associate interface configurations in a single policy group. In this way, large numbers of interfaces across the fabric can be configured at once.

However, in addition to the ability to define policy groups for a large group of switches and switch interfaces (i.e., a common policy group for 1GigAuto access ports with CDP enabled), policy groups are used for things that are not commonly reused, like port-channels and vPC interfaces. When you use policy groups for port channels and vPCs, the name of the policy group becomes very important.

Figure 188: Interface Policy Groups


The naming convention of the interface policy groups is extremely important, as this is the name that will show up in several places in the ACI fabric during configurations. A sampling of places where your interface policy group will show up:

Figure 189: Tenant EPG statis bindings (for vPCs and port channels)


Figure 190: Tenant L3Out interface selection (for vPCs and port channels)


As you can see from the previous example, when you select which interface to deploy, the L3Out across (which is a vPC in this case), you do not select the switch and switch interface, instead, you select the switches and associated vPC policy group. Make sure the name you choose passes the 2am test; meaning, someone working in the NOC will know what you are talking about at 2am during an outage.


Our naming convention for the fabric:

· 1 2 3

· policyGrpVPC_DCCORE

· #1 – It is a policy group

· #2 – It is a vPC (could be a vPC, PC, or access)

· #3 – What is the device you are connecting to? In this case, there is a double-sided vPC to DCCORE01/02, so it gets shortened to DCCORE.

These naming conventions are suggestions. It is important that you develop naming conventions before you start configuring policy groups.

Obtaining Documentation and Submitting a Service Request

For information on obtaining documentation, using the Cisco Bug Search Tool (BST), submitting a service request, and gathering additional information, see What’s New in Cisco Product Documentation at: http://www.cisco.com/c/en/us/td/docs/general/whatsnew/whatsnew.html.

Subscribe to What’s New in Cisco Product Documentation, which lists all new and revised Cisco technical documentation, as an RSS feed and deliver content directly to your desktop using a reader application. The RSS feeds are a free service.


Q: How can I ensure that my Cisco IOS XE version works seamlessly with APIC?
A: Cross-reference your Cisco IOS XE version with the list of supported versions provided in APIC's release notes.

Q: What steps should I follow to verify compatibility between different Cisco IOS versions and Nexus Dashboard?
A: Maintain a list of supported Cisco IOS versions from Nexus Dashboard's release notes and ensure your devices are running on these versions.

Q: How do I determine if my Cisco Viptela OS version is compatible with cAPIC?
A: Refer to cAPIC's documentation and release notes to determine support for specific Cisco Viptela OS versions.

Q: What should I consider when checking version compatibility between Cisco Unified CM and APIC?
A: Always consult APIC's release notes, which will provide details on supported versions of Cisco Unified Communication Manager.

Q: How can I validate that my Cisco NX-OS version is supported by Nexus Dashboard?
A: Cross-reference your Cisco NX-OS version with the supported versions list available on Nexus Dashboard's official documentation.

Q: What are the best practices to ensure version compatibility between Cisco Catalyst 9000 switches and cAPIC?
A: For Catalyst 9000 switches, ensure your devices are running firmware versions that are listed as supported in cAPIC's release notes.

Q: How do I determine if my Cisco WSA software version works seamlessly with APIC?
A: Consult APIC's documentation and release notes to verify support for specific Cisco Web Security Appliance (WSA) software versions.

Q: What steps should I follow to ensure compatibility between different Cisco ASA X versions and Nexus Dashboard?
A: Refer to Nexus Dashboard's documentation and release notes to determine support for specific Cisco ASA X versions.

Q: How can I validate that my Cisco Umbrella version is compatible with cAPIC?
A: Cross-reference your Cisco Umbrella version with the list of supported versions in cAPIC's release notes.

Q: What considerations should I keep in mind for APIC's compatibility with Cisco TelePresence versions?
A: Ensure that the Cisco TelePresence version you're using is explicitly mentioned as supported in APIC's documentation.

Q: How can I ensure that my Cisco DNA Spaces version works with Nexus Dashboard?
A: Check the compatibility matrix or release notes provided by Nexus Dashboard to verify support for specific Cisco DNA Spaces versions.

Q: What steps should I follow to ensure compatibility between Cisco Stealthwatch versions and APIC?
A: Consult APIC's documentation and release notes to verify support for specific Cisco Stealthwatch versions.

Q: How can I verify if my current Cisco CUCM version is compatible with cAPIC?
A: For Cisco Unified Communication Manager (CUCM), refer to cAPIC's release notes to ensure your version is supported.

Q: What should I consider regarding APIC's compatibility with different Cisco Meeting Server versions?
A: Always consult APIC's documentation, which will provide details on supported versions of Cisco Meeting Server.

Q: How can I ensure that my current Cisco ENCS platform version is supported by Nexus Dashboard?
A: Cross-reference your Cisco ENCS platform version with the supported list available on Nexus Dashboard's official documentation.

Q: What steps can I take to validate compatibility between my Cisco Tetration version and APIC?
A: Ensure your Cisco Tetration version is listed as supported in APIC's official release notes or compatibility matrix.

Q: How do I determine if my current version of Cisco CloudCenter works with cAPIC?
A: Consult cAPIC's documentation to determine support for specific Cisco CloudCenter versions.

Q: How can I validate that my Cisco DUO version integrates seamlessly with Nexus Dashboard?
A: Cross-reference your Cisco DUO version with the list of supported versions provided in Nexus Dashboard's release notes.

Q: What considerations should I keep in mind regarding APIC's compatibility with Cisco Webex Teams versions?
A: Consult the APIC's documentation to verify compatibility with specific Cisco Webex Teams versions.

Q: How do I check if my Cisco FMC version is supported by Nexus Dashboard?
A: Refer to Nexus Dashboard's documentation to determine if your specific Cisco Firepower Management Center (FMC) version is supported.

Q: How can I integrate APIC with third-party monitoring solutions?
A: Use the External Management solutions in APIC and integrate via SNMP or API with third-party monitoring tools.

Q: How is traffic encrypted between cAPIC instances?
A: Traffic between cAPIC instances is encrypted using industry-standard encryption protocols.

Q: How can I set up redundancy for Nexus Dashboard?
A: For Nexus Dashboard redundancy, set up a cluster of Nexus Dashboard instances and use load balancers.

Q: How do I configure network policies in APIC?
A: Navigate to the Tenants tab in APIC and set up network policies under the desired tenant.

Q: How do I restore a backup on cAPIC?
A: In cAPIC, go to the Backup and Restore settings and choose the appropriate backup file to restore.

Q: How can I optimize the performance of my Nexus Dashboard?
A: Optimize the performance by monitoring resource usage, scaling resources, and ensuring the latest firmware updates.

Q: How do I set up a VPN connection in APIC?
A: In APIC, navigate to the VPN settings and configure the desired VPN parameters.

Q: How can I secure my cAPIC against external threats?
A: Regularly update cAPIC, use strong authentication methods, and configure firewall rules to protect against external threats.

Q: How do I monitor bandwidth usage in Nexus Dashboard?
A: In Nexus Dashboard, navigate to the Network Insights feature to monitor bandwidth usage.

Q: How can I set up alerts in APIC for system events?
A: Go to the System Settings in APIC and configure alert settings for desired system events.

Q: How do I migrate configurations from an old APIC to a new one?
A: Use the export and import functionality in APIC to migrate configurations.

Q: How can I troubleshoot network latency issues in cAPIC?
A: In cAPIC, use the monitoring and diagnostics tools to identify and troubleshoot network latency issues.

Q: How do I configure multi-site replication in Nexus Dashboard?
A: In Nexus Dashboard, navigate to the Multi-Site settings and configure replication parameters.

Q: How can I backup my APIC configurations automatically?
A: In APIC, use the scheduled backup feature under Backup and Restore settings.

Q: How do I update the SSL certificate for my cAPIC?
A: Navigate to the SSL settings in cAPIC and update the desired certificate.

Q: How can I integrate Nexus Dashboard with other Cisco solutions?
A: Nexus Dashboard supports various integrations through APIs with other Cisco solutions.

Q: How do I set up role-based access control in APIC?
A: In APIC, navigate to the Users settings and configure roles and permissions for role-based access control.

Q: How can I extend the storage capacity of my Nexus Dashboard?
A: Add additional storage to the Nexus Dashboard instance or configure external storage integrations.

Q: How do I integrate APIC with cloud providers?
A: APIC supports integrations with various cloud providers via the cloud connectors.

Q: How can I monitor device health in cAPIC?
A: In cAPIC, navigate to the Health tab to monitor the health of connected devices.

Q: How do I access the Nexus Dashboard interface?
A: Open a web browser and navigate to the IP address or FQDN of the Nexus Dashboard. Ensure the URL starts with 'https://'.

Q: What's the default username and password for Nexus Dashboard?
A: The default username is 'admin'. The default password is set during the initial setup and there isn't a factory default.

Q: How do I integrate Nexus Dashboard with other Cisco platforms?
A: Use the 'Integration' tab in the Nexus Dashboard GUI to connect with platforms like ACI, SD-WAN, etc.

Q: Can I monitor multiple sites using Nexus Dashboard?
A: Yes, Nexus Dashboard offers a multi-site view to manage and monitor multiple sites from a single pane.

Q: How do I upgrade the Nexus Dashboard software?
A: Navigate to the 'System' tab in the GUI, select 'Software Updates', and follow the prompts to upgrade the software.

Q: What are the key features of Nexus Dashboard Insights?
A: Insights provide proactive monitoring, anomaly detection, and root cause analysis for network issues.

Q: How do I backup the Nexus Dashboard configuration?
A: Go to the 'System' tab in the GUI, select 'Backup & Restore', and follow the prompts to backup your configuration.

Q: How do I configure user roles and permissions in Nexus Dashboard?
A: Navigate to the 'Admin' tab and under 'User Management', define roles and assign permissions.

Q: How can I integrate Nexus Dashboard with third-party monitoring tools?
A: Use the available APIs and SDKs provided by Cisco to integrate Nexus Dashboard with external monitoring and analytics tools.

Q: How do I troubleshoot issues using Nexus Dashboard?
A: Use the 'Health' and 'Insights' features to get detailed metrics, logs, and analytics for troubleshooting.

Q: Can I customize the dashboard view in Nexus Dashboard?
A: Yes, the GUI provides options to customize widgets, layouts, and data views as per user preferences.

Q: How do I configure notifications and alerts in Nexus Dashboard?
A: Navigate to the 'Alerts' section, define alert criteria, and set up notification methods like email or SNMP traps.

Q: How secure is data in Nexus Dashboard?
A: Nexus Dashboard employs data encryption, RBAC, and other security measures to ensure data integrity and security.

Q: Can I deploy Nexus Dashboard in a cloud environment?
A: Yes, Nexus Dashboard supports deployments in private and public cloud environments.

Q: How do I connect Nexus Dashboard to Cisco Intersight?
A: Use the 'Integration' tab in the GUI and follow the prompts to connect Nexus Dashboard with Cisco Intersight.

Q: How do I monitor application performance with Nexus Dashboard?
A: Use the Application Experience feature in Nexus Dashboard to monitor application health, performance, and SLAs.

Q: How can I restore a backup configuration in Nexus Dashboard?
A: Navigate to the 'System' tab, select 'Backup & Restore', and upload your backup file to restore the configuration.

Q: What are the hardware requirements for Nexus Dashboard?
A: Refer to the Cisco documentation for detailed hardware specifications and requirements for Nexus Dashboard.

Q: How do I integrate Nexus Dashboard with Cisco DNA Center?
A: Use the 'Integration' tab in the GUI and follow the integration steps for Cisco DNA Center.

Q: How do I set up telemetry streaming in Nexus Dashboard?
A: Navigate to the 'Telemetry' section, define your telemetry sources, and set up the streaming destination.

Q: How do I deploy cAPIC on AWS?
A: Use the Cisco-provided CloudFormation template to deploy cAPIC in your AWS environment.

Q: Can cAPIC integrate with Azure services?
A: Yes, cAPIC supports integration with Azure services using the Azure Resource Manager templates.

Q: How do I connect cAPIC with on-premises APIC?
A: Use the site-to-site VPN or direct connectivity options in cAPIC to connect with on-premises APIC.

Q: How do I monitor cAPIC health in the cloud?
A: Use the cAPIC GUI's dashboard which provides a health score and detailed metrics for the cloud deployment.

Q: How can I backup the cAPIC configuration?
A: Go to the cAPIC GUI, navigate to the 'Admin' tab, and select 'Export Configuration'.

Q: How do I upgrade the cAPIC version in cloud?
A: Navigate to the firmware tab in the cAPIC GUI, upload the desired firmware image, then apply it to the desired nodes.

Q: What are the networking requirements for cAPIC in AWS?
A: Ensure proper VPC, subnet configurations, and security group settings to allow communication between cAPIC and other AWS services.

Q: How do I integrate cAPIC with AWS Transit Gateway?
A: Use the native integration options in cAPIC to connect with AWS Transit Gateway for extended connectivity.

Q: How can I scale cAPIC deployments in the cloud?
A: Use the auto-scaling features provided by cloud platforms in conjunction with cAPIC configurations.

Q: What's the difference between APIC and cAPIC?
A: While APIC is designed for on-premises data centers, cAPIC is designed for cloud environments and offers integrations with cloud-native services.

Q: How do I troubleshoot cAPIC in Azure?
A: Use Azure Monitor and Diagnostic settings in conjunction with cAPIC's built-in troubleshooting tools.

Q: How do I connect cAPIC to cloud databases?
A: Use the service integration options in cAPIC to connect with cloud-native database services.

Q: Can I deploy cAPIC in a multi-cloud environment?
A: Yes, cAPIC supports multi-cloud deployments and can manage resources across different cloud platforms.

Q: How do I secure cAPIC deployments in the cloud?
A: Use the native security features of cloud platforms, such as AWS Security Groups or Azure Network Security Groups, in conjunction with cAPIC's security policies.

Q: Can I integrate cAPIC with cloud load balancers?
A: Yes, cAPIC provides integration options for cloud-native load balancers like AWS ALB or Azure Load Balancer.

Q: How do I monitor traffic flow in cAPIC in the cloud?
A: Use cloud-native monitoring solutions like AWS CloudWatch or Azure Monitor, in conjunction with cAPIC's monitoring features.

Q: How do I automate cAPIC deployments in the cloud?
A: Use Infrastructure as Code (IaC) tools like Terraform or cloud-native templating solutions in conjunction with cAPIC APIs.

Q: How can I restore a backup in cAPIC?
A: Navigate to the 'Backup & Restore' option in the cAPIC GUI and upload the backup file to restore configurations.

Q: What are the storage requirements for cAPIC in GCP?
A: Ensure proper storage bucket configurations and permissions in GCP when deploying cAPIC.

Q: How do I integrate cAPIC with cloud identity services?
A: Use the native integration options in cAPIC to connect with cloud identity services like AWS Cognito or Azure AD.

Q: How do I verify if my current Cisco IOS version is compatible with APIC?
A: Refer to the APIC software compatibility matrix on Cisco's official website for supported IOS versions.

Q: How can I ensure my virtual network functions are compatible with cAPIC?
A: Review the cAPIC documentation, especially sections related to virtual network functions, for compatibility details.

Q: How do I confirm if Nexus Dashboard supports my current data center topology?
A: Nexus Dashboard's documentation provides details on supported data center topologies.

Q: How do I determine if my current Nexus switch models are APIC compatible?
A: Cisco's compatibility matrix provides detailed information on supported Nexus switch models for APIC.

Q: Is there a compatibility checker tool for APIC and third-party applications?
A: While there isn't a direct checker tool, Cisco's compatibility matrix and documentation provide guidance on third-party application compatibility.

Q: How do I ensure my network protocols are compatible with Nexus Dashboard?
A: Refer to Nexus Dashboard's documentation for a list of supported network protocols.

Q: Which versions of OpenStack are compatible with APIC?
A: APIC's documentation provides details on supported OpenStack versions.

Q: How do I validate if my current firewall model is supported by cAPIC?
A: Review the cAPIC documentation, specifically the sections related to firewalls, for supported models.

Q: Are there any compatibility concerns when integrating APIC with multi-cloud environments?
A: While APIC provides cloud connectors for various cloud providers, always refer to the latest documentation for any compatibility concerns.

Q: How do I check Nexus Dashboard compatibility with my current Cisco UCS setup?
A: Check the Nexus Dashboard documentation or compatibility matrix for UCS support details.

Q: How often does Cisco release compatibility updates for cAPIC?
A: Cisco releases compatibility updates as part of their regular software updates. Keep an eye on release notes and documentation.

Q: How can I determine if my current monitoring solution is compatible with APIC?
A: Consult the integration section of APIC's documentation to verify compatibility with monitoring solutions.

Q: Are there any known compatibility concerns with cAPIC and Kubernetes deployments?
A: cAPIC documentation will provide details on its compatibility with Kubernetes versions and deployments.

Q: How can I validate Nexus Dashboard's compatibility with my WAN setup?
A: Consult the Nexus Dashboard's documentation and compatibility matrix for WAN setup compatibility.

Q: How do I confirm if my current storage solutions are APIC compatible?
A: APIC's official documentation provides details on supported storage solutions.

Q: Is cAPIC compatible with non-Cisco hypervisors?
A: cAPIC primarily supports popular hypervisors like VMware and Hyper-V, but always check the latest documentation for detailed compatibility.

Q: How can I ensure that my current load balancers are supported by Nexus Dashboard?
A: Refer to Nexus Dashboard's documentation for a list of supported load balancers.

Q: How do I find out if APIC supports my current version of vCenter?
A: APIC's documentation provides a list of supported vCenter versions.

Q: How do I check if cAPIC is compatible with my current SD-WAN solution?
A: For SD-WAN compatibility with cAPIC, refer to Cisco's official documentation and compatibility matrix.

Q: Are there any known compatibility issues between Nexus Dashboard and third-party logging solutions?
A: While Nexus Dashboard supports integration with popular logging solutions, always check the documentation for specific compatibility details.

Q: How can I implement role-based access control (RBAC) in APIC?
A: Navigate to the APIC GUI, go to 'Admin' > 'RBAC' > 'Roles', and create or modify roles with specific privileges. Assign these roles to users or groups as required.

Q: How can I monitor system temperature in cAPIC?
A: In cAPIC, navigate to 'Dashboard' > 'Environmental Monitoring', and you can view temperature readings and other environmental metrics.

Q: How do I integrate Nexus Dashboard with third-party analytics tools?
A: In Nexus Dashboard, go to 'Integrations' > 'Analytics', select the desired analytics tool, and provide the necessary configurations and credentials.

Q: How do I reset a forgotten admin password in APIC?
A: You'll need console access to the APIC. Restart the APIC, interrupt the boot process, and follow the password recovery procedure to reset the admin password.

Q: How can I set up rate limiting for specific services in cAPIC?
A: In cAPIC, navigate to 'Policies' > 'Rate Limiting', and define rate limit rules for specific services or traffic types.

Q: How do I upgrade firmware for devices managed by Nexus Dashboard?
A: In Nexus Dashboard, navigate to 'Admin' > 'Firmware Management', upload the desired firmware, and initiate the upgrade process for targeted devices.

Q: How do I export configuration data from APIC?
A: In APIC GUI, navigate to 'Admin' > 'Export/Import', select the desired configuration data, and initiate the export process.

Q: How can I view CPU and memory usage in cAPIC?
A: In cAPIC, navigate to 'Dashboard' > 'System Monitoring', where you can view detailed metrics about CPU, memory, and other system resources.

Q: How can I set up network overlays in Nexus Dashboard?
A: In Nexus Dashboard, go to 'Network' > 'Overlays', and set up the desired network overlay configurations.

Q: How do I integrate APIC with third-party cloud providers like Azure?
A: In APIC GUI, navigate to 'Integrations' > 'Cloud Providers', select Azure, and provide the necessary configurations and credentials.

Q: (Follow up to Q10) How can I sync virtual network configurations between APIC and Azure?
A: In the 'Cloud Providers' section, after integrating with Azure, use the 'Sync Virtual Networks' option to synchronize configurations between APIC and Azure.

Q: How do I backup and restore configurations in cAPIC?
A: In cAPIC, navigate to 'Admin' > 'Backup & Restore', where you can initiate backup processes and restore from saved backups.

Q: How can I view active user sessions in Nexus Dashboard?
A: In Nexus Dashboard, navigate to 'Admin' > 'Active Sessions' to see a list of currently active user sessions and their details.

Q: How do I secure APIC against DDoS attacks?
A: In APIC, navigate to 'Policies' > 'Security' > 'DDoS Protection', and configure the desired protection mechanisms and thresholds.

Q: How can I monitor bandwidth usage on specific links in cAPIC?
A: In cAPIC, navigate to 'Dashboard' > 'Bandwidth Monitoring', and select the desired links or interfaces to view bandwidth metrics.

Q: How do I enable API access in Nexus Dashboard?
A: In Nexus Dashboard, go to 'Admin' > 'API Access', enable the API endpoint, and configure the necessary access permissions and keys.

Q: (Follow up to Q16) How can I generate API keys for third-party integrations in Nexus Dashboard?
A: In the 'API Access' section, there's an option for 'Generate API Key'. Use this option to create API keys for third-party applications.

Q: How can I set up traffic shaping policies in cAPIC?
A: In cAPIC, navigate to 'Policies' > 'Traffic Shaping', and set up rules that define how different traffic types should be treated.

Q: How can I view the operational history of tasks in Nexus Dashboard?
A: In Nexus Dashboard, navigate to 'Admin' > 'Task History' to view a log of tasks performed and their outcomes.

Q: How do I set up cross-site connectivity in APIC?
A: In APIC, navigate to 'Network' > 'Cross-Site Connectivity', and configure the necessary parameters for inter-site communication.

Q: How can I monitor disk space usage on APIC?
A: Navigate to the APIC GUI, go to 'System' > 'Storage Management' to view disk usage and storage metrics.

Q: How can I set up multi-factor authentication in cAPIC?
A: In cAPIC, go to 'Admin' > 'AAA' and configure multi-factor authentication settings using the desired provider.

Q: How do I deploy a service graph in Nexus Dashboard?
A: In Nexus Dashboard, navigate to 'Services' > 'Service Graphs', create a new graph, and define the required nodes and connections.

Q: How can I backup the database in APIC?
A: In APIC GUI, go to 'Admin' > 'Backup & Restore' and initiate a backup for the system database.

Q: How can I set up virtual networks in cAPIC?
A: In cAPIC, navigate to 'Network' > 'Virtual Networks' and create or manage existing virtual network configurations.

Q: How can I integrate Nexus Dashboard with LDAP for user authentication?
A: In Nexus Dashboard, go to 'Admin' > 'AAA' > 'LDAP Configuration' and provide the LDAP server details and settings.

Q: How can I optimize the routing in APIC?
A: In APIC, use the 'Routing Policies' feature to define routing rules and path preferences to optimize network traffic.

Q: How do I manage licenses in cAPIC?
A: In cAPIC, navigate to 'Admin' > 'License Management' to add, renew, or revoke software licenses.

Q: How can I set up high availability for Nexus Dashboard?
A: Deploy multiple instances of Nexus Dashboard and configure them in a cluster mode for high availability and failover capabilities.

Q: How can I view the software version running on APIC?
A: In APIC GUI, navigate to 'Admin' > 'System Information'. The software version will be displayed in the overview section.

Q: (Follow up to Q10) How do I roll back to a previous software version in APIC?
A: In APIC, navigate to 'Admin' > 'Software Management', select the desired software version, and initiate the rollback process.

Q: How can I automate repetitive tasks in cAPIC?
A: Use the cAPIC's built-in automation features or integrate with tools like Ansible or Terraform for task automation.

Q: How do I set up telemetry in Nexus Dashboard?
A: In Nexus Dashboard, navigate to 'Telemetry' > 'Settings' and configure the desired telemetry parameters and endpoints.

Q: How can I view the topology map in cAPIC?
A: In cAPIC, go to the 'Dashboard' > 'Topology View' to visualize the network topology and device interconnections.

Q: How can I enforce security policies in APIC?
A: In APIC, navigate to 'Policies' > 'Security' and define the required security rules and profiles to enforce on the network.

Q: How do I integrate Nexus Dashboard with SNMP monitoring tools?
A: In Nexus Dashboard, go to 'Admin' > 'SNMP Configuration' and provide the details of the SNMP monitoring tool.

Q: (Follow up to Q16) How do I set up SNMP traps in Nexus Dashboard?
A: In the 'SNMP Configuration' section, define the trap destinations and select the events for which traps should be generated.

Q: How can I troubleshoot slow network performance using cAPIC?
A: Use the cAPIC's 'Analytics' and 'Insights' features to identify bottlenecks, congestion points, and other issues impacting network performance.

Q: How can I configure remote access to Nexus Dashboard?
A: In Nexus Dashboard, navigate to 'Admin' > 'Remote Access' and configure VPN or other remote access methods.

Q: How can I integrate APIC with external orchestration platforms?
A: In APIC, use the API integrations to connect with external orchestration platforms like Kubernetes, OpenShift, etc., and manage configurations.

Q: How do you configure a bridge domain in APIC?
A: Navigate to the APIC GUI, select the desired Tenant, go to Networking > Bridge Domains, and click on 'Create'.

Q: How can I verify the status of interfaces on Nexus Dashboard?
A: In the Nexus Dashboard, go to Inventory > Devices, select a device, and navigate to the 'Interfaces' tab.

Q: What is a contract in the context of Cisco APIC?
A: A contract in APIC is a policy-based mechanism to define the communication between Endpoint Groups (EPGs).

Q: How do I enable API access for third-party applications in cAPIC?
A: In the cAPIC GUI, navigate to Admin > API Access and configure API access policies and roles.

Q: How can I view logs in the Nexus Dashboard?
A: Navigate to the Nexus Dashboard GUI, go to Monitoring > Logs to view system and application logs.

Q: How do you configure an IP address pool in cAPIC?
A: In the cAPIC GUI, go to Fabric > Access Policies > Pools > IP Pools, and define your IP address range.

Q: What is the function of a VTEP in Cisco's VXLAN implementation?
A: A VTEP (VXLAN Tunnel Endpoint) is responsible for encapsulating and decapsulating VXLAN traffic from and to the underlay network.

Q: How do I integrate Cisco ACI with Microsoft Azure using APIC?
A: Navigate to the APIC GUI, go to Fabric > External Cloud Network Pools, and set up the Azure integration parameters.

Q: Can I configure Role-Based Access Control (RBAC) in Nexus Dashboard?
A: Yes, in the Nexus Dashboard GUI, navigate to Administration > Roles & Users to configure RBAC.

Q: How do you troubleshoot OSPF issues in APIC?
A: Check the OSPF status using the 'show ospf' command, inspect related faults in the APIC GUI, and verify OSPF configurations and neighbor status.

Q: What's the purpose of a service graph in APIC?
A: A service graph in APIC represents a series of network services (like firewalls, load balancers) applied to the traffic flow between EPGs.

Q: How can I set up LDAP authentication for Nexus Dashboard?
A: Navigate to the Nexus Dashboard GUI, go to Administration > External Authentication, and configure the LDAP parameters.

Q: How do you view the operational status of a specific EPG in cAPIC?
A: In the cAPIC GUI, navigate to Tenant > Application Profile > EPG, select the desired EPG, and view its operational status.

Q: What is the significance of a Policy Enforcement Point (PEP) in Cisco ACI?
A: PEP is where the policies (like contracts) defined in APIC are enforced in the data plane, typically at the leaf switch.

Q: How do I set up a vPC (Virtual Port Channel) in Nexus Dashboard?
A: In the Nexus Dashboard, navigate to Fabric > Policies > vPC and configure the vPC parameters and member interfaces.

Q: How can I monitor traffic flow in real-time in APIC?
A: Use the APIC's built-in tools like SPAN (Switched Port Analyzer) or ERSPAN (Encapsulated Remote SPAN) to capture and analyze real-time traffic.

Q: How do you deploy a virtual machine through Nexus Dashboard?
A: Nexus Dashboard primarily manages and monitors network infrastructure. To deploy VMs, integrate with orchestration platforms like VMware vCenter or Cisco UCS Director.

Q: What is the function of an Endpoint Group (EPG) in APIC?
A: EPG in APIC is a logical entity used to group together similar network endpoints, and it's the primary object to which policies are applied.

Q: How do I view the topology of my ACI fabric in cAPIC?
A: In the cAPIC GUI, navigate to Fabric > Topology to view a graphical representation of your ACI fabric.

Q: How can I migrate configurations from one Nexus Dashboard to another?
A: Use the import/export functionality in the Nexus Dashboard GUI under Administration > Import/Export to migrate configurations.

Q: I'm receiving 'Insufficient Resources' errors on APIC when deploying a new service. How can I address this?
A: Review the resource utilization on APIC. You may need to optimize resources or consider scaling up the APIC deployment.

Q: Some devices are marked as 'Unreachable' in cAPIC. How can I troubleshoot?
A: Verify network connectivity to the devices. Also, ensure the devices are online and there's no configuration that may block communication with cAPIC.

Q: I've configured a multicast group on APIC, but some VMs aren't receiving the traffic. What could be the reason?
A: Ensure the VMs are part of the correct EPG associated with the multicast group. Also, check multicast routing and IGMP configurations.

Q: cAPIC is showing inconsistent data when compared to my cloud provider's dashboard. How can I resolve this discrepancy?
A: Manually sync cAPIC with your cloud provider. Ensure there are no API issues or rate limits causing the discrepancy.

Q: I've noticed that after an APIC failover, some configurations aren't syncing to the standby APIC. Why might this be?
A: Ensure both the primary and standby APICs are in sync. Check for any network issues or configurations preventing synchronization.

Q: I'm facing issues with the VxLAN configurations on cAPIC. What steps should I take?
A: Verify the VxLAN configurations, including VNI and VTEP settings. Ensure there's proper routing and bridging for the VxLAN traffic.

Q: APIC is not sending notifications/alerts even though I've set them up. How can I troubleshoot?
A: Check the notification settings in APIC. Ensure the notification channel (e.g., email or SNMP) is correctly configured and operational.

Q: I've observed a decrease in performance for VMs managed by cAPIC after a software update. How can I address this?
A: Review the release notes of the software update for any known issues. Consider rolling back or optimizing VM settings for performance.

Q: My APIC dashboard is showing 'License Expiration' warnings. What should I do?
A: You should renew or purchase the required licenses. Ensure that the licenses are correctly installed and activated on APIC.

Q: I've integrated third-party monitoring tools with cAPIC, but they aren't receiving data. How can I troubleshoot?
A: Ensure cAPIC has the correct configurations to send data to the monitoring tools. Check for any network issues or firewalls blocking the data.

Q: A specific tenant's configurations on APIC are not propagating to the associated devices. How can I resolve this?
A: Ensure the tenant's configurations are correctly set. Also, check for any synchronization issues between APIC and the devices.

Q: I've set up a logging server, but APIC isn't sending logs to it. What could be the reason?
A: Verify the logging server's configurations in APIC. Ensure there's network connectivity between APIC and the logging server.

Q: I'm trying to backup cAPIC configurations, but the process fails midway. How can I address this?
A: Check for any storage or network issues during the backup process. Ensure cAPIC has the correct permissions to backup configurations.

Q: I've noticed some VMs are experiencing packet loss in my APIC-managed environment. How can I troubleshoot?
A: Review the network path for the affected VMs. Check for any network congestion or misconfigurations causing the packet loss.

Q: I've set up an IPsec VPN on cAPIC, but the tunnel isn't coming up. What steps should I take?
A: Verify the IPsec VPN configurations, including peer IPs and encryption settings. Ensure the VPN endpoints are reachable and operational.

Q: Some configurations made on APIC aren't reflecting in the real-time operational status. How can I resolve this?
A: Force a manual sync in APIC. Review any logs or warnings related to the real-time operational status.

Q: I'm unable to integrate a third-party firewall with cAPIC. How can I troubleshoot?
A: Ensure the firewall supports integration with cAPIC. Check for any API or protocol incompatibilities.

Q: APIC is showing 'Memory Overutilization' alerts. What could be causing this?
A: Review the processes and services running on APIC. Consider optimizing or scaling up the APIC deployment to handle the memory requirements.

Q: I've deployed a new service graph in cAPIC, but some nodes aren't active. How can I address this?
A: Verify the service graph's configurations. Ensure all nodes are correctly configured and online.

Q: I've observed intermittent connectivity issues between APIC and external databases. How can I troubleshoot?
A: Check the network path between APIC and the databases. Review any logs or warnings related to database connectivity.

Q: How can I set up redundancy for my APIC cluster?
A: Deploy a minimum of three APIC nodes to form a cluster, ensuring data replication and fault tolerance.

Q: Why can't I see all my devices in the Nexus Dashboard inventory?
A: Ensure all devices are correctly configured and reachable. Also, check if they are added to the data sources in Nexus Dashboard.

Q: How can I perform a health check on my cAPIC?
A: In the cAPIC GUI, navigate to 'Dashboard'. The health scores and status of various components will be displayed.

Q: How do I restore a backup configuration in APIC?
A: In the APIC GUI, navigate to Admin > Import/Export and use the 'Import Configuration' option. Upload the backup file to restore.

Q: Can I integrate APIC with Cisco DNA Center?
A: Yes, use the API integrations available in both APIC and Cisco DNA Center for synchronization and data exchange.

Q: How do I troubleshoot connectivity issues between cAPIC and Nexus Dashboard?
A: Check the network path between cAPIC and Nexus Dashboard. Ensure both systems are reachable and there's no firewall blocking the communication.

Q: How can I upgrade the software on Nexus Dashboard?
A: Navigate to 'System Settings' in Nexus Dashboard. Under 'Software Management', select the desired software version and initiate the upgrade.

Q: My APIC is showing high CPU usage. What could be the reason?
A: High CPU usage can be due to excessive logging, heavy network traffic, or malfunctioning processes. Check active processes and logs for anomalies.

Q: How do I integrate third-party applications with Nexus Dashboard?
A: Use the 'App Store' feature in Nexus Dashboard to install and integrate third-party applications.

Q: How can I view active sessions in cAPIC?
A: In the cAPIC GUI, navigate to 'Admin' > 'Active Sessions' to view and manage current user sessions.

Q: (Follow up to Q10) How can I terminate an active session in cAPIC?
A: In the 'Active Sessions' view, select the desired session and use the 'Terminate Session' option.

Q: How can I set up notifications in APIC for specific events?
A: Navigate to 'Event Management' in the APIC GUI. Define the events of interest and configure notification settings.

Q: How do I configure API rate limiting in Nexus Dashboard?
A: In Nexus Dashboard, navigate to 'API Management'. Define the rate limits based on the desired criteria.

Q: How can I view historical data in cAPIC?
A: Use the 'Analytics' feature in cAPIC to access and analyze historical data and trends.

Q: How can I set up a backup schedule for APIC?
A: In the APIC GUI, navigate to Admin > Import/Export. Configure a recurring backup schedule and specify the destination.

Q: Can I integrate cAPIC with Cisco SecureX for enhanced security?
A: Yes, use the API integrations available in both cAPIC and Cisco SecureX to achieve seamless data exchange and security enhancements.

Q: (Follow up to Q16) How do I obtain the SecureX integration key for cAPIC?
A: In SecureX, navigate to 'Integrations' and generate a new integration key specifically for cAPIC.

Q: How can I monitor the power consumption of devices in APIC?
A: In APIC, navigate to 'Device Management' > 'Power Monitoring' to view power consumption metrics.

Q: How do I add a new user with specific privileges in Nexus Dashboard?
A: In Nexus Dashboard, navigate to 'Admin' > 'User Management'. Create a new user and assign the desired roles and privileges.

Q: I want to integrate APIC with a cloud provider. How can I do that?
A: In APIC, navigate to 'Cloud Integrations' and configure the desired cloud provider's settings and credentials.

Q: I'm unable to access the APIC GUI after a software update. How can I troubleshoot?
A: Verify network connectivity to APIC. You may also want to check APIC logs for any errors or warnings related to the GUI.

Q: I noticed that cAPIC is not syncing with my cloud provider. What steps should I take?
A: Ensure that cAPIC has the correct API keys and permissions to communicate with your cloud provider. Also, check for any network issues between cAPIC and the cloud.

Q: After deploying a new VM, it's not being recognized by APIC. How can I resolve this?
A: Verify the VM's network configurations and ensure it's on a network segment managed by APIC. Also, check APIC logs for discovery-related issues.

Q: I've configured a new subnet on APIC, but it's not propagating to my leaf switches. Why might this be?
A: Ensure that the subnet is correctly configured in APIC and that the associated EPG is mapped to the correct leaf switches. Verify fabric connectivity.

Q: I'm facing latency issues when accessing applications hosted in a VM managed by cAPIC. How do I address this?
A: Check the network path between the client and the VM for any bottlenecks or issues. Also, verify VM's network configurations in cAPIC.

Q: I've added a new leaf switch to my topology, but it's not being recognized by APIC. What could be the reason?
A: Ensure the leaf switch is online and reachable. Verify its discovery settings and ensure it's in the correct APIC management domain.

Q: The VMs in my cAPIC-managed environment are unable to communicate with each other. How can I troubleshoot?
A: Check the network configurations of the VMs and ensure they are on the same EPG or have the necessary contracts to communicate.

Q: I'm seeing an 'Out of Sync' status for some devices in my APIC dashboard. What steps should I take?
A: Manually attempt to sync the devices. If unsuccessful, check for configuration mismatches or conflicts that might be causing the sync issues.

Q: After a power outage, some of my network settings in cAPIC seem to have reverted to defaults. How can I prevent this in the future?
A: Ensure that cAPIC's configurations are regularly backed up. In case of power outages, always validate configurations post-recovery.

Q: I've set up a new VLAN in APIC, but VMs within this VLAN can't access the internet. What could be wrong?
A: Verify the gateway configurations for the VLAN and ensure there's a route to the internet. Check for any ACLs or firewall rules blocking traffic.

Q: I'm receiving error messages related to certificate expiration in cAPIC. How do I resolve this?
A: Update or renew the certificates causing the error. Ensure that cAPIC has the correct time settings to validate certificates.

Q: APIC is showing high CPU usage consistently. What could be causing this?
A: Review running processes on APIC to identify the cause. Ensure there's no unexpected traffic or processes overloading the APIC.

Q: Some VMs in my environment are unable to obtain IP addresses from the DHCP server managed by APIC. How can I troubleshoot?
A: Ensure the DHCP server is online and has a sufficient IP address pool. Check for any conflicting DHCP settings in APIC.

Q: I've configured inter-tenant communication in APIC, but it's not working as expected. How do I address this?
A: Verify the contracts and EPGs associated with inter-tenant communication. Ensure they are correctly set up to allow the desired traffic.

Q: cAPIC is not reflecting the correct status of some VMs. What steps should I take?
A: Resync the VM status in cAPIC. Also, verify the communication between cAPIC and the host where the VMs reside.

Q: I've deployed a service graph in APIC, but traffic is not being directed through it as intended. How can I troubleshoot?
A: Verify the service graph's configurations in APIC. Ensure that the relevant EPGs are associated with the service graph and that the nodes in the graph are online.

Q: A particular subnet is not able to communicate with another subnet in my cAPIC-managed environment. How do I resolve this?
A: Check the routing and contract configurations for the subnets in cAPIC. Ensure there are no ACLs blocking the communication.

Q: I'm seeing frequent disconnections between APIC and leaf switches. What could be the reason?
A: Verify the network connectivity between APIC and the leaf switches. Check for any issues on the switches that might be causing the disconnections.

Q: VM migration between hosts is failing in my cAPIC environment. How can I address this?
A: Check the network configurations in cAPIC for the source and destination hosts. Ensure there's sufficient bandwidth and no network issues causing the failure.

Q: I'm unable to access the REST API of APIC after a recent configuration change. How can I troubleshoot?
A: Verify the REST API settings in APIC. Ensure that any recent configurations didn't inadvertently restrict API access.

Q: How can I monitor disk space usage on APIC?
A: Navigate to the APIC GUI, go to 'System' > 'Storage Management' to view disk usage and storage metrics.

Q: How can I set up multi-factor authentication in cAPIC?
A: In cAPIC, go to 'Admin' > 'AAA' and configure multi-factor authentication settings using the desired provider.

Q: How do I deploy a service graph in Nexus Dashboard?
A: In Nexus Dashboard, navigate to 'Services' > 'Service Graphs', create a new graph, and define the required nodes and connections.

Q: How can I backup the database in APIC?
A: In APIC GUI, go to 'Admin' > 'Backup & Restore' and initiate a backup for the system database.

Q: How can I set up virtual networks in cAPIC?
A: In cAPIC, navigate to 'Network' > 'Virtual Networks' and create or manage existing virtual network configurations.

Q: How can I integrate Nexus Dashboard with LDAP for user authentication?
A: In Nexus Dashboard, go to 'Admin' > 'AAA' > 'LDAP Configuration' and provide the LDAP server details and settings.

Q: How can I optimize the routing in APIC?
A: In APIC, use the 'Routing Policies' feature to define routing rules and path preferences to optimize network traffic.

Q: How do I manage licenses in cAPIC?
A: In cAPIC, navigate to 'Admin' > 'License Management' to add, renew, or revoke software licenses.

Q: How can I set up high availability for Nexus Dashboard?
A: Deploy multiple instances of Nexus Dashboard and configure them in a cluster mode for high availability and failover capabilities.

Q: How can I view the software version running on APIC?
A: In APIC GUI, navigate to 'Admin' > 'System Information'. The software version will be displayed in the overview section.

Q: (Follow up to Q10) How do I roll back to a previous software version in APIC?
A: In APIC, navigate to 'Admin' > 'Software Management', select the desired software version, and initiate the rollback process.

Q: How can I automate repetitive tasks in cAPIC?
A: Use the cAPIC's built-in automation features or integrate with tools like Ansible or Terraform for task automation.

Q: How do I set up telemetry in Nexus Dashboard?
A: In Nexus Dashboard, navigate to 'Telemetry' > 'Settings' and configure the desired telemetry parameters and endpoints.

Q: How can I view the topology map in cAPIC?
A: In cAPIC, go to the 'Dashboard' > 'Topology View' to visualize the network topology and device interconnections.

Q: How can I enforce security policies in APIC?
A: In APIC, navigate to 'Policies' > 'Security' and define the required security rules and profiles to enforce on the network.

Q: How do I integrate Nexus Dashboard with SNMP monitoring tools?
A: In Nexus Dashboard, go to 'Admin' > 'SNMP Configuration' and provide the details of the SNMP monitoring tool.

Q: (Follow up to Q16) How do I set up SNMP traps in Nexus Dashboard?
A: In the 'SNMP Configuration' section, define the trap destinations and select the events for which traps should be generated.

Q: How can I troubleshoot slow network performance using cAPIC?
A: Use the cAPIC's 'Analytics' and 'Insights' features to identify bottlenecks, congestion points, and other issues impacting network performance.

Q: How can I configure remote access to Nexus Dashboard?
A: In Nexus Dashboard, navigate to 'Admin' > 'Remote Access' and configure VPN or other remote access methods.

Q: How can I integrate APIC with external orchestration platforms?
A: In APIC, use the API integrations to connect with external orchestration platforms like Kubernetes, OpenShift, etc., and manage configurations.

Q: How can I check if my current hardware is compatible with the latest APIC version?
A: You can check Cisco's official compatibility matrix or use the 'show version compatibility' command on APIC.

Q: Are there any known compatibility issues between cAPIC and Azure cloud services?
A: Always refer to Cisco's official documentation for any known compatibility issues between cAPIC and Azure.

Q: How do I ensure Nexus Dashboard is compatible with my current network infrastructure?
A: Refer to the Nexus Dashboard's installation guide and compatibility matrix on Cisco's official website.

Q: Which versions of APIC are compatible with Nexus 9000 series switches?
A: Cisco's official compatibility matrix provides detailed information on APIC versions compatible with Nexus 9000 series switches.

Q: Are there any plugins or modules to enhance cAPIC compatibility with third-party apps?
A: Yes, there are various plugins available in the Cisco marketplace to enhance cAPIC's compatibility with third-party applications.

Q: How can I verify if my Nexus Dashboard is compatible with other Cisco management tools?
A: Refer to the integration and compatibility guides provided in Nexus Dashboard's official documentation.

Q: Which cloud providers are officially supported and compatible with cAPIC?
A: cAPIC is officially supported on AWS, Azure, and Google Cloud. Always check the latest documentation for any additions.

Q: Is APIC compatible with third-party SDN solutions?
A: APIC primarily focuses on Cisco's ACI environment, but it provides open APIs that can potentially integrate with third-party SDN solutions.

Q: How do I find a list of devices and software versions compatible with Nexus Dashboard?
A: Cisco's official website provides a compatibility matrix detailing devices and versions supported by Nexus Dashboard.

Q: Are there any known issues with APIC's compatibility with older Cisco routers?
A: Refer to the release notes and compatibility matrix for APIC on Cisco's official website for any known issues.

Q: How often does Cisco update the compatibility matrix for APIC?
A: Cisco regularly updates the compatibility matrix, especially during major software releases. Check the official website for the latest updates.

Q: Can I run cAPIC on private cloud infrastructures, or is it compatible only with public clouds?
A: While cAPIC is designed for public clouds, you can deploy it in private cloud setups, but official support might vary. Check documentation for specifics.

Q: Is Nexus Dashboard compatible with non-Cisco network devices?
A: Nexus Dashboard primarily supports Cisco devices, but its open APIs might allow for some level of integration with non-Cisco devices.

Q: How do I resolve compatibility issues after a cAPIC update?
A: Review the release notes for any known issues after an update. If you face issues, contact Cisco TAC for assistance.

Q: Which versions of VMware are compatible with APIC controller deployments?
A: Refer to the APIC installation and deployment guide on Cisco's website for details on VMware compatibility.

Q: How do I check if my security solutions are compatible with Nexus Dashboard?
A: Nexus Dashboard's documentation will provide information on compatible security solutions.

Q: Are there any tools to validate compatibility before an APIC deployment?
A: Cisco provides various pre-deployment validation tools. Always check the Cisco website or contact a Cisco representative for recommendations.

Q: How do I troubleshoot compatibility issues between cAPIC and my storage solution?
A: Review cAPIC's logs and documentation, and consider reaching out to Cisco TAC for in-depth troubleshooting.

Q: Is APIC backward compatible with previous software versions?
A: While APIC aims to provide backward compatibility, always refer to release notes and documentation for any exceptions.

Q: How can I ensure plugins in Nexus Dashboard are compatible with my current version?
A: Always ensure that plugins or applications are tested in a non-production environment first and refer to the Nexus Dashboard's compatibility matrix.

Q: How do I verify if my current Cisco IOS version is compatible with APIC?
A: Use the 'show version' command on your Cisco device, then compare the IOS version with the APIC's supported version list found in its release notes.

Q: How can I ensure my virtual network functions are compatible with cAPIC?
A: Test the virtual network functions in a lab environment with cAPIC. Ensure functions are working correctly and there are no error messages in the cAPIC logs.

Q: How do I confirm if Nexus Dashboard supports my current data center topology?
A: Deploy Nexus Dashboard in a test environment mimicking your data center topology. Monitor its performance and check for any anomalies or error messages.

Q: How do I determine if my current Nexus switch models are APIC compatible?
A: Use the 'show version' command on your Nexus switch. Then, cross-check the model and software version with the APIC's supported device list.

Q: How can I troubleshoot potential compatibility issues between APIC and third-party applications?
A: Enable detailed logging in APIC, try integrating the third-party application, and check the logs for any error messages or warnings.

Q: How do I ensure my network protocols are compatible with Nexus Dashboard?
A: In Nexus Dashboard, navigate to the protocol configurations and verify if your desired protocols are listed and can be enabled.

Q: What should I consider regarding OpenStack versions and APIC compatibility?
A: Before integrating with OpenStack, ensure that the OpenStack version has APIs that match the APIC's expected API versions.

Q: How do I validate if my current firewall model works with cAPIC?
A: In a lab environment, deploy your firewall model and try to integrate it with cAPIC. Monitor the integration process for any issues or error messages.

Q: What steps should I take for integrating APIC in multi-cloud environments?
A: Start by testing one cloud provider at a time with APIC. Ensure that network configurations, security policies, and connectivity are properly set up for each cloud.

Q: How do I check Nexus Dashboard compatibility with my current Cisco UCS setup?
A: Deploy Nexus Dashboard in a lab setup with the same UCS version. Monitor its performance and integration process for any incompatibilities.

Q: How can I troubleshoot compatibility issues after a cAPIC update?
A: Roll back cAPIC to its previous version, then update again while monitoring logs closely. Look for any error messages related to the update.

Q: What steps can I take to ensure my monitoring solution works with APIC?
A: Test your monitoring solution in a test environment with APIC. Ensure that metrics are being pulled correctly and there are no connectivity issues.

Q: Are there any common compatibility concerns with cAPIC and Kubernetes deployments?
A: Ensure that your Kubernetes version uses APIs and configurations that are supported by cAPIC. Test the deployment in a lab setup before a full-scale deployment.

Q: How can I validate Nexus Dashboard's compatibility with my WAN setup?
A: In a lab environment, replicate your WAN setup and integrate it with Nexus Dashboard. Monitor the integration process and performance for any issues.

Q: How can I determine if my storage solutions will work with APIC?
A: Test your storage solutions in a non-production environment with APIC. Ensure data can be read/written without issues and with expected performance.

Q: How can I check if cAPIC will work with non-Cisco hypervisors?
A: Deploy cAPIC in a lab environment with the non-Cisco hypervisor. Monitor its performance and stability to ensure compatibility.

Q: What steps should I follow to ensure my load balancers are supported by Nexus Dashboard?
A: In Nexus Dashboard, navigate to the supported devices/services list and check for your load balancer model. If uncertain, test in a lab setup.

Q: How can I check APIC's support for my current version of vCenter?
A: In APIC, navigate to the virtualization settings and check for vCenter integration options. Test the integration in a non-production environment first.

Q: How do I ensure cAPIC's compatibility with my SD-WAN solution?
A: In a lab setup, deploy your SD-WAN solution and try integrating it with cAPIC. Monitor the process closely for any compatibility issues.

Q: What can I do to prevent compatibility issues between Nexus Dashboard and third-party logging solutions?
A: Before integrating a third-party logging solution, test its integration in a lab setup with Nexus Dashboard. Ensure logs are sent/received correctly.

Q: How do I reset the password on a Cisco APIC controller?
A: To reset the password on a Cisco APIC controller, access the APIC CLI and use the 'acidiag fnvread' command.

Q: What is the default username for cAPIC?
A: The default username for cAPIC is 'admin'.

Q: How do I upgrade the firmware on a Nexus Dashboard?
A: To upgrade the firmware on Nexus Dashboard, navigate to the System Upgrade option and follow the on-screen instructions.

Q: I'm getting an error when trying to connect my APIC to the network. Any suggestions?
A: Ensure that your APIC's management IP address is correctly configured and that there are no network obstructions like firewalls blocking the connection.

Q: Why is my cAPIC not discovering new devices?
A: Check the discovery policy and ensure that the target devices are reachable from cAPIC.

Q: How do I set up VLANs in the Cisco APIC?
A: To set up VLANs in APIC, navigate to the Fabric tab and then to Access Policies. Create a new VLAN pool.

Q: What's the difference between an APIC and cAPIC?
A: APIC is a physical appliance, while cAPIC (Cloud APIC) is a version of APIC designed to run in cloud environments.

Q: How do I back up my Nexus Dashboard configurations?
A: Navigate to the Admin tab in Nexus Dashboard and use the backup and restore functionality.

Q: Is it possible to integrate third-party applications with APIC?
A: Yes, APIC provides an open API that allows integration with third-party applications.

Q: What protocols does cAPIC support for device discovery?
A: cAPIC primarily uses LLDP and CDP for device discovery.

Q: How do I troubleshoot connectivity issues between my Nexus Dashboard and its nodes?
A: Check the connectivity status and logs in the Nexus Dashboard. Ensure the nodes are reachable and there are no network issues.

Q: What are the licensing requirements for APIC?
A: Licensing for APIC is based on the number of leaf switches. Ensure you have the required licenses.

Q: Can I use REST API with the Nexus Dashboard?
A: Yes, Nexus Dashboard supports REST API for various operations.

Q: What are the best practices for securing my APIC?
A: Ensure strong password policies, regularly update the firmware, and restrict physical and network access.

Q: I've accidentally deleted a tenant in cAPIC, how do I recover it?
A: Deleted tenants cannot be directly recovered. It's recommended to restore from a backup.

Q: Is multi-site configuration supported in APIC?
A: Yes, APIC supports multi-site configurations to manage multiple data centers.

Q: How can I monitor the health of devices connected to Nexus Dashboard?
A: Use the Health tab in Nexus Dashboard to monitor the health scores of connected devices.

Q: What are the recommended backup intervals for cAPIC?
A: For cAPIC, it's recommended to take weekly backups or after significant configuration changes.

Q: How do I add a new user to Nexus Dashboard?
A: Go to the Admin tab in Nexus Dashboard, and use the user management options to add a new user.

Q: My APIC is running slow. How do I improve its performance?
A: Check for resource-intensive processes, ensure firmware is up-to-date, and consider scaling your setup.

Q: I've set up inter-tenant communication in APIC, but the traffic isn't flowing as expected. How can I troubleshoot?
A: Review the inter-tenant communication policies in APIC. Ensure there's no firewall or ACL blocking the desired traffic between tenants.

Q: cAPIC is not reflecting the correct status of my cloud-based load balancers. How can I address this?
A: Manually sync the status of load balancers in cAPIC. Ensure there's no communication issue between cAPIC and the cloud provider.

Q: I'm observing 'Insufficient Bandwidth' warnings on APIC for specific links. How can I resolve this?
A: Review the bandwidth utilization on the affected links in APIC. Consider optimizing traffic or upgrading the link capacity.

Q: After migrating VMs, I've noticed that cAPIC isn't correctly updating their statuses. How can I address this?
A: Force a manual sync of VM statuses in cAPIC. Ensure there's no network or configuration issue preventing the status update.

Q: APIC is not sending alerts to my third-party monitoring tool via Syslog. How can I troubleshoot?
A: Verify the Syslog configurations in APIC. Ensure the monitoring tool's IP and port are correctly set and there's no network issue blocking the Syslog traffic.

Q: I've configured a private network in cAPIC, but some devices aren't reachable. What could be the reason?
A: Review the private network configurations in cAPIC. Ensure all devices have the correct IP, subnet, and routing settings.

Q: APIC is showing 'Power Supply Failures' for certain devices. How can I address this?
A: Check the power supply units of the affected devices. Consider replacing faulty units or ensuring they are correctly plugged in and operational.

Q: cAPIC is not able to scale resources for my cloud-based applications. How can I troubleshoot?
A: Review the resource scaling policies in cAPIC. Ensure there's sufficient cloud resources available and cAPIC has the correct permissions to scale.

Q: I'm receiving 'Invalid Endpoint' errors on APIC when trying to integrate with an external service. How can I resolve?
A: Ensure the external service's endpoint URL is correctly set in APIC. Verify there's no DNS or network issue preventing the connection.

Q: I've deployed an EPG in cAPIC, but some VMs aren't being categorized correctly. How can I address this?
A: Review the EPG configurations in cAPIC. Ensure the VMs match the criteria set for the EPG categorization.

Q: I'm observing 'Control Plane Failures' in my APIC-managed fabric. How can I troubleshoot?
A: Check the control plane health and metrics in APIC. Review any related logs or warnings to identify the root cause.

Q: cAPIC isn't able to backup configurations to my external storage. What could be the reason?
A: Verify the backup configurations and permissions in cAPIC. Ensure there's network connectivity to the external storage.

Q: APIC is showing 'Overutilized CPU' warnings. How can I optimize my setup?
A: Review the CPU utilization on APIC. Consider optimizing processes or scaling the APIC deployment to handle the CPU demands.

Q: cAPIC isn't reflecting the correct security posture of my cloud environment. How can I synchronize the data?
A: Manually sync the security posture data in cAPIC. Ensure there's no communication issue or API rate limit with the cloud provider.

Q: I've configured a service redirection policy in APIC, but the traffic isn't being redirected. How can I troubleshoot?
A: Review the service redirection policy and its associated devices in APIC. Ensure traffic matches the criteria set for redirection.

Q: Certain cloud resources aren't being discovered by cAPIC. How can I address this?
A: Ensure that cAPIC has the necessary permissions and configurations to discover the cloud resources. Check for any network or API issues.

Q: I've observed 'High Latency' warnings on APIC for specific services. What steps should I take?
A: Review the services experiencing high latency in APIC. Check for any network congestion or misconfigurations causing the delay.

Q: I've integrated cAPIC with a cloud database, but the data sync is failing. How can I troubleshoot?
A: Verify the database configurations in cAPIC. Ensure the database is reachable and cAPIC has the correct credentials to sync data.

Q: APIC is not able to fetch firmware updates. What could be causing this?
A: Ensure APIC has a stable internet connection and the necessary permissions to fetch firmware updates. Check any proxy or firewall settings.

Q: I've set up a security policy in cAPIC, but some cloud resources aren't adhering to it. How can I troubleshoot?
A: Review the security policy configurations in cAPIC. Ensure the cloud resources are correctly set to adhere to the policy and there's no overriding configuration.

Q: I'm facing issues after upgrading my Cisco IOS XR to a new version. How can I check its compatibility with APIC?
A: Cross-reference your Cisco IOS XR version with APIC's supported version list. If not supported, consider downgrading or checking for patches.

Q: After a recent Nexus OS update, my device isn't integrating well with cAPIC. How can I troubleshoot this?
A: Check the error logs on both Nexus OS and cAPIC. Ensure the OS version is listed as compatible in cAPIC's release notes.

Q: My Cisco ASA firewall, after a firmware update, is not working seamlessly with Nexus Dashboard. What steps should I take?
A: Inspect the logs on both the ASA firewall and Nexus Dashboard. Cross-reference the firmware version with Nexus Dashboard's supported list.

Q: I updated my Cisco Catalyst IOS, and now it's having issues with APIC. How do I resolve this?
A: Refer to APIC's release notes to check if the new IOS version is supported. If not, consider reverting to a supported version.

Q: My Cisco Aironet device, post update, isn't working well with cAPIC. How can I troubleshoot?
A: Ensure the Aironet firmware version is supported by cAPIC. Check device and cAPIC logs for specific error messages.

Q: After updating Cisco Meraki firmware, it's not integrating with Nexus Dashboard. What should I do?
A: Cross-reference the Meraki firmware version with Nexus Dashboard's supported versions. Check for any known issues or patches.

Q: I've updated my Cisco Firepower device, and it's showing compatibility issues with APIC. How can I fix this?
A: Verify the Firepower version against APIC's supported list. Check both device and APIC logs for compatibility errors.

Q: Post updating Cisco UCS Manager, I'm facing issues with cAPIC. How can I address this?
A: Ensure the UCS Manager version is listed in cAPIC's compatibility matrix. Inspect error logs on both platforms for clues.

Q: I recently updated my Cisco ACI, and it's not integrating well with Nexus Dashboard. How can I troubleshoot?
A: Cross-reference the ACI version with Nexus Dashboard's supported list. Examine the logs on both platforms for integration errors.

Q: After a Cisco DNA Center update, it's not working seamlessly with APIC. What steps should I take?
A: Ensure the DNA Center version is supported by APIC. Check logs on both platforms for any incompatibility errors.

Q: My Cisco ISR router, post IOS update, is showing issues with cAPIC. How can I resolve this?
A: Cross-check the router's IOS version with cAPIC's supported list. Review device and cAPIC logs for compatibility issues.

Q: After updating the Cisco WLC version, it's not integrating well with Nexus Dashboard. What should I do?
A: Ensure the WLC version is supported by Nexus Dashboard. Check logs on both platforms for integration errors.

Q: I've updated my Cisco SD-WAN and now it's having compatibility issues with APIC. How can I troubleshoot?
A: Refer to APIC's release notes to verify SD-WAN version compatibility. Inspect logs on both systems for clues.

Q: Post updating Cisco HyperFlex, I'm facing integration issues with Nexus Dashboard. How can I address this?
A: Cross-check the HyperFlex version with Nexus Dashboard's supported versions. Examine logs for any incompatibility messages.

Q: After a Cisco FTD update, it's not working well with cAPIC. What steps should I take?
A: Ensure the FTD version is supported by cAPIC. Review logs on both systems for integration errors.

Q: My Cisco Webex device, after a recent update, isn't integrating seamlessly with Nexus Dashboard. How can I troubleshoot?
A: Cross-reference the Webex device version with Nexus Dashboard's compatibility matrix. Check logs for specific error messages.

Q: I updated my Cisco ISE and now it's showing compatibility issues with APIC. How can I fix this?
A: Verify the ISE version against APIC's supported list. Examine logs on both systems for compatibility errors.

Q: After updating Cisco Prime Infrastructure, I'm facing issues with cAPIC. How can I resolve this?
A: Cross-check the Prime Infrastructure version with cAPIC's release notes. Check logs on both platforms for issues.

Q: Post updating Cisco Jabber, it's not integrating well with APIC. What should I do?
A: Ensure the Jabber version is supported by APIC. Inspect logs on both platforms for specific errors.

Q: I've recently updated Cisco AMP for Endpoints and it's having issues with Nexus Dashboard. How can I troubleshoot?
A: Refer to Nexus Dashboard's release notes to verify AMP version compatibility. Review logs on both systems for clues.

Q: How do you reset an APIC to its factory settings?
A: To reset an APIC to factory settings, use the 'acidiag touch clean' command and then reboot the APIC.

Q: What is the default username and password for Cisco APIC?
A: The default username is 'admin' and the default password is set during the initial setup.

Q: How do you troubleshoot connectivity issues between cAPIC and the cloud?
A: Check the IPsec VPN status, verify cloud credentials, and inspect the fault list in cAPIC.

Q: What is the primary role of the Nexus Dashboard in a Cisco network?
A: Nexus Dashboard provides a single pane of glass for operations, analytics, and assurance across Cisco's data center networking products.

Q: How can you monitor the health of fabric in APIC?
A: Navigate to the APIC GUI, then System > Health Scores. Here, you can view the health scores for the entire fabric and individual nodes.

Q: What command is used to display the version of cAPIC?
A: Use the 'show version' command in the cAPIC CLI.

Q: How can I integrate third-party applications with Nexus Dashboard?
A: Use Nexus Dashboard's OpenAPIs and SDK to integrate third-party applications.

Q: What is the difference between a Tenant and a VRF in APIC?
A: A Tenant is an isolation boundary in ACI, while a VRF (Virtual Routing and Forwarding) is a layer 3 construct within a tenant to provide IP-level isolation.

Q: How do you configure an out-of-band management IP on cAPIC?
A: Navigate to the cAPIC GUI, go to Fabric > Nodes, select the desired node, and configure the out-of-band management IP.

Q: How do I backup the configuration of my APIC?
A: Navigate to APIC GUI, go to Admin > Import/Export > Configuration Export. Choose your export policy and trigger an immediate backup.

Q: I'm facing issues with my VXLAN configuration in the Nexus Dashboard. How can I troubleshoot it?
A: Check the VXLAN EVPN status, verify the BGP peering status, and inspect the related fault list in Nexus Dashboard.

Q: How can I enable SNMP on my Cisco APIC?
A: In the APIC GUI, navigate to Admin > External Management > SNMP and configure the necessary SNMP policies.

Q: What is the role of a spine switch in Cisco's ACI architecture?
A: Spine switches provide the backbone of the ACI fabric, interconnecting all leaf switches and ensuring optimal path selection.

Q: My cAPIC cluster is unstable. How can I verify the health of the cluster?
A: Use the 'show cluster extended' command in the cAPIC CLI to view cluster health and status.

Q: How do I integrate Cisco ACI with VMware vCenter using APIC?
A: In the APIC GUI, navigate to VM Networking > VMM Domains, add a new VMM domain, and provide the vCenter details.

Q: How do I upgrade the firmware on my Nexus Dashboard?
A: Navigate to the Nexus Dashboard GUI, go to Administration > Software Updates, and follow the on-screen instructions.

Q: I'm getting a certificate error when trying to access the APIC GUI. How can I resolve this?
A: The most common solution is to import and trust the APIC's self-signed certificate in your browser. Alternatively, deploy a valid certificate on the APIC.

Q: How do you configure a Layer 3 out in cAPIC?
A: In the cAPIC GUI, navigate to Networking > L3Outs, and follow the wizard to create and configure a new Layer 3 out.

Q: What is the purpose of the 'Tenant mgmt' in APIC?
A: 'Tenant mgmt' is a default tenant in APIC dedicated to management purposes, including out-of-band connectivity.

Q: How can I enable syslog forwarding in Nexus Dashboard?
A: Navigate to the Nexus Dashboard GUI, go to Administration > External Logging, and configure your syslog server details.

Q: How can I configure inter-VLAN routing in APIC?
A: In APIC, navigate to the Tenants tab, create or select a Tenant, and configure Bridge Domains and EPGs for inter-VLAN routing.

Q: How do I set up a firewall rule in cAPIC?
A: In cAPIC, go to the Security Policies tab and configure the desired firewall rules.

Q: How can I cluster multiple Nexus Dashboard instances?
A: For clustering Nexus Dashboard instances, use the Cluster Management settings and add instances.

Q: How do I set up a guest network in APIC?
A: In APIC, navigate to the Tenants tab and create a guest EPG and associate it with a guest VLAN.

Q: How do I recover a forgotten password in cAPIC?
A: For a forgotten password in cAPIC, you might need to reset the password via the cloud provider or access the cAPIC CLI.

Q: How can I view logs in Nexus Dashboard?
A: In Nexus Dashboard, navigate to the Logs tab to view system and application logs.

Q: How do I set up an IPsec VPN in APIC?
A: In APIC, go to the VPN settings and select IPsec VPN to configure the desired parameters.

Q: How can I optimize traffic flow in cAPIC?
A: For optimizing traffic in cAPIC, use Route Maps and Traffic Engineering policies.

Q: How do I configure network telemetry in Nexus Dashboard?
A: In Nexus Dashboard, navigate to the Telemetry settings and configure the desired parameters.

Q: How can I schedule tasks in APIC?
A: In APIC, navigate to the Scheduled Tasks tab and configure tasks as needed.

Q: How do I configure remote access in cAPIC?
A: For remote access in cAPIC, set up VPN settings and ensure firewall rules permit remote connections.

Q: How can I integrate APIC with other Cisco security solutions?
A: APIC can be integrated with other Cisco security solutions via APIs and connectors available in the marketplace.

Q: How do I set up a load balancer in Nexus Dashboard?
A: In Nexus Dashboard, use the Load Balancer settings under the Services tab.

Q: How can I automate tasks in APIC?
A: Tasks in APIC can be automated using API integrations, scripts, or third-party automation tools.

Q: How do I optimize storage usage in cAPIC?
A: To optimize storage in cAPIC, regularly clean up logs and old data, and use storage optimization features.

Q: How can I configure QoS policies in Nexus Dashboard?
A: In Nexus Dashboard, navigate to the QoS tab and configure the desired policies.

Q: How do I set up multi-factor authentication in APIC?
A: For multi-factor authentication in APIC, integrate with external MFA providers under the Security settings.

Q: How can I monitor network performance in cAPIC?
A: In cAPIC, navigate to the Performance Monitoring tab to monitor network performance.

Q: How do I integrate APIC with SD-WAN solutions?
A: APIC can be integrated with SD-WAN solutions using available connectors and APIs.

Q: How can I set up alerts for device failures in Nexus Dashboard?
A: In Nexus Dashboard, navigate to the Alerts tab and set up alerts for device failures.

Q: I've recently configured EtherChannel on my Cisco switches, but APIC isn't reflecting the aggregated links. How can I troubleshoot?
A: Ensure EtherChannel configurations, including member ports and mode settings, are correct on switches. Review logs on APIC for EtherChannel-related errors.

Q: I've set up VTP in my network topology, but the VLANs aren't propagating as expected on Nexus Dashboard. What steps should I take?
A: Verify VTP configurations, including domain names and version. Ensure there's connectivity between VTP servers and clients. Check Nexus Dashboard logs for VTP errors.

Q: I'm trying to deploy a leaf-spine topology using Cisco devices and facing issues with cAPIC. How can I resolve this?
A: Ensure all devices in the leaf-spine topology have the correct configurations and are reachable. Review logs on cAPIC for any topology-related errors.

Q: I've added a new Cisco router in my topology, but it's not being recognized by Nexus Dashboard. What could be the reason?
A: Verify the router is online and reachable. Ensure SNMP and other required protocols are enabled for integration with Nexus Dashboard.

Q: I've enabled RIP in my topology, but routes aren't being updated on APIC. How can I troubleshoot?
A: Ensure RIP configurations, including version and network statements, are correct. Check logs on APIC for RIP-related issues.

Q: I'm seeing issues with port-channel configurations not reflecting correctly on cAPIC. What should I do?
A: Verify port-channel configurations on the device and cross-reference with cAPIC. Ensure member ports are active and correctly aggregated.

Q: I've implemented MPLS in my network topology, but facing challenges with APIC. How do I address this?
A: Check MPLS configurations on devices and ensure labels are being correctly assigned. Review APIC logs for MPLS-related errors.

Q: I've configured private VLANs, but they're not working as expected on Nexus Dashboard. How can I troubleshoot?
A: Ensure private VLAN configurations, including primary and secondary VLANs, are correctly set. Check Nexus Dashboard logs for PVLAN-related issues.

Q: I'm trying to deploy a three-tier network architecture using Cisco devices, but facing issues with cAPIC. How can I resolve?
A: Verify the configurations for all devices in the three-tier architecture. Ensure there's proper connectivity and review cAPIC logs for errors.

Q: I've introduced a Cisco load balancer in my topology, but it's not integrating well with Nexus Dashboard. What steps should I take?
A: Ensure the load balancer is online and reachable. Verify required protocols for integration with Nexus Dashboard are enabled.

Q: I've set up a DMZ using Cisco firewalls, but facing challenges with APIC. How do I troubleshoot?
A: Check DMZ configurations on firewalls and ensure security policies are correctly set. Review logs on APIC for DMZ-related errors.

Q: After introducing a new wireless access point in my topology, it's not being detected by cAPIC. What could be the reason?
A: Ensure the wireless access point is powered on and reachable. Verify SNMP settings and other required configurations for integration with cAPIC.

Q: I'm trying to implement a point-to-point link using Cisco devices, but facing issues with Nexus Dashboard. How do I address this?
A: Check configurations on devices for the point-to-point link. Ensure there's connectivity and review Nexus Dashboard logs for any errors.

Q: After enabling PIM for multicast routing, the configurations aren't reflecting on APIC. What should I do?
A: Ensure PIM configurations, including mode and DR priority, are correct. Review logs on APIC for PIM-related errors.

Q: I'm facing issues with IP SLA configurations on Nexus Dashboard. How can I troubleshoot?
A: Verify IP SLA configurations on devices and ensure probes are being sent/received. Check Nexus Dashboard logs for IP SLA-related issues.

Q: I've implemented a new VPN topology using Cisco devices, but it's not working seamlessly with cAPIC. How do I resolve?
A: Check VPN configurations on devices, including peer IPs, encryption settings, and keys. Review logs on cAPIC for VPN-related errors.

Q: I've added a new Cisco switch stack in my topology, but it's not being recognized by Nexus Dashboard. What steps should I take?
A: Ensure the switch stack is correctly configured and all member switches are active. Check Nexus Dashboard logs for stack-related issues.

Q: I'm seeing challenges with configuring Q-in-Q tunneling on APIC. How can I troubleshoot?
A: Verify Q-in-Q configurations, including outer and inner VLAN tags. Review logs on APIC for tunneling-related errors.

Q: I've set up a new WAN link using Cisco devices, but facing issues with cAPIC. How can I address this?
A: Check configurations on devices for the new WAN link. Ensure there's connectivity and review logs on cAPIC for any WAN-related errors.

Q: I've configured a new DHCP pool, but it's not working as expected on Nexus Dashboard. How can I troubleshoot?
A: Ensure DHCP pool configurations, including IP range and options, are correctly set. Check Nexus Dashboard logs for DHCP-related issues.

Q: How do I initialize a new APIC controller?
A: Use the setup script on APIC, enter the required details such as name, IP, password, and follow the on-screen prompts.

Q: What's the default admin password for APIC?
A: The default password is 'cisco123', but it's recommended to change it during initial setup.

Q: How can I integrate cAPIC with AWS?
A: Deploy the cAPIC OVA in your AWS VPC, set up the necessary IAM roles and policies, and then use the cAPIC GUI to integrate with AWS services.

Q: What is the purpose of the Nexus Dashboard?
A: Nexus Dashboard provides a single pane for managing, monitoring, and troubleshooting Cisco data center products.

Q: How do I add a new device to Nexus Dashboard?
A: Navigate to the 'Inventory' tab, click 'Add Device', and enter the device details.

Q: Can I integrate APIC with Nexus Dashboard?
A: Yes, you can integrate APIC with Nexus Dashboard for centralized monitoring and management.

Q: What topologies are supported by cAPIC in Azure?
A: cAPIC in Azure supports multiple topologies including two-tier, three-tier, and anycast services.

Q: How can I backup my APIC configuration?
A: Use the APIC GUI, navigate to 'Admin' > 'Export Policies' and select the configuration items you want to backup.

Q: What is the recommended method for upgrading cAPIC?
A: Always follow the Cisco recommended upgrade path, typically using the cAPIC GUI or CLI, and ensure backups are taken before starting.

Q: How do I configure VXLAN on Nexus?
A: Use the 'feature nv overlay' command, configure the NVE interface, and then define your VNI and associate it with a VLAN.

Q: Can I monitor traffic flow in real-time on Nexus Dashboard?
A: Yes, Nexus Dashboard offers real-time flow analytics to monitor traffic patterns and detect anomalies.

Q: How to enable REST API on APIC?
A: REST API is enabled by default on APIC. You can use tools like Postman to interact with the APIC REST API.

Q: What are the key features of cAPIC in AWS?
A: cAPIC in AWS offers native AWS integration, cloud-native services, policy-driven automation, and consistent policy model across hybrid environments.

Q: How do I reset my APIC admin password?
A: Use the APIC console, boot in rescue mode, and follow the prompts to reset the password.

Q: How is network policy applied in cAPIC for Azure?
A: Network policies in cAPIC for Azure are applied using Azure Network Security Groups (NSGs) and are mapped to cAPIC contracts.

Q: Can I integrate APIC with third-party monitoring tools?
A: Yes, APIC offers APIs and SNMP support which can be used to integrate with third-party monitoring tools.

Q: How to troubleshoot connectivity issues in Nexus Dashboard?
A: Use the built-in health dashboard, check device status, and verify network connectivity between Nexus Dashboard and devices.

Q: What's the role of spines in a leaf-spine topology in ACI?
A: Spines provide a high-speed, low-latency backbone interconnecting leaf switches, ensuring all devices are equidistant.

Q: How do I scale out my ACI fabric?
A: Add more leaf and spine nodes as needed and ensure they're correctly configured and integrated into the existing fabric.

Q: What's the maximum number of leaf switches supported in a single APIC cluster?
A: As of the last update, an APIC cluster can manage up to 400 leaf switches, but always refer to the latest Cisco documentation for any updates.

Q: How do I reset the admin password on APIC?
A: Use the APIC console and choose the 'Reset admin password' option. You'll need physical or KVM-over-IP access to the console.

Q: I'm unable to access the APIC GUI. What should I do?
A: Check network connectivity to APIC. If there's connectivity, ensure the APIC services are running. Restarting the APIC might also help.

Q: How can I backup the APIC configuration?
A: Go to the APIC GUI, navigate to Admin > Import/Export and select 'Export Configuration'. Choose your desired format and download.

Q: Why am I seeing a 'certificate expired' error on my cAPIC?
A: The SSL certificate has expired. You'll need to renew and install a new certificate. Navigate to the certificate management section in cAPIC to do this.

Q: How do I add a new node to my existing cAPIC cluster?
A: Navigate to the Cluster Management section in the cAPIC GUI. Click 'Add Node', and input the new node details.

Q: I can't connect to Nexus Dashboard. Any suggestions?
A: Ensure that the Nexus Dashboard is powered on and that there's network connectivity. You can also try restarting the Nexus Dashboard.

Q: How do I integrate APIC with Nexus Dashboard?
A: In Nexus Dashboard, navigate to 'Data Sources' and add APIC as a new data source. Input the APIC details and credentials.

Q: What's the best practice for upgrading cAPIC firmware?
A: Always backup your configuration first. Review the release notes for upgrade paths. Upgrade in a maintenance window and follow a tested upgrade procedure.

Q: How do I monitor traffic flow in my network using APIC?
A: Use the APIC GUI, navigate to the 'Tenants' section, and under the specific tenant, you'll find 'Traffic Statistics' for detailed insights.

Q: My Nexus Dashboard is running slow. How can I optimize its performance?
A: Check the system resources. If they're high, consider scaling up the hardware. Also, clear any unnecessary logs and old data.

Q: (Follow up to Q10) How do I clear old logs from Nexus Dashboard?
A: Navigate to 'System Settings' in Nexus Dashboard. Under 'Log Management', select and delete old log entries.

Q: How can I set up role-based access control (RBAC) in cAPIC?
A: In the cAPIC GUI, navigate to 'Admin' > 'AAA' > 'Roles and Privileges'. Create roles and assign privileges as needed.

Q: I suspect a node failure in my APIC cluster. How do I verify?
A: Go to the APIC GUI, under 'Fabric Membership'. Any failed nodes will be indicated there.

Q: How do I create a new VLAN in Nexus Dashboard?
A: In Nexus Dashboard, navigate to 'Network Settings'. Under 'VLAN Management', click 'Add VLAN' and specify the details.

Q: My cAPIC is not discovering a newly added device. What could be the issue?
A: Ensure that the device is powered on and connected. Check the discovery policies in cAPIC and ensure the device IP range is included.

Q: Can I integrate third-party monitoring tools with APIC?
A: Yes, APIC supports integration with many third-party monitoring tools using APIs. Navigate to 'External Management' in APIC to set it up.

Q: (Follow up to Q16) How do I generate API tokens for third-party integration in APIC?
A: In the APIC GUI, go to 'Admin' > 'AAA' > 'Token Management'. Create and generate a new token for the third-party tool.

Q: Why is my cAPIC dashboard showing a mismatch in policy configuration?
A: This usually indicates a discrepancy between the intended and applied policies. Review the policies in cAPIC and reconcile any differences.

Q: How can I set up alerts in Nexus Dashboard?
A: Navigate to 'Alert Management' in Nexus Dashboard. Create and configure new alerts based on your criteria.

Q: I'm facing latency issues in my network. How can I use APIC to diagnose?
A: In APIC, use the 'Health Scores' feature to monitor network health. Check for any nodes or links with low health scores and investigate further.

Q: How do I check if my Cisco IOS XR version is compatible with the latest APIC release?
A: Use the 'show version' command on your device running Cisco IOS XR, then cross-reference with the APIC's supported version list.

Q: What steps should I follow to ensure compatibility between Nexus OS versions and cAPIC?
A: Maintain a record of tested and supported Nexus OS versions from cAPIC's release notes and ensure your devices run on these versions.

Q: How can I determine if my current Cisco ASA firmware version is supported by Nexus Dashboard?
A: Cross-reference your Cisco ASA firmware version with the supported list available on Nexus Dashboard's official release notes.

Q: Is there a way to ensure compatibility between different Cisco Catalyst IOS versions and APIC?
A: For Catalyst switches, refer to APIC's release notes to find a list of supported IOS versions and ensure your devices are updated accordingly.

Q: What should I consider when checking version compatibility between Cisco Aironet devices and cAPIC?
A: Always check the release notes of cAPIC, which provides details on supported Cisco Aironet firmware versions.

Q: How do I validate that my Cisco Meraki firmware version is supported by Nexus Dashboard?
A: Refer to Nexus Dashboard's release notes or compatibility matrix to validate support for specific Cisco Meraki firmware versions.

Q: What are the considerations for version compatibility between Cisco Firepower devices and APIC?
A: Cross-reference your Firepower device's version with the APIC's release notes, which provides a list of supported versions.

Q: How can I ensure my Cisco UCS Manager version is compatible with cAPIC?
A: Check the compatibility matrix provided by cAPIC for a list of supported Cisco UCS Manager versions.

Q: What steps should I follow to validate compatibility between my Cisco ACI version and Nexus Dashboard?
A: Ensure you're using a Nexus Dashboard version that explicitly supports your ACI version, as mentioned in its release notes.

Q: How can I determine if my Cisco DNA Center version is compatible with APIC?
A: Consult the APIC's documentation, especially the compatibility section, to verify support for specific Cisco DNA Center versions.

Q: What steps should I take to troubleshoot version compatibility issues between Cisco ISR routers and cAPIC?
A: Cross-check the ISR router's IOS version with the supported list from cAPIC's release notes. If issues persist, check for known bugs or conflicts.

Q: How do I validate Nexus Dashboard's support for different Cisco WLC versions?
A: Consult the Nexus Dashboard documentation to verify support for specific Cisco Wireless LAN Controller (WLC) versions.

Q: How can I ensure that my Cisco SD-WAN version is compatible with APIC?
A: Cross-reference your SD-WAN version with the supported list available in APIC's release notes.

Q: What are the best practices to ensure compatibility between Cisco HyperFlex versions and Nexus Dashboard?
A: Maintain a record of tested and supported Cisco HyperFlex versions from Nexus Dashboard's release notes.

Q: How do I verify if my current Cisco FTD version is compatible with cAPIC?
A: For Cisco FTD devices, refer to cAPIC's release notes or compatibility matrix to ensure your device's version is supported.

Q: What steps can I take to ensure version compatibility between my Cisco Webex devices and Nexus Dashboard?
A: Consult Nexus Dashboard's documentation and release notes to determine support for specific Webex device versions.

Q: How do I determine if my Cisco ISE version is fully supported by APIC?
A: Cross-reference your ISE version with the supported list available in APIC's official documentation.

Q: How can I validate that my Cisco Prime Infrastructure version works seamlessly with cAPIC?
A: Ensure your Cisco Prime Infrastructure version is listed as supported in cAPIC's release notes or compatibility matrix.

Q: What considerations should I keep in mind for APIC's compatibility with different Cisco Jabber versions?
A: Consult the APIC's documentation to determine compatibility with specific Cisco Jabber versions.

Q: How do I check if my Cisco AMP for Endpoints version is compatible with Nexus Dashboard?
A: Cross-reference your Cisco AMP version with the supported list provided in Nexus Dashboard's release notes.

Q: How do I configure an SNMP trap on the APIC?
A: To configure an SNMP trap on APIC, navigate to the Admin tab and configure SNMP under External Management.

Q: Is there a way to integrate cAPIC with AWS VPCs?
A: Yes, cAPIC can be integrated with AWS VPCs using the cloud integration features.

Q: What are the high availability options for Nexus Dashboard?
A: Nexus Dashboard supports clustering for high availability. Ensure you have multiple instances running in sync.

Q: I've lost connectivity to my APIC, how can I regain access?
A: Check physical connections, and if necessary, access the device console directly for troubleshooting.

Q: How do I increase storage on my cAPIC?
A: To increase storage in cAPIC, provision additional storage in your cloud provider and attach it to the cAPIC instance.

Q: Can I segment traffic in APIC using VRFs?
A: Yes, APIC supports traffic segmentation using VRFs. Create a VRF under the Tenant settings.

Q: How do I migrate workloads from one APIC to another?
A: For workload migration, use tools like Cisco ACI Multi-Site Orchestrator (MSO) to manage and migrate between APICs.

Q: Is there a way to visualize traffic flows in Nexus Dashboard?
A: Yes, Nexus Dashboard provides visualization tools under the Network Insights feature.

Q: Can I integrate APIC with Active Directory for user authentication?
A: Yes, APIC can be integrated with Active Directory under the External Authentication settings.

Q: What are the supported VPN protocols in cAPIC?
A: cAPIC supports IPsec and SSL VPN protocols for secure connections.

Q: How do I restore Nexus Dashboard from a backup?
A: To restore Nexus Dashboard, use the backup and restore functionality under the Admin tab.

Q: How can I get the list of devices managed by APIC?
A: Navigate to the Fabric Inventory in APIC to get a list of managed devices.

Q: Is there a dashboard in Nexus Dashboard that shows node health?
A: Yes, Nexus Dashboard has a Health tab that shows the health of all nodes.

Q: How do I enforce security policies across multiple APICs?
A: Use Cisco ACI Multi-Site Orchestrator (MSO) to enforce policies across multiple APICs.

Q: Can I rollback configurations in cAPIC?
A: Yes, cAPIC provides configuration versioning and rollback features.

Q: How do I enable logging in APIC?
A: Enable logging in APIC under the Admin tab, and configure Syslog or local logging as required.

Q: Is there a way to get notifications from Nexus Dashboard?
A: Yes, Nexus Dashboard provides notification settings for alerts and alarms.

Q: How often should I update the firmware of cAPIC?
A: It's recommended to check for firmware updates every 3-6 months or when critical patches are released.

Q: Can I integrate Nexus Dashboard with other monitoring tools?
A: Yes, Nexus Dashboard supports integration with various monitoring tools via its open API.

Q: How do I troubleshoot high CPU usage on APIC?
A: Check the processes running on APIC and identify any resource-intensive tasks. Consider scaling resources or optimizing configurations.

Q: I've configured a new OSPF area in my network topology, but it's not being recognized by APIC. How do I troubleshoot?
A: Ensure OSPF configurations are correct and consistent across devices. Check logs on both devices and APIC for OSPF-related errors.

Q: I'm facing issues with VLAN configurations not reflecting on Nexus Dashboard. What should I do?
A: Verify VLAN configurations on the device and cross-reference with Nexus Dashboard. Ensure there are no conflicting VLAN IDs or settings.

Q: How can I resolve issues related to BGP peering not establishing between devices in my topology using cAPIC?
A: Check BGP configurations on the devices and ensure AS numbers, and peer IPs are correct. Review logs on cAPIC for BGP errors.

Q: I'm trying to implement a hub-and-spoke topology using Cisco routers but facing connectivity issues. How do I address this?
A: Ensure hub-and-spoke configurations are correctly set, and there are no IP conflicts. Check routing tables on the routers and verify connectivity.

Q: After introducing a new Nexus switch into my topology, it's not being detected by cAPIC. What steps should I take?
A: Verify the Nexus switch is online and reachable. Ensure SNMP and other required protocols are enabled for integration with cAPIC.

Q: I've configured a new multicast group, but it's not working as expected on Nexus Dashboard. How can I troubleshoot?
A: Ensure multicast configurations, including group addresses and source IPs, are correctly set. Check Nexus Dashboard logs for multicast-related errors.

Q: I'm facing challenges with STP configurations causing loops in my network topology. How do I resolve this with APIC?
A: Verify STP configurations and ensure there are no conflicting settings. Check APIC logs for STP-related errors or loops.

Q: After implementing EIGRP in my topology, some routes are not being advertised on cAPIC. How do I address this?
A: Ensure EIGRP configurations, including AS numbers and network statements, are correct. Check logs on cAPIC for EIGRP-related issues.

Q: I've added a new Cisco ASA firewall to my topology, but it's not integrating well with Nexus Dashboard. What should I do?
A: Verify the ASA firewall is online and reachable. Ensure required protocols for integration with Nexus Dashboard are enabled.

Q: I'm trying to implement a redundant topology using HSRP, but facing issues with cAPIC. How can I troubleshoot?
A: Check HSRP configurations, including virtual IPs and group numbers. Verify there are no IP conflicts and review cAPIC logs for HSRP errors.

Q: After updating my network topology, certain ACLs are not working as expected on APIC. How do I fix this?
A: Review ACL configurations and ensure they align with the updated topology. Check logs on APIC for ACL-related errors.

Q: I've introduced QoS configurations in my topology, but they're not reflecting correctly on Nexus Dashboard. What steps should I take?
A: Verify QoS configurations, including bandwidth settings and policies, are correctly set. Review Nexus Dashboard logs for QoS-related errors.

Q: After a topology change, my VPN configurations on cAPIC are causing connectivity issues. How can I resolve this?
A: Check VPN configurations, including peer IPs, encryption settings, and keys. Review logs on cAPIC for VPN-related errors.

Q: I've added a new Cisco wireless controller to my topology, but it's having integration issues with APIC. How do I address this?
A: Ensure the wireless controller is online and reachable. Verify required protocols for integration with APIC are enabled.

Q: I'm trying to implement VxLAN in my topology, but facing challenges with Nexus Dashboard. How can I troubleshoot?
A: Verify VxLAN configurations, including VNI and multicast settings, are correct. Check Nexus Dashboard logs for VxLAN-related errors.

Q: After changing my network topology, certain NAT configurations are not working seamlessly with cAPIC. What should I do?
A: Ensure NAT configurations align with the updated topology. Check logs on cAPIC for NAT-related errors or conflicts.

Q: I've introduced LACP in my topology for link aggregation, but it's causing issues on APIC. How can I fix this?
A: Check LACP configurations, including port channel and member port settings. Review logs on APIC for LACP-related issues.

Q: I'm facing challenges with GRE tunnel configurations in my topology using Nexus Dashboard. How do I address this?
A: Verify GRE tunnel configurations, including source and destination IPs, are correctly set. Check Nexus Dashboard logs for GRE-related errors.

Q: After a topology update, certain port security configurations on APIC are causing issues. How can I resolve?
A: Review port security configurations and ensure they align with the updated topology. Check logs on APIC for port security-related issues.

Q: I've implemented a new DMVPN topology, but it's not working as expected with Nexus Dashboard. How can I troubleshoot?
A: Ensure DMVPN configurations, including tunnel IPs and NHRP settings, are correct. Review logs on Nexus Dashboard for DMVPN-related errors.

Q: I'm facing 'Policy Conflicts' warnings on APIC. How can I resolve them?
A: Review the conflicting policies on APIC. Consider revising or consolidating policies to ensure they don't override or conflict with each other.

Q: cAPIC isn't integrating properly with my multi-cloud environment. How can I troubleshoot?
A: Ensure that cAPIC has the necessary API keys, permissions, and configurations for each cloud provider. Review logs for any integration errors.

Q: After a security patch, I'm unable to make API calls to APIC. What could be the reason?
A: Verify the API access settings in APIC. Ensure the security patch didn't inadvertently modify permissions or access controls.

Q: I've deployed an application profile in cAPIC, but it's not taking effect. How can I address this?
A: Review the application profile configurations in cAPIC. Ensure it's correctly associated with the intended devices and networks.

Q: APIC is failing to establish a secure connection with external services. How can I troubleshoot?
A: Verify the SSL/TLS settings on APIC. Ensure the external services have valid certificates and there's no protocol mismatch.

Q: I've observed increased packet drops in my cAPIC-managed environment. How can I resolve this?
A: Review the network path in the cAPIC environment. Check for any misconfigurations, congestion, or hardware issues causing the packet drops.

Q: A specific module on APIC keeps crashing. How can I address this issue?
A: Check the logs related to the crashing module on APIC. Consider restarting the module or APIC if necessary. Ensure the module is compatible with the current APIC version.

Q: cAPIC isn't reflecting the accurate resource utilization of my cloud environment. How can I troubleshoot?
A: Manually sync resource utilization data in cAPIC. Ensure there's no API rate limits or communication issues with the cloud provider.

Q: I've implemented QoS policies on APIC, but some devices aren't adhering to them. Why might this be?
A: Verify the QoS configurations on APIC. Ensure the devices support the applied QoS settings and there's no overriding configuration on the devices.

Q: I'm unable to integrate cAPIC with my on-premises data center. What steps should I take?
A: Ensure that cAPIC has the necessary network configurations and permissions to communicate with the on-premises data center. Review any firewall or routing issues.

Q: I've observed 'Session Limit Exceeded' warnings on APIC. How can I address this?
A: Review the active sessions on APIC. Consider optimizing or increasing the session limit based on the requirements.

Q: Certain security policies applied via cAPIC are causing connectivity issues. How can I troubleshoot?
A: Identify the security policies causing the issues. Review their configurations in cAPIC and ensure they are correctly set to allow the required traffic.

Q: APIC isn't able to discover new devices added to the network. What could be the reason?
A: Ensure the new devices are reachable and have the necessary configurations for discovery by APIC. Check for any network issues preventing the discovery.

Q: I've set up analytics in cAPIC, but the data seems inconsistent. How can I address this?
A: Review the data collection and processing settings in cAPIC. Ensure the analytics module is correctly configured and receiving the necessary data.

Q: My APIC cluster is facing synchronization issues. How can I troubleshoot?
A: Ensure all nodes in the APIC cluster are online and reachable. Review any network or synchronization issues in the cluster logs.

Q: After a failover, cAPIC isn't redirecting traffic as expected. How can I resolve this?
A: Review the failover settings and traffic redirection configurations in cAPIC. Ensure the devices and networks are correctly set for the desired traffic flow post-failover.

Q: I've observed 'Storage I/O Errors' on APIC. What steps should I take?
A: Check the storage infrastructure connected to APIC. Ensure there's no hardware issues or misconfigurations causing the I/O errors.

Q: cAPIC isn't able to establish a VPN connection to my cloud provider. How can I troubleshoot?
A: Review the VPN configurations in cAPIC. Ensure the cloud provider's VPN endpoint is reachable and the necessary settings are correctly configured.

Q: APIC is showing 'High Temperature' warnings. What could be causing this?
A: Ensure the APIC device is in a properly ventilated environment. Consider checking the hardware for any issues or malfunctions causing overheating.

Q: I'm unable to access certain modules in cAPIC after an upgrade. How can I address this?
A: Verify the permissions and access controls post-upgrade. Ensure the upgrade didn't modify or restrict access to certain modules in cAPIC.

Q: How can I determine if my current SFP modules are compatible with APIC-connected devices?
A: Check the specifications of your SFP modules and cross-reference with the supported module list for APIC-connected devices.

Q: What steps should I take to ensure my containerized applications work well with cAPIC?
A: Deploy your containerized applications in a test environment with cAPIC, ensuring they communicate correctly and without latency issues.

Q: How can I validate the compatibility of my current VPN setup with Nexus Dashboard?
A: Set up a test VPN connection in Nexus Dashboard matching your current configuration and monitor its stability and performance.

Q: How can I check if my current Nexus OS is compatible with the latest APIC release?
A: Use the 'show version' command on your Nexus device, then cross-reference the OS version with the APIC's supported version list.

Q: What can I do to ensure seamless integration between APIC and my current server infrastructure?
A: Test the integration between APIC and a few servers from your infrastructure in a lab setup, ensuring data flow and control commands work correctly.

Q: How can I ensure that my current network segmentation setup is compatible with Nexus Dashboard?
A: Create a test environment mimicking your network segmentation setup and try integrating it with Nexus Dashboard, monitoring for any anomalies.

Q: How do I validate if my current orchestration tools will integrate well with APIC?
A: In a lab setup, integrate your orchestration tools with APIC, ensuring that orchestration commands and feedback loops work seamlessly.

Q: What steps should I take to ensure my load balancers work seamlessly with cAPIC?
A: Test your load balancers in a lab environment with cAPIC, ensuring traffic distribution and health checks function correctly.

Q: How can I determine if my current IDS/IPS solutions will work with APIC?
A: Integrate a few of your IDS/IPS devices with APIC in a test environment and monitor for any issues or incompatibilities.

Q: How can I validate the compatibility of my current backup solutions with Nexus Dashboard?
A: In a non-production setup, integrate your backup solutions with Nexus Dashboard, ensuring backups are created and restored correctly.

Q: How can I troubleshoot potential compatibility issues between my VM configurations and APIC?
A: Replicate your VM configurations in a test environment with APIC and monitor VM performance and interactions with APIC.

Q: What considerations should I keep in mind regarding cAPIC's compatibility with different database solutions?
A: Ensure that your database solutions use standard protocols and configurations that cAPIC supports. Test a few databases in a lab setup with cAPIC.

Q: How can I ensure that my current network monitoring tools work seamlessly with Nexus Dashboard?
A: Integrate a few of your network monitoring tools with Nexus Dashboard in a test setup, ensuring they pull data and metrics correctly.

Q: How do I check if my current QoS configurations are compatible with APIC?
A: Set up a few of your QoS configurations in a lab environment with APIC and monitor the performance and adherence to the QoS rules.

Q: What steps can I take to ensure my wireless configurations are supported by cAPIC?
A: Test your wireless configurations in a non-production environment with cAPIC, ensuring stable connectivity and performance.

Q: How do I determine if my current packet capture tools are compatible with Nexus Dashboard?
A: Integrate your packet capture tools with Nexus Dashboard in a lab setup, ensuring packets are captured and analyzed correctly.

Q: How can I ensure that my VLAN configurations are supported by APIC?
A: Create a few VLANs in a test setup with APIC and monitor their performance and stability.

Q: How do I validate that my current cloud management platform works seamlessly with cAPIC?
A: In a lab environment, integrate your cloud management platform with cAPIC, ensuring seamless control and data flow.

Q: What can I do to ensure compatibility between APIC and my current network automation tools?
A: Test your network automation tools in a lab setup with APIC, ensuring they execute tasks and receive feedback correctly.

Q: How can I check if my current logging solutions are compatible with Nexus Dashboard?
A: Integrate a few of your logging solutions with Nexus Dashboard in a test environment, ensuring logs are sent and received correctly.

Q: How do I integrate cAPIC with external authentication systems like LDAP?
A: In the cAPIC GUI, navigate to 'Admin' > 'External Authentication' and configure the LDAP settings with the necessary details.

Q: My APIC is running out of storage. How can I increase its capacity?
A: Consider adding more storage capacity to the APIC node or offloading older logs and data to external storage systems.

Q: How can I configure email alerts for critical events in Nexus Dashboard?
A: Navigate to 'Alert Management' in Nexus Dashboard. Create an alert and specify the email action for notifications.

Q: Why am I unable to deploy policies from cAPIC to the underlying infrastructure?
A: Ensure that the underlying devices are reachable and there's no network connectivity issue. Also, validate the policy definitions in cAPIC.

Q: How do I integrate APIC with external logging servers?
A: In the APIC GUI, navigate to 'Admin' > 'External Logging' and configure the external server's IP address and other details.

Q: How do I troubleshoot VXLAN issues in my network using Nexus Dashboard?
A: Use the 'VXLAN Insights' feature in Nexus Dashboard. It provides visibility into VXLAN tunnels and potential issues.

Q: How can I configure IP address pools in cAPIC?
A: Navigate to 'Network Settings' in the cAPIC GUI. Under 'IP Address Management', define and manage IP address pools.

Q: My APIC is not syncing with other nodes in the cluster. What should I do?
A: Check the cluster connectivity and ensure all nodes are reachable. If there's an issue, consider restarting the affected APIC node.

Q: How do I set up multi-site orchestration with Nexus Dashboard?
A: In Nexus Dashboard, navigate to 'Multi-Site' and follow the setup wizard to configure multi-site orchestration.

Q: How can I monitor interface errors in APIC?
A: In the APIC GUI, navigate to 'Fabric' > 'Inventory' > 'Interfaces'. Look for any interfaces with errors or abnormal status.

Q: (Follow up to Q10) How can I reset an interface in APIC?
A: Navigate to the specific interface in APIC, right-click and select 'Reset Interface' from the context menu.

Q: Why is my cAPIC dashboard not displaying real-time data?
A: Ensure that the data collection agents are running. If they're not, consider restarting the cAPIC or the data collection services.

Q: How can I configure SNMP in Nexus Dashboard for external monitoring?
A: Navigate to 'System Settings' in Nexus Dashboard. Under 'SNMP Configuration', set up SNMP parameters and community strings.

Q: My cAPIC is frequently disconnecting from the network. What could be the cause?
A: Check the physical connections, cables, and network devices between cAPIC and the rest of the network. Also, verify the network settings in cAPIC.

Q: How can I set up a VPN connection in APIC?
A: In the APIC GUI, navigate to 'Network' > 'VPN Configuration'. Define the VPN parameters and establish the connection.

Q: Can I integrate Nexus Dashboard with cloud-based monitoring tools?
A: Yes, Nexus Dashboard supports API integrations. Configure the integration settings in Nexus Dashboard under 'External Monitoring'.

Q: (Follow up to Q16) How do I obtain the API key for such integrations in Nexus Dashboard?
A: Navigate to 'API Management' in Nexus Dashboard. Generate a new API key for the desired integration.

Q: I'm seeing packet drops in my network. How can I identify the cause using cAPIC?
A: In cAPIC, navigate to 'Analytics' and look for any anomalies or spikes in traffic. Check the affected devices and links for potential issues.

Q: How do I set up user roles and permissions in Nexus Dashboard?
A: In Nexus Dashboard, go to 'Admin' > 'User Management'. Define roles, assign permissions, and associate them with users.

Q: How can I migrate data from one APIC to another?
A: Backup the configuration from the source APIC (Admin > Import/Export). Restore this backup on the target APIC using the same Import/Export feature.

Q: How can I determine if my router's firmware is compatible with the latest APIC release?
A: Update your router to a known compatible firmware version from APIC's release notes before integrating with APIC.

Q: What steps can I take to ensure my VMs run smoothly on cAPIC?
A: Ensure that your VMs are using supported OS versions and configurations for cAPIC. Regularly patch and update the VMs for optimal performance.

Q: How can I verify Nexus Dashboard's compatibility with my current firewall models?
A: Create a test environment and try integrating your firewall with Nexus Dashboard. Monitor for any errors or issues during the integration process.

Q: How can I ascertain that my Nexus series switches work seamlessly with cAPIC?
A: Check the model and firmware version of your Nexus switches. Cross-check with cAPIC's supported devices list to ensure compatibility.

Q: How can I troubleshoot any compatibility issues between APIC and my VoIP setup?
A: Isolate any issues in the VoIP setup, check APIC's logs for related errors, and adjust configurations based on error messages or warnings.

Q: What's the best way to ensure my VPN configurations are compatible with Nexus Dashboard?
A: Set up a test VPN connection in Nexus Dashboard and monitor its stability and performance to ensure compatibility.

Q: How can I confirm that my current data storage solutions will integrate well with APIC?
A: In a non-production environment, integrate your storage solution with APIC and monitor its performance and stability.

Q: What steps can I take to ensure my IoT devices work well with cAPIC?
A: Ensure your IoT devices are using standard communication protocols and test a few devices in a lab setup with cAPIC before full-scale deployment.

Q: How can I determine if my current WAN setup will work seamlessly with APIC?
A: Test your WAN setup in a lab environment with APIC, monitoring its performance and looking for any error messages.

Q: How do I ensure my current security and encryption standards are supported by Nexus Dashboard?
A: In Nexus Dashboard, navigate to the security settings and verify if your encryption standards are listed and can be enabled.

Q: How can I check for potential compatibility issues between my endpoints and APIC?
A: Use APIC's endpoint monitoring features to check for any anomalies or errors with the current endpoints.

Q: What should I consider regarding cAPIC's compatibility with different cloud service models (IaaS, PaaS, SaaS)?
A: cAPIC primarily supports IaaS models. However, ensure that the cloud service APIs and configurations match cAPIC's expectations.

Q: How can I validate that my current VLAN setup is supported by Nexus Dashboard?
A: Set up a test VLAN configuration in Nexus Dashboard and monitor its performance and stability to ensure compatibility.

Q: How do I verify that my current authentication and authorization models are compatible with APIC?
A: Test your authentication models in a lab setup with APIC, monitoring for any issues or incompatibilities.

Q: What steps can I take to ensure my DNS and DHCP configurations are supported by cAPIC?
A: Configure DNS and DHCP settings in a lab setup with cAPIC and monitor for any errors or incompatibilities.

Q: How do I determine if my current monitoring and logging solutions are compatible with Nexus Dashboard?
A: In a non-production environment, integrate your monitoring solutions with Nexus Dashboard and monitor for any incompatibilities.

Q: How can I ensure that my multicast configurations are supported by APIC?
A: Test your multicast configurations in a lab environment with APIC, ensuring that multicast traffic flows as expected.

Q: How do I check if my current load balancing solutions are compatible with cAPIC?
A: In a lab setup, integrate your load balancing solutions with cAPIC and monitor for any incompatibilities.

Q: What can I do to ensure seamless integration between APIC and my current MPLS setup?
A: In a lab environment, set up your MPLS configurations and integrate with APIC, monitoring for any issues.

Q: How do I validate that my current network management tools are supported by Nexus Dashboard?
A: Test your network management tools in a non-production setup with Nexus Dashboard, ensuring they can pull data and metrics correctly.

Q: How do I access the APIC web interface?
A: Open a web browser and navigate to the IP address assigned to the APIC. Use 'https://' before the IP.

Q: What is the default username and password for APIC?
A: The default username is 'admin' and there isn't a default password; it's set during the initial setup.

Q: How do I reset the APIC administrator password?
A: Connect to the APIC CLI, use the command 'acidiag reboot' and follow the prompts to reset the password.

Q: How do I upgrade the APIC firmware?
A: Navigate to the firmware tab in the APIC GUI and upload the desired firmware image, then apply it to the desired nodes.

Q: How can I backup the APIC configuration?
A: Go to the APIC GUI, navigate to the 'Admin' tab and select 'Export Configuration'.

Q: What's the role of the fabric in APIC?
A: The fabric provides the connectivity between APIC and the network nodes, enabling centralized management and automation.

Q: How can I monitor the health of the APIC cluster?
A: Use the APIC GUI's dashboard which provides a health score and detailed metrics for the cluster.

Q: What are Tenant, VRF, and Bridge Domain in APIC?
A: Tenant is an isolated network domain in APIC. VRF is a layer-3 segmentation within a Tenant. Bridge Domain represents a layer-2 forwarding construct within a Tenant.

Q: How can I integrate external L3 networks with APIC?
A: Use an L3Out object which connects a private network inside APIC to an external L3 network.

Q: Can I connect to APIC using SSH?
A: Yes, APIC supports SSH access to its CLI for management tasks.

Q: How do I configure user roles in APIC?
A: Navigate to the 'Admin' tab in the GUI, and under 'AAA', configure user roles and privileges.

Q: How do I troubleshoot connectivity issues in APIC?
A: Use tools like 'ICMP Ping', 'Traceroute', and 'Endpoint Tracker' available within the APIC GUI.

Q: How do I backup the APIC database?
A: Use the 'Backup & Restore' functionality under the 'Admin' tab in the APIC GUI.

Q: What's the difference between local and remote users in APIC?
A: Local users are created and managed within APIC. Remote users are authenticated via external systems like LDAP or RADIUS.

Q: Can I integrate APIC with VMware vCenter?
A: Yes, you can integrate APIC with vCenter using the VMware VDS integration.

Q: How do I configure an interface policy in APIC?
A: Go to the 'Fabric' tab in the GUI, navigate to 'Access Policies', and under 'Interface Policies', create or modify policies.

Q: What is an EPG in APIC?
A: EPG (Endpoint Group) is a grouping of similar endpoints, like VMs or physical servers, based on application requirements.

Q: How can I monitor traffic flow in APIC?
A: Use the 'SPAN' or 'ERSPAN' functionality within APIC to monitor traffic across specific ports or EPGs.

Q: What is a contract in APIC?
A: A contract defines the communication policy between different EPGs.

Q: How do I integrate APIC with external authentication systems?
A: Navigate to the 'Admin' tab in the GUI, and under 'AAA', configure external authentication sources like LDAP or RADIUS.

Q: How can I monitor the bandwidth utilization of my network using APIC?
A: In the APIC GUI, navigate to 'Fabric' > 'Operational Stats' to view bandwidth and throughput metrics.

Q: How do I set up a firewall between two segments in cAPIC?
A: In cAPIC, navigate to 'Policies' > 'Security' and define a new firewall rule between the desired segments.

Q: How can I view the active VLANs in Nexus Dashboard?
A: Go to Nexus Dashboard, navigate to 'Network Settings' > 'VLAN Management' to view and manage active VLANs.

Q: How do I integrate APIC with a VPN gateway?
A: In APIC, navigate to 'Network' > 'VPN Configuration' and configure the desired VPN gateway settings and credentials.

Q: How can I set up traffic mirroring in cAPIC?
A: In cAPIC, go to 'Policies' > 'Traffic Mirroring' and define the source and destination for mirrored traffic.

Q: How do I restore a device to its factory settings using Nexus Dashboard?
A: In Nexus Dashboard, navigate to 'Device Management', select the device, and use the 'Factory Reset' option.

Q: How can I optimize the data storage in APIC?
A: Regularly backup and purge old logs and data. Also, consider integrating APIC with external storage solutions for better optimization.

Q: How do I set up load balancing in cAPIC?
A: In cAPIC, navigate to 'Policies' > 'Load Balancing' and define the required parameters for load balancing.

Q: How can I view the system logs in Nexus Dashboard?
A: In Nexus Dashboard, go to 'System Settings' > 'Log Management' to view and manage system logs.

Q: How can I upgrade the firmware of devices managed by APIC?
A: In APIC, navigate to 'Device Management' > 'Firmware Management', select the devices, and initiate the firmware upgrade.

Q: (Follow up to Q10) How do I backup the device configuration before upgrading?
A: In APIC, go to 'Device Management', select the device, and use the 'Backup Configuration' option.

Q: How can I set up alerts for device disconnections in cAPIC?
A: In cAPIC, navigate to 'Alert Management', define a new alert for device disconnections and configure the notification settings.

Q: How do I integrate Nexus Dashboard with Cisco Intersight?
A: In Nexus Dashboard, go to 'Integrations', select 'Cisco Intersight' and provide the required credentials and settings.

Q: How can I schedule maintenance windows in APIC?
A: In APIC, navigate to 'Admin' > 'Maintenance Windows', define the time frame, and select the tasks to be executed during this period.

Q: How can I view the health scores of devices in cAPIC?
A: In cAPIC, navigate to the 'Dashboard', where the health scores of various devices and components will be displayed.

Q: How do I configure VLAN trunking in Nexus Dashboard?
A: In Nexus Dashboard, navigate to 'Network Settings' > 'VLAN Trunking' and specify the trunking parameters.

Q: (Follow up to Q16) How do I add VLANs to an existing trunk in Nexus Dashboard?
A: In the 'VLAN Trunking' section, select the trunk and use the 'Add VLAN' option to include additional VLANs.

Q: How can I monitor the temperature of devices using APIC?
A: In APIC, navigate to 'Device Management' > 'Temperature Monitoring' to view device temperature metrics.

Q: How do I set up user authentication using RADIUS in Nexus Dashboard?
A: In Nexus Dashboard, navigate to 'Admin' > 'AAA' > 'RADIUS Configuration' and provide the RADIUS server details.

Q: How can I migrate workloads between two sites using cAPIC?
A: In cAPIC, navigate to 'Workload Management', select the workloads, and use the 'Migrate' option to move them to the desired site.

Q: I'm seeing errors related to 'Database Consistency' on APIC. How can I address this?
A: Ensure that the database services are running on APIC. Consider running a database consistency check or restore from a recent backup if needed.

Q: The latency between devices and cAPIC has significantly increased. How can I troubleshoot?
A: Review the network path and conditions between the devices and cAPIC. Check for any network congestion or misconfigurations causing the increased latency.

Q: I've noticed that certain policies applied through APIC aren't taking effect on my devices. Why might this be?
A: Verify that the policies are correctly configured in APIC. Ensure there's no sync issue or conflicting policies that might be overriding the desired configurations.

Q: cAPIC is failing to integrate with a specific cloud region. How can I resolve this?
A: Ensure that cAPIC has the necessary permissions and configurations to integrate with the specific cloud region. Check for any API rate limits or region-specific issues.

Q: I'm receiving 'Storage Overutilization' warnings on APIC. What steps should I take?
A: Review the storage utilization on APIC. Consider cleaning up old logs, backups, or unnecessary data. Ensure the storage infrastructure is functioning optimally.

Q: I've configured a telemetry solution with cAPIC, but it isn't receiving data. How can I troubleshoot?
A: Verify the telemetry configurations in cAPIC. Ensure the telemetry solution is reachable and correctly configured to receive data.

Q: My APIC cluster is showing one node as 'Isolated'. How can I address this?
A: Ensure network connectivity to the isolated node. Check for any hardware or software issues causing the isolation. Consider rejoining the node to the cluster if needed.

Q: I'm observing discrepancies in the health scores reported by cAPIC and another monitoring tool. Why might this be?
A: Cross-reference the metrics and methods used by both cAPIC and the other monitoring tool. Differences in data collection intervals or methods might cause discrepancies.

Q: Backup and restore operations on APIC are taking longer than usual. How can I troubleshoot?
A: Check the storage and network performance during backup and restore operations. Ensure there's no contention or bottleneck causing the delay.

Q: I've noticed that after a reboot, cAPIC isn't retaining some configurations. How can I prevent this?
A: Ensure that cAPIC is correctly saving configurations to persistent storage. Consider verifying the storage health and integrity.

Q: I'm observing 'Clock Sync' errors on multiple devices managed by APIC. How can I resolve this?
A: Ensure that all devices, including APIC, are synchronized to a reliable NTP server. Check for any network issues preventing clock synchronization.

Q: The plugins I've installed on cAPIC aren't functioning correctly. How can I troubleshoot?
A: Review the plugin configurations and logs in cAPIC. Ensure the plugins are compatible with the current cAPIC version.

Q: I'm unable to apply bulk configurations on APIC using scripts. What could be the reason?
A: Ensure that the scripts are correctly formatted and there's no API rate limit being hit. Check APIC logs for any related errors.

Q: cAPIC isn't correctly reflecting the status of some services in my cloud environment. How can I address this?
A: Resync the service statuses in cAPIC. Ensure there's no communication issue between cAPIC and the cloud services.

Q: I've configured RBAC on APIC, but some users are reporting access issues. How can I troubleshoot?
A: Review the RBAC configurations in APIC. Ensure that users are assigned to the correct roles and permissions are correctly set.

Q: Certain VMs managed by cAPIC are experiencing intermittent disconnections. How can I address this?
A: Check the network configurations and conditions for the affected VMs. Review logs and metrics in cAPIC for any related issues.

Q: I'm seeing 'License Limit Exceeded' warnings on APIC. What steps should I take?
A: Review the licenses installed on APIC. Consider purchasing additional licenses or optimizing the use of existing licenses.

Q: I've set up a third-party integration with cAPIC, but I'm not receiving any callbacks. How can I troubleshoot?
A: Verify the callback configurations in the third-party tool. Ensure cAPIC is correctly set up to send callbacks and there's no network issue blocking the communication.

Q: APIC is showing 'Fabric Membership Issues'. What could be causing this?
A: Review the fabric membership settings in APIC. Ensure all devices are correctly registered and there's no sync issue causing the membership errors.

Q: cAPIC isn't reflecting the correct health scores for some VMs in my cloud environment. How can I address this?
A: Manually sync the health scores for VMs in cAPIC. Review any logs or warnings related to health score calculations.

Q: How do I configure an IP address on a Cisco router interface?
A: Use the commands `interface <interface-name>`, followed by `ip address <ip-address> <subnet-mask>`.

Q: What is the purpose of a VLAN?
A: VLANs are used to segment network traffic and create logically isolated networks within a physical network.

Q: How do I enable OSPF on a Cisco router?
A: Use the `router ospf <process-id>` command, followed by `network <network-address> <wildcard-mask> area <area-id>`.

Q: What's the difference between a hub and a switch?
A: A hub broadcasts data to all devices in a network, while a switch sends data only to the intended recipient.

Q: How can I configure a trunk port on a Cisco switch?
A: Use the commands `interface <interface-name>`, followed by `switchport mode trunk`.

Q: What is the STP protocol used for?
A: Spanning Tree Protocol (STP) prevents network loops in Ethernet networks and selects the optimal path for data flow.

Q: How do I set up a static route on a Cisco router?
A: Use the command `ip route <destination-network> <subnet-mask> <next-hop-address>`.

Q: What is the difference between RIP and OSPF?
A: RIP is a distance-vector routing protocol, while OSPF is a link-state routing protocol. OSPF is more scalable and efficient than RIP.

Q: How do I configure an access list on a Cisco router?
A: Use the command `access-list <list-number> <permit|deny> <conditions>`.

Q: How can I view the routing table on a Cisco router?
A: Use the command `show ip route`.

Q: What is the purpose of a DMZ in network topologies?
A: A DMZ (Demilitarized Zone) provides a layer of security, isolating public-facing servers from the internal network.

Q: How do I set up NAT (Network Address Translation) on a Cisco router?
A: Use the commands `ip nat inside` and `ip nat outside` on the relevant interfaces, and define NAT rules using `ip nat inside source` commands.

Q: What is a mesh network topology?
A: In a mesh topology, every device is connected to every other device, providing high redundancy and reliability.

Q: How do I configure a DHCP server on a Cisco router?
A: Use the `ip dhcp pool <pool-name>` command, followed by the necessary DHCP configuration commands.

Q: What is a star network topology?
A: In a star topology, all devices connect to a central device (like a switch), which acts as a hub for data transfer.

Q: How can I secure a network switch?
A: Use features like port security, access lists, VLAN segmentation, and switch management access controls.

Q: What is a hybrid network topology?
A: A hybrid topology combines two or more different types of topologies, such as star-ring or star-mesh.

Q: How do I reset a Cisco switch to factory defaults?
A: Use the `write erase` command followed by a reload.

Q: What is the function of a gateway in a network?
A: A gateway allows communication between different networks, translating data for different network architectures.

Q: How do I configure port security on a Cisco switch?
A: Use the commands `interface <interface-name>`, followed by `switchport port-security` and the desired port security settings.

Q: I've set up an MPLS L3 VPN in my topology, but it's not working as expected on APIC. How can I troubleshoot?
A: Ensure MPLS L3 VPN configurations, including VRF and route-target settings, are correct. Review logs on APIC for VPN-related errors.

Q: I'm seeing issues with OSPFv3 configurations not reflecting correctly on Nexus Dashboard. What steps should I take?
A: Verify OSPFv3 configurations, including area and interface settings. Ensure there's connectivity between OSPFv3 neighbors and check Nexus Dashboard logs for OSPFv3 errors.

Q: I've implemented a ring topology using Cisco devices, but I'm facing challenges with cAPIC. How can I resolve?
A: Check the configurations for all devices in the ring topology. Ensure there's proper connectivity and redundancy settings are correctly configured. Review cAPIC logs for topology-related errors.

Q: I've added a new Cisco Catalyst switch in my topology, but it's not integrating well with Nexus Dashboard. What could be the reason?
A: Ensure the Catalyst switch is online and reachable. Verify SNMP and other required protocols for integration with Nexus Dashboard are enabled.

Q: I've deployed IPv6 in my topology, but routes aren't being updated on APIC. How can I troubleshoot?
A: Ensure IPv6 configurations, including addresses and routing, are correct. Check routing tables and logs on APIC for IPv6-related errors.

Q: I'm facing issues with trunk port configurations on cAPIC. What should I do?
A: Verify trunk port configurations on the device and cross-reference with cAPIC. Ensure allowed VLANs and encapsulation settings are correct.

Q: I've set up a site-to-site VPN using Cisco routers, but it's not working seamlessly with APIC. How do I address this?
A: Check VPN configurations on routers, including peer IPs, encryption settings, and keys. Ensure there's connectivity between VPN peers and review logs on APIC for VPN errors.

Q: I've configured dynamic ARP inspection, but it's causing issues on Nexus Dashboard. How can I troubleshoot?
A: Ensure dynamic ARP inspection configurations, including trusted ports and DHCP snooping settings, are correctly set. Check Nexus Dashboard logs for ARP-related issues.

Q: I'm trying to deploy a collapsed core topology using Cisco devices, but facing challenges with cAPIC. How can I resolve?
A: Verify the configurations for all devices in the collapsed core topology. Ensure there's proper connectivity and redundancy settings are correctly configured. Review cAPIC logs for topology-related errors.

Q: After setting up IGMP snooping, it's not working as expected on Nexus Dashboard. What steps should I take?
A: Ensure IGMP snooping configurations are correctly set. Verify multicast groups and sources are correctly recognized. Check Nexus Dashboard logs for IGMP-related errors.

Q: I've configured a new IPsec tunnel, but it's not being recognized by APIC. How can I troubleshoot?
A: Verify IPsec tunnel configurations, including peer IPs, encryption settings, and keys. Ensure there's connectivity between IPsec peers and check logs on APIC for tunnel-related errors.

Q: After enabling LLDP on my devices, the configurations aren't reflecting on cAPIC. What could be the reason?
A: Ensure LLDP is enabled on devices and there's no conflicting protocol running. Check the LLDP neighbors and logs on cAPIC for LLDP-related issues.

Q: I've set up a metro Ethernet link using Cisco devices, but I'm seeing issues with Nexus Dashboard. How do I address this?
A: Check configurations for the metro Ethernet link. Ensure there's connectivity and the link settings are correctly configured. Review Nexus Dashboard logs for metro Ethernet-related errors.

Q: I've implemented SNMPv3 in my topology, but it's causing challenges on APIC. What should I do?
A: Ensure SNMPv3 configurations, including user settings and encryption, are correctly set. Verify SNMP polling and traps are working. Check logs on APIC for SNMP-related errors.

Q: I'm seeing issues with configuring route redistribution on Nexus Dashboard. How can I troubleshoot?
A: Verify route redistribution configurations, including source and destination protocols. Ensure redistribution policies are correctly set. Check Nexus Dashboard logs for redistribution errors.

Q: I've set up a new content filtering solution using Cisco devices, but it's not integrating well with cAPIC. How can I resolve?
A: Ensure the content filtering solution is correctly configured and integrated with Cisco devices. Check logs on cAPIC for filtering-related issues.

Q: After adding a new Cisco wireless LAN controller in my topology, it's not being detected by Nexus Dashboard. What steps should I take?
A: Ensure the wireless LAN controller is online and reachable. Verify SNMP settings and other required configurations for integration with Nexus Dashboard.

Q: I'm facing challenges with configuring NetFlow on APIC. How can I troubleshoot?
A: Verify NetFlow configurations, including exporters and monitors, are correctly set. Ensure there's proper flow data being sent. Check logs on APIC for NetFlow-related errors.

Q: I've deployed a new SD-WAN solution using Cisco devices, but I'm facing issues with cAPIC. How can I address this?
A: Check configurations for SD-WAN, including WAN edge devices and policies. Ensure there's proper connectivity between SD-WAN components. Review logs on cAPIC for SD-WAN errors.

Q: I've configured dynamic NAT, but it's not working as expected on Nexus Dashboard. How can I troubleshoot?
A: Ensure dynamic NAT configurations, including pools and interfaces, are correctly set. Verify NAT translations are working. Check Nexus Dashboard logs for NAT-related issues.

Q: How do I determine if my current routing protocols are supported by APIC?
A: Check APIC's supported protocols list and ensure your routing protocols are listed. If unsure, set up a test environment to validate.

Q: What can I do to ensure my multicast setup is fully compatible with cAPIC?
A: Deploy your multicast setup in a lab environment with cAPIC and monitor multicast traffic flow for any issues.

Q: How can I validate Nexus Dashboard's compatibility with my network management protocols?
A: Test your network management protocols in a non-production environment with Nexus Dashboard, ensuring stable communication and control.

Q: How do I ensure my current Nexus device configurations are compatible with cAPIC?
A: Cross-reference your Nexus device's configurations with cAPIC's best practice configurations to ensure compatibility.

Q: What steps can I take to troubleshoot compatibility issues between APIC and my current DHCP configurations?
A: Replicate your DHCP configurations in a lab setup with APIC and monitor DHCP lease assignments and renewals for any issues.

Q: How can I ensure my current security standards are supported by Nexus Dashboard?
A: Review Nexus Dashboard's supported security standards and configurations, and compare with your current standards.

Q: How can I validate that my current SAN setup will work seamlessly with APIC?
A: In a test environment, integrate your SAN setup with APIC and monitor data flow and performance for compatibility.

Q: How do I determine if my current network redundancy configurations are compatible with cAPIC?
A: Deploy your network redundancy configurations in a lab setup with cAPIC and monitor failover and redundancy mechanisms for any issues.

Q: What steps should I take to ensure my current NAC solutions work with APIC?
A: Test your NAC solutions in a lab environment with APIC, ensuring proper authentication and access control functionalities.

Q: How can I validate the compatibility of my current remote access solutions with Nexus Dashboard?
A: Deploy your remote access solutions in a test setup with Nexus Dashboard and monitor remote access connections for stability and performance.

Q: How do I troubleshoot compatibility issues between my current firewall configurations and cAPIC?
A: In a lab environment, set up your firewall configurations and integrate with cAPIC, monitoring for any incompatibilities or errors.

Q: What considerations should I keep in mind for APIC's compatibility with different IP address management solutions?
A: Ensure that the IP address management solutions use APIs and configurations that are supported by APIC. Test in a lab setup before full-scale deployment.

Q: How can I ensure that my current SNMP configurations are compatible with Nexus Dashboard?
A: Test your SNMP configurations in a lab environment with Nexus Dashboard, monitoring SNMP traps and data flow for any issues.

Q: How do I validate that my current traffic shaping configurations are supported by APIC?
A: Deploy your traffic shaping configurations in a test environment with APIC and monitor traffic flow to ensure it adheres to shaping rules.

Q: What steps should I follow to ensure my content filtering solutions are compatible with cAPIC?
A: Test your content filtering solutions in a non-production setup with cAPIC, monitoring for any filtering errors or issues.

Q: How do I determine if my current network analytics tools will work with Nexus Dashboard?
A: Integrate a few of your network analytics tools with Nexus Dashboard in a lab setup, ensuring accurate data collection and analysis.

Q: How can I ensure my current OSPF configurations are supported by APIC?
A: Set up a few OSPF areas and routes in a test environment with APIC and monitor OSPF adjacency and routing updates for stability.

Q: What steps can I take to validate cAPIC's compatibility with my current network visibility tools?
A: Test your network visibility tools in a lab environment with cAPIC, ensuring they provide accurate and comprehensive visibility into the network.

Q: How can I ensure that APIC works seamlessly with my current WAN optimization tools?
A: In a test environment, integrate your WAN optimization tools with APIC and monitor WAN traffic for optimization and performance improvements.

Q: How do I check if my current IDS solutions are compatible with Nexus Dashboard?
A: Deploy a few of your IDS devices in a lab setup with Nexus Dashboard and monitor for accurate intrusion detection and alerts.

